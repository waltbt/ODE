{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_ODE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VU_p_XDL0ooz",
        "tP3q_esu0aT2"
      ],
      "authorship_tag": "ABX9TyMUx7T7wgYiXjxrSdvFQQP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waltbt/ODE/blob/master/Basic_ODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsvUjlsq0x7j",
        "colab_type": "text"
      },
      "source": [
        "# Set up of basic modules\n",
        "I don't know why, but you must first uninstall tensorflow, so that you can properly install tensorflow-gpu or there will be errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVmRUQ021_fC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "98d30694-3f75-4f4d-8be2-ef3210e9d929"
      },
      "source": [
        "!pip install pyDOE # I believe this is only used for the latin hypercube random selection"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyDOE\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/ac/91fe4c039e2744466621343d3b8af4a485193ed0aab53af5b1db03be0989/pyDOE-0.3.8.zip\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyDOE) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyDOE) (1.4.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-cp36-none-any.whl size=18178 sha256=54a8d5558cc98a0869e2675a86deab0d96ed6bc7a0976ca9d5cd46e30c90c880\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/c8/58/a6493bd415e8ba5735082b5e0c096d7c1f2933077a8ce34544\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO7ETUzG2NOf",
        "colab_type": "text"
      },
      "source": [
        "# Optional GPU Support"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MFlGpsrvNHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip uninstall tensorflow -y\n",
        "# !pip install tensorflow-gpu \n",
        "# !pip install pyDOE # I believe this is only used for the latin hypercube random selection"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qLG9Pe72X70",
        "colab_type": "text"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhIT1W7VvTeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU_p_XDL0ooz",
        "colab_type": "text"
      },
      "source": [
        "# Logger and Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_h96oonvfyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyDOE import lhs\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
        "repoPath = os.path.join(\".\", \"PINNs\")\n",
        "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
        "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
        "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
        "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
        "\n",
        "# sys.path.insert(0, utilsPath)\n",
        "# from plotting import newfig, savefig\n",
        "\n",
        "def prep_data(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
        "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
        "    data = scipy.io.loadmat(path)\n",
        "\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = data['t'].flatten()[:,None] # T x 1\n",
        "    x = data['x'].flatten()[:,None] # N x 1\n",
        "\n",
        "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.real(data['usol']).T # T x N\n",
        "\n",
        "    if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
        "      dt = t[idx_t_1] - t[idx_t_0]\n",
        "      idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "        \n",
        "      # Boudanry data\n",
        "      x_1 = np.vstack((lb, ub))\n",
        "      \n",
        "      # Test data\n",
        "      x_star = x\n",
        "      u_star = Exact_u[idx_t_1,:]\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
        "      IRK_times = tmp[q**2+q:]\n",
        "\n",
        "      return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
        "\n",
        "    # Meshing x and t in 2D (256,100)\n",
        "    X, T = np.meshgrid(x,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = Exact_u.flatten()[:,None]\n",
        "                \n",
        "    # Noiseless data TODO: add support for noisy data    \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "\n",
        "    if N_0 != None and N_1 != None:\n",
        "      Exact_u = Exact_u.T\n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "          \n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
        "      x_1 = x[idx_x,:]\n",
        "      u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
        "      u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
        "      \n",
        "      dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
        "      q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
        "      IRK_alpha = weights[0:-1,:]\n",
        "      IRK_beta = weights[-1:,:] \n",
        "      return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
        "\n",
        "    if N_f == None:\n",
        "      lb = X_star.min(axis=0)\n",
        "      ub = X_star.max(axis=0) \n",
        "      return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
        "\n",
        "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0) \n",
        "    #Â Getting the initial conditions (t=0)\n",
        "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "    uu1 = Exact_u[0:1,:].T\n",
        "    # Getting the lowest boundary conditions (x=-1) \n",
        "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "    uu2 = Exact_u[:,0:1]\n",
        "    # Getting the highest boundary conditions (x=1) \n",
        "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "    uu3 = Exact_u[:,-1:]\n",
        "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
        "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "    u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "    #â¯Generating the x and t collocation points for f, with each having a N_f size\n",
        "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
        "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
        "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
        "    X_u_train = X_u_train[idx,:]\n",
        "    #â¯Getting the corresponding u_train\n",
        "    u_train = u_train [idx,:]\n",
        "\n",
        "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb\n",
        "\n",
        "class Logger(object):\n",
        "  def __init__(self, frequency=10):\n",
        "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "    self.start_time = time.time()\n",
        "    self.frequency = frequency\n",
        "\n",
        "  def __get_elapsed(self):\n",
        "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
        "\n",
        "  def __get_error_u(self):\n",
        "    return self.error_fn()\n",
        "\n",
        "  def set_error_fn(self, error_fn):\n",
        "    self.error_fn = error_fn\n",
        "  \n",
        "  def log_train_start(self, model):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "    self.model = model\n",
        "    print(self.model.summary())\n",
        "\n",
        "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "    if epoch % self.frequency == 0:\n",
        "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "  def log_train_opt(self, name):\n",
        "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
        "    print(f\"ââ Starting {name} optimization ââ\")\n",
        "\n",
        "  def log_train_end(self, epoch, custom=\"\"):\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP3q_esu0aT2",
        "colab_type": "text"
      },
      "source": [
        "# A custom LBFGS optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_pHSDzWvhM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoZ6--Bo0ISZ",
        "colab_type": "text"
      },
      "source": [
        "# PINN Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4AuAeiXvx4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
        "    # Descriptive Keras model [2, 20, â¦, 20, 1]\n",
        "    self.u_model = tf.keras.Sequential()\n",
        "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.u_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.nu = nu\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    #Â Separating the collocation coordinates\n",
        "    # self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
        "    self.t_f = tf.convert_to_tensor(X_f[:], dtype=self.dtype)\n",
        "    \n",
        "  # Defining custom loss\n",
        "  def __loss(self, u, u_pred):\n",
        "    f_pred = self.f_model()\n",
        "    # print(\"u from co-location points\")\n",
        "    # print(u)\n",
        "    # print(\"u from model\")\n",
        "    # print(u_pred)\n",
        "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
        "      tf.reduce_mean(tf.square(f_pred)) # I think they are making used of the fact that f=0 when system is perfect, thus all they need is the output of the neural net derived f\n",
        "\n",
        "  def __grad(self, X, u):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(u, self.u_model(X))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.u_model.trainable_variables\n",
        "    return var\n",
        "\n",
        "  # The actual PINN\n",
        "  def f_model(self):\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs weâll need later, x and t\n",
        "      # tape.watch(self.x_f)\n",
        "      tape.watch(self.t_f)\n",
        "      # Packing together the inputs\n",
        "      # X_f = tf.stack([self.t_f[:]], axis=1)\n",
        "      # print(X_f)\n",
        "      X_f = self.t_f\n",
        "\n",
        "      # Getting the prediction\n",
        "      u = self.u_model(X_f)\n",
        "      # Deriving INSIDE the tape (since weâll need the x derivative of this later, u_xx)\n",
        "      # u_x = tape.gradient(u, self.x_f)\n",
        "    \n",
        "    # Getting the other derivatives\n",
        "    # u_xx = tape.gradient(u_x, self.x_f)\n",
        "    u_t = tape.gradient(u, self.t_f)\n",
        "    # print(\"u\")\n",
        "    # print(u)\n",
        "    # print(\"U_t\")\n",
        "    # print(u_t)\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "\n",
        "    nu = self.get_params(numpy=True)\n",
        "\n",
        "    # Buidling the PINNs\n",
        "    # print(u_t - 5.0*u +3.0)\n",
        "    return u_t -5.0*u + 3.0\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    return self.nu\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = []\n",
        "    for layer in self.u_model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  def summary(self):\n",
        "    return self.u_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(X_u, u)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
        "      self.logger.log_train_epoch(epoch, loss_value)\n",
        "    \n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.__loss(u, self.u_model(X_u))\n",
        "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "\n",
        "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    u_star = self.u_model(X_star)\n",
        "    f_star = self.f_model()\n",
        "    return u_star, f_star"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBbSb7Akz618",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HenvBCql4GPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data size on the solution u\n",
        "N_u = 50\n",
        "# Collocation points size, where weâll check for f = 0\n",
        "N_f = 500\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "# layers = [1, 5,5, 1]\n",
        "# layers = [1,20,20,20,20,20,20,20,1] # Bad\n",
        "layers = [1, 5,5,5,5, 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 1000\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate=0.1,\n",
        "  beta_1=0.99,\n",
        "  epsilon=1e-8)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 1000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d8Bu9yN0N5c",
        "colab_type": "text"
      },
      "source": [
        "# Getting Data\n",
        "\n",
        "*   x is the (256,1) it is the dsicrete x axis - numbers range from -1 to 1\n",
        "*   t is the (100,10) and is the time axis - numbers range from 0 to 1\n",
        "*   X is (100,256) is a mesh of the values of x and t - Why? - This creates pairs\n",
        "*   T is (100,256) is a mesh of the values of x and t - Why? - This creates pairs\n",
        "*   Exact_u is (100,256) - it is the solution directly from the data file\n",
        "*   X_star is (25600, 2) - is made of pairs of x and t for every point in the meshes\n",
        "*   u_star is (25600, 1) - flattened Exact_u\n",
        "*   X_u_train is (50, 2) - (N_u,2) - it is built from X_star - N_u randomly selected points\n",
        "*   u_train is (50, 1) - (N_u,1) - it is built from u_star - N_u randomly selected points\n",
        "*   X_f is (10000, 2) -  (N_f,2) - x and t pairs selected using latin hyper cube...  These are are colocation points\n",
        "*   ub is [ub of x, ub of t]\n",
        "*   lb is [lb of x, lb of t]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFjYWdvpv5UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the data\n",
        "# path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "# x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "#   X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDXrqaeL6OG3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "440ae9e8-4c27-44b4-9690-a0c8b038a278"
      },
      "source": [
        "# import numpy as np\n",
        "# N_f = 200\n",
        "# N_u = 50\n",
        "t = np.linspace(0.0,2.0,1000)\n",
        "Exact_u = (2.0/5.0)*np.exp(5.0*(t-2.0)) + (3.0/5.0)\n",
        "\n",
        "idx = np.random.choice(t.shape[0], N_f, replace=False)\n",
        "X_f =  t[idx]  # Need - This is a bunch of t\n",
        "X_f = np.reshape(X_f, (-1, 1))\n",
        "# print(X_f)\n",
        "# print(X_f.shape)\n",
        "\n",
        "X_star = t\n",
        "X_star = np.reshape(X_star, (-1, 1))\n",
        "u_star = Exact_u\n",
        "u_star = np.reshape(u_star, (-1, 1))\n",
        "# print(u_star.shape)\n",
        "\n",
        "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "X_u_train = [2.0] # X_star[idx] # Need\n",
        "X_u_train = np.reshape(X_u_train, (-1, 1))\n",
        "u_train = [1.0] # u_star[idx] # Need\n",
        "u_train = np.reshape(u_train, (-1, 1))\n",
        "# print(X_u_train.shape)\n",
        "# print(u_train.shape)\n",
        "\n",
        "ub = np.array([2.0])\n",
        "lb = np.array([0.0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRr3pJWw0RXi",
        "colab_type": "text"
      },
      "source": [
        "# Actual Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGKN8VFNv-H1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1cc0a7f-51fb-4240-abcf-b7e0b93e1517"
      },
      "source": [
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
        "def error():\n",
        "  u_pred, _ = pinn.predict(X_star)\n",
        "  return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_pred, f_pred = pinn.predict(X_star)\n",
        "# print(u_pred.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.2.0\n",
            "Eager execution: True\n",
            "GPU-accerelated: False\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_5 (Lambda)            (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 5)                 10        \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 106\n",
            "Trainable params: 106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "ââ Starting Adam optimization ââ\n",
            "tf_epoch =      0  elapsed = 00:00  loss = 1.1964e+01  error = 7.2129e-01  \n",
            "tf_epoch =     10  elapsed = 00:00  loss = 1.6362e+00  error = 1.6183e-01  \n",
            "tf_epoch =     20  elapsed = 00:00  loss = 5.7878e-01  error = 3.0063e-01  \n",
            "tf_epoch =     30  elapsed = 00:00  loss = 1.8946e-01  error = 1.3438e-01  \n",
            "tf_epoch =     40  elapsed = 00:00  loss = 3.1636e-01  error = 1.3688e-01  \n",
            "tf_epoch =     50  elapsed = 00:00  loss = 2.2593e-01  error = 1.4311e-01  \n",
            "tf_epoch =     60  elapsed = 00:00  loss = 2.2064e-01  error = 1.4193e-01  \n",
            "tf_epoch =     70  elapsed = 00:00  loss = 1.6928e-01  error = 1.7097e-01  \n",
            "tf_epoch =     80  elapsed = 00:01  loss = 2.5646e-01  error = 1.3239e-01  \n",
            "tf_epoch =     90  elapsed = 00:01  loss = 1.5587e-01  error = 1.2534e-01  \n",
            "tf_epoch =    100  elapsed = 00:01  loss = 1.8502e-01  error = 1.7102e-01  \n",
            "tf_epoch =    110  elapsed = 00:01  loss = 2.2878e-01  error = 1.2936e-01  \n",
            "tf_epoch =    120  elapsed = 00:01  loss = 1.6087e-01  error = 1.2838e-01  \n",
            "tf_epoch =    130  elapsed = 00:01  loss = 1.6705e-01  error = 1.5883e-01  \n",
            "tf_epoch =    140  elapsed = 00:01  loss = 2.1028e-01  error = 1.2782e-01  \n",
            "tf_epoch =    150  elapsed = 00:02  loss = 1.6775e-01  error = 1.3273e-01  \n",
            "tf_epoch =    160  elapsed = 00:02  loss = 1.5396e-01  error = 1.4468e-01  \n",
            "tf_epoch =    170  elapsed = 00:02  loss = 1.9009e-01  error = 1.2528e-01  \n",
            "tf_epoch =    180  elapsed = 00:02  loss = 1.7219e-01  error = 1.3504e-01  \n",
            "tf_epoch =    190  elapsed = 00:02  loss = 1.4844e-01  error = 1.3390e-01  \n",
            "tf_epoch =    200  elapsed = 00:02  loss = 1.7867e-01  error = 1.1953e-01  \n",
            "tf_epoch =    210  elapsed = 00:02  loss = 1.5692e-01  error = 1.2269e-01  \n",
            "tf_epoch =    220  elapsed = 00:03  loss = 1.4394e-01  error = 1.2960e-01  \n",
            "tf_epoch =    230  elapsed = 00:03  loss = 1.5919e-01  error = 1.0907e-01  \n",
            "tf_epoch =    240  elapsed = 00:03  loss = 1.3914e-01  error = 1.0818e-01  \n",
            "tf_epoch =    250  elapsed = 00:03  loss = 1.2221e-01  error = 9.6246e-02  \n",
            "tf_epoch =    260  elapsed = 00:03  loss = 1.3163e-01  error = 8.1796e-02  \n",
            "tf_epoch =    270  elapsed = 00:03  loss = 1.0808e-01  error = 9.1912e-02  \n",
            "tf_epoch =    280  elapsed = 00:03  loss = 8.7479e-02  error = 9.4040e-02  \n",
            "tf_epoch =    290  elapsed = 00:03  loss = 8.2339e-02  error = 7.6864e-02  \n",
            "tf_epoch =    300  elapsed = 00:04  loss = 7.8730e-02  error = 9.2508e-02  \n",
            "tf_epoch =    310  elapsed = 00:04  loss = 7.2591e-02  error = 8.9567e-02  \n",
            "tf_epoch =    320  elapsed = 00:04  loss = 7.3718e-02  error = 3.3094e-02  \n",
            "tf_epoch =    330  elapsed = 00:04  loss = 5.1419e-02  error = 5.7759e-02  \n",
            "tf_epoch =    340  elapsed = 00:04  loss = 7.5964e-02  error = 9.0282e-02  \n",
            "tf_epoch =    350  elapsed = 00:04  loss = 6.2244e-02  error = 7.9579e-02  \n",
            "tf_epoch =    360  elapsed = 00:04  loss = 5.9645e-02  error = 3.6627e-02  \n",
            "tf_epoch =    370  elapsed = 00:04  loss = 3.9780e-02  error = 4.0494e-02  \n",
            "tf_epoch =    380  elapsed = 00:05  loss = 5.5144e-02  error = 7.2051e-02  \n",
            "tf_epoch =    390  elapsed = 00:05  loss = 3.4554e-02  error = 4.3651e-02  \n",
            "tf_epoch =    400  elapsed = 00:05  loss = 4.3277e-02  error = 3.4416e-02  \n",
            "tf_epoch =    410  elapsed = 00:05  loss = 3.3091e-02  error = 5.8007e-02  \n",
            "tf_epoch =    420  elapsed = 00:05  loss = 3.3061e-02  error = 4.5133e-02  \n",
            "tf_epoch =    430  elapsed = 00:05  loss = 2.4432e-02  error = 3.9475e-02  \n",
            "tf_epoch =    440  elapsed = 00:05  loss = 2.6600e-02  error = 3.4012e-02  \n",
            "tf_epoch =    450  elapsed = 00:06  loss = 2.0916e-02  error = 4.5438e-02  \n",
            "tf_epoch =    460  elapsed = 00:06  loss = 2.5999e-02  error = 4.5257e-02  \n",
            "tf_epoch =    470  elapsed = 00:06  loss = 1.9392e-02  error = 3.0664e-02  \n",
            "tf_epoch =    480  elapsed = 00:06  loss = 1.9517e-02  error = 2.3847e-02  \n",
            "tf_epoch =    490  elapsed = 00:06  loss = 1.6039e-02  error = 2.7173e-02  \n",
            "tf_epoch =    500  elapsed = 00:06  loss = 1.7911e-02  error = 3.9545e-02  \n",
            "tf_epoch =    510  elapsed = 00:06  loss = 1.3850e-02  error = 3.1337e-02  \n",
            "tf_epoch =    520  elapsed = 00:07  loss = 1.1811e-02  error = 1.8268e-02  \n",
            "tf_epoch =    530  elapsed = 00:07  loss = 9.0809e-03  error = 2.9105e-02  \n",
            "tf_epoch =    540  elapsed = 00:07  loss = 9.8260e-03  error = 2.2331e-02  \n",
            "tf_epoch =    550  elapsed = 00:07  loss = 7.4063e-03  error = 2.3334e-02  \n",
            "tf_epoch =    560  elapsed = 00:07  loss = 7.6684e-03  error = 1.4585e-02  \n",
            "tf_epoch =    570  elapsed = 00:07  loss = 7.5073e-03  error = 2.2647e-02  \n",
            "tf_epoch =    580  elapsed = 00:07  loss = 5.4971e-03  error = 1.7957e-02  \n",
            "tf_epoch =    590  elapsed = 00:07  loss = 5.4924e-03  error = 1.8652e-02  \n",
            "tf_epoch =    600  elapsed = 00:08  loss = 4.3883e-03  error = 1.1690e-02  \n",
            "tf_epoch =    610  elapsed = 00:08  loss = 3.3716e-03  error = 2.2040e-02  \n",
            "tf_epoch =    620  elapsed = 00:08  loss = 4.7070e-03  error = 1.4788e-02  \n",
            "tf_epoch =    630  elapsed = 00:08  loss = 4.9288e-03  error = 1.0572e-02  \n",
            "tf_epoch =    640  elapsed = 00:08  loss = 3.1905e-03  error = 1.9810e-02  \n",
            "tf_epoch =    650  elapsed = 00:08  loss = 3.9701e-03  error = 1.1125e-02  \n",
            "tf_epoch =    660  elapsed = 00:08  loss = 2.4309e-03  error = 1.3312e-02  \n",
            "tf_epoch =    670  elapsed = 00:08  loss = 3.8568e-03  error = 1.0766e-02  \n",
            "tf_epoch =    680  elapsed = 00:09  loss = 2.7009e-03  error = 1.5270e-02  \n",
            "tf_epoch =    690  elapsed = 00:09  loss = 3.5621e-03  error = 1.1767e-02  \n",
            "tf_epoch =    700  elapsed = 00:09  loss = 2.6772e-03  error = 1.1043e-02  \n",
            "tf_epoch =    710  elapsed = 00:09  loss = 2.4342e-03  error = 1.1341e-02  \n",
            "tf_epoch =    720  elapsed = 00:09  loss = 2.7537e-03  error = 1.1951e-02  \n",
            "tf_epoch =    730  elapsed = 00:09  loss = 2.2563e-03  error = 1.5344e-02  \n",
            "tf_epoch =    740  elapsed = 00:09  loss = 2.8151e-03  error = 1.2112e-02  \n",
            "tf_epoch =    750  elapsed = 00:10  loss = 2.1950e-03  error = 1.2134e-02  \n",
            "tf_epoch =    760  elapsed = 00:10  loss = 1.9890e-03  error = 9.7707e-03  \n",
            "tf_epoch =    770  elapsed = 00:10  loss = 2.0172e-03  error = 1.0966e-02  \n",
            "tf_epoch =    780  elapsed = 00:10  loss = 1.8207e-03  error = 1.0604e-02  \n",
            "tf_epoch =    790  elapsed = 00:10  loss = 2.0062e-03  error = 8.9555e-03  \n",
            "tf_epoch =    800  elapsed = 00:10  loss = 1.4980e-03  error = 8.5933e-03  \n",
            "tf_epoch =    810  elapsed = 00:10  loss = 1.5342e-03  error = 9.2280e-03  \n",
            "tf_epoch =    820  elapsed = 00:10  loss = 1.4431e-03  error = 1.0132e-02  \n",
            "tf_epoch =    830  elapsed = 00:11  loss = 1.3608e-03  error = 8.6881e-03  \n",
            "tf_epoch =    840  elapsed = 00:11  loss = 1.4289e-03  error = 8.9343e-03  \n",
            "tf_epoch =    850  elapsed = 00:11  loss = 1.1989e-03  error = 7.7317e-03  \n",
            "tf_epoch =    860  elapsed = 00:11  loss = 1.0705e-03  error = 7.5581e-03  \n",
            "tf_epoch =    870  elapsed = 00:11  loss = 1.1550e-03  error = 6.8502e-03  \n",
            "tf_epoch =    880  elapsed = 00:11  loss = 1.2101e-03  error = 6.4051e-03  \n",
            "tf_epoch =    890  elapsed = 00:11  loss = 1.1231e-03  error = 5.9774e-03  \n",
            "tf_epoch =    900  elapsed = 00:11  loss = 1.1054e-03  error = 6.6390e-03  \n",
            "tf_epoch =    910  elapsed = 00:12  loss = 1.1149e-03  error = 6.9257e-03  \n",
            "tf_epoch =    920  elapsed = 00:12  loss = 1.0893e-03  error = 6.5794e-03  \n",
            "tf_epoch =    930  elapsed = 00:12  loss = 1.0082e-03  error = 6.6007e-03  \n",
            "tf_epoch =    940  elapsed = 00:12  loss = 1.0035e-03  error = 5.9667e-03  \n",
            "tf_epoch =    950  elapsed = 00:12  loss = 1.0151e-03  error = 6.0928e-03  \n",
            "tf_epoch =    960  elapsed = 00:12  loss = 9.8304e-04  error = 5.7168e-03  \n",
            "tf_epoch =    970  elapsed = 00:12  loss = 9.2858e-04  error = 6.0518e-03  \n",
            "tf_epoch =    980  elapsed = 00:12  loss = 9.2177e-04  error = 6.2803e-03  \n",
            "tf_epoch =    990  elapsed = 00:13  loss = 9.2842e-04  error = 6.0335e-03  \n",
            "ââ Starting LBFGS optimization ââ\n",
            "nt_epoch =     10  elapsed = 00:13  loss = 8.8577e-04  error = 5.8021e-03  \n",
            "nt_epoch =     20  elapsed = 00:13  loss = 8.6676e-04  error = 5.6687e-03  \n",
            "nt_epoch =     30  elapsed = 00:13  loss = 8.6302e-04  error = 5.4892e-03  \n",
            "nt_epoch =     40  elapsed = 00:14  loss = 8.2434e-04  error = 5.8571e-03  \n",
            "nt_epoch =     50  elapsed = 00:14  loss = 7.8794e-04  error = 5.7869e-03  \n",
            "nt_epoch =     60  elapsed = 00:14  loss = 3.4419e-02  error = 5.3036e-02  \n",
            "nt_epoch =     70  elapsed = 00:14  loss = 6.9903e-04  error = 5.6207e-03  \n",
            "nt_epoch =     80  elapsed = 00:15  loss = 6.0987e-04  error = 4.7204e-03  \n",
            "nt_epoch =     90  elapsed = 00:15  loss = 5.4958e-04  error = 4.4528e-03  \n",
            "nt_epoch =    100  elapsed = 00:15  loss = 5.0719e-04  error = 4.3512e-03  \n",
            "nt_epoch =    110  elapsed = 00:15  loss = 4.6661e-04  error = 4.2810e-03  \n",
            "nt_epoch =    120  elapsed = 00:16  loss = 4.6302e-04  error = 4.4349e-03  \n",
            "nt_epoch =    130  elapsed = 00:16  loss = 4.0927e-04  error = 4.0930e-03  \n",
            "nt_epoch =    140  elapsed = 00:16  loss = 3.9251e-04  error = 4.0665e-03  \n",
            "nt_epoch =    150  elapsed = 00:16  loss = 3.8438e-04  error = 4.1256e-03  \n",
            "nt_epoch =    160  elapsed = 00:17  loss = 3.7720e-04  error = 4.1969e-03  \n",
            "nt_epoch =    170  elapsed = 00:17  loss = 3.5965e-04  error = 3.7085e-03  \n",
            "nt_epoch =    180  elapsed = 00:17  loss = 3.4819e-04  error = 3.6577e-03  \n",
            "nt_epoch =    190  elapsed = 00:18  loss = 3.4512e-04  error = 3.5751e-03  \n",
            "nt_epoch =    200  elapsed = 00:18  loss = 3.3687e-04  error = 3.4891e-03  \n",
            "nt_epoch =    210  elapsed = 00:18  loss = 3.8936e-04  error = 2.8989e-03  \n",
            "nt_epoch =    220  elapsed = 00:18  loss = 3.0234e-04  error = 3.6582e-03  \n",
            "nt_epoch =    230  elapsed = 00:19  loss = 2.3255e-04  error = 2.3293e-03  \n",
            "nt_epoch =    240  elapsed = 00:19  loss = 1.2655e-04  error = 1.4797e-03  \n",
            "nt_epoch =    250  elapsed = 00:19  loss = 1.1178e-04  error = 1.4636e-03  \n",
            "nt_epoch =    260  elapsed = 00:20  loss = 9.7592e-05  error = 1.2965e-03  \n",
            "nt_epoch =    270  elapsed = 00:20  loss = 8.9848e-05  error = 1.1695e-03  \n",
            "nt_epoch =    280  elapsed = 00:20  loss = 8.0156e-05  error = 1.0910e-03  \n",
            "nt_epoch =    290  elapsed = 00:20  loss = 7.0717e-05  error = 7.7859e-04  \n",
            "nt_epoch =    300  elapsed = 00:21  loss = 6.7575e-05  error = 7.7377e-04  \n",
            "nt_epoch =    310  elapsed = 00:21  loss = 5.3338e-05  error = 7.3064e-04  \n",
            "nt_epoch =    320  elapsed = 00:21  loss = 4.7163e-05  error = 6.0741e-04  \n",
            "nt_epoch =    330  elapsed = 00:22  loss = 4.6125e-05  error = 5.3043e-04  \n",
            "nt_epoch =    340  elapsed = 00:22  loss = 3.6962e-05  error = 3.7504e-04  \n",
            "nt_epoch =    350  elapsed = 00:22  loss = 3.2714e-05  error = 5.4775e-04  \n",
            "nt_epoch =    360  elapsed = 00:22  loss = 2.3130e-05  error = 6.2910e-04  \n",
            "nt_epoch =    370  elapsed = 00:23  loss = 2.3072e-05  error = 6.4163e-04  \n",
            "nt_epoch =    380  elapsed = 00:23  loss = 2.2919e-05  error = 6.8482e-04  \n",
            "nt_epoch =    390  elapsed = 00:23  loss = 2.2573e-05  error = 6.7070e-04  \n",
            "nt_epoch =    400  elapsed = 00:23  loss = 2.2332e-05  error = 6.9142e-04  \n",
            "nt_epoch =    410  elapsed = 00:24  loss = 2.2034e-05  error = 6.7146e-04  \n",
            "nt_epoch =    420  elapsed = 00:24  loss = 2.1502e-05  error = 6.4483e-04  \n",
            "nt_epoch =    430  elapsed = 00:24  loss = 3.3721e-05  error = 8.6175e-04  \n",
            "==================\n",
            "Training finished (epoch 2000): duration = 00:24  error = 5.7148e-01  \n",
            "(1000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-68wRAs4pJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "bf2f4d61-e40a-4363-bcb8-3fbefb1a5eba"
      },
      "source": [
        "plt.plot(t, Exact_u)\n",
        "plt.plot(t, u_pred)\n",
        "plt.legend(['Exact Answer', 'NN solution'])\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ZsgTBvF7qB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d14b782-3b12-43bf-e546-3dc40d06ab05"
      },
      "source": [
        "# test = [0.9]\n",
        "# test = np.reshape(test, (-1, 1))\n",
        "# pinn.predict(test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6003282]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(500, 1), dtype=float32, numpy=\n",
              " array([[-6.19411469e-04],\n",
              "        [ 1.51395798e-04],\n",
              "        [ 7.41720200e-04],\n",
              "        [-5.86748123e-04],\n",
              "        [-3.90529633e-04],\n",
              "        [-2.14815140e-04],\n",
              "        [-6.20365143e-04],\n",
              "        [-5.57899475e-04],\n",
              "        [-6.04629517e-04],\n",
              "        [ 1.18565559e-03],\n",
              "        [-5.95092773e-04],\n",
              "        [-3.70979309e-04],\n",
              "        [-3.62396240e-04],\n",
              "        [ 1.27625465e-03],\n",
              "        [ 9.41753387e-05],\n",
              "        [-4.87565994e-04],\n",
              "        [ 1.91712379e-03],\n",
              "        [ 6.08921051e-04],\n",
              "        [-4.41312790e-04],\n",
              "        [ 1.78551674e-03],\n",
              "        [-5.68628311e-04],\n",
              "        [-5.14984131e-04],\n",
              "        [-2.05278397e-04],\n",
              "        [ 1.01566315e-04],\n",
              "        [-5.69820404e-05],\n",
              "        [-4.01735306e-04],\n",
              "        [-6.20603561e-04],\n",
              "        [-6.20126724e-04],\n",
              "        [-6.06298447e-04],\n",
              "        [-3.44753265e-04],\n",
              "        [ 9.90390778e-04],\n",
              "        [ 8.28027725e-04],\n",
              "        [-5.73158264e-04],\n",
              "        [-4.40835953e-04],\n",
              "        [ 1.07073784e-03],\n",
              "        [-1.42574310e-04],\n",
              "        [-6.15119934e-04],\n",
              "        [-6.07728958e-04],\n",
              "        [-4.59909439e-04],\n",
              "        [-3.25202942e-04],\n",
              "        [ 5.28097153e-04],\n",
              "        [-1.62124634e-04],\n",
              "        [ 9.42230225e-04],\n",
              "        [-3.92675400e-04],\n",
              "        [-6.11782074e-04],\n",
              "        [-6.17265701e-04],\n",
              "        [-2.29835510e-04],\n",
              "        [-4.60386276e-04],\n",
              "        [-6.17742538e-04],\n",
              "        [-2.74181366e-04],\n",
              "        [-5.70774078e-04],\n",
              "        [-5.63144684e-04],\n",
              "        [-3.80277634e-04],\n",
              "        [-4.78267670e-04],\n",
              "        [ 1.10626221e-03],\n",
              "        [-5.60760498e-04],\n",
              "        [-2.64167786e-04],\n",
              "        [-4.01258469e-04],\n",
              "        [-4.00304794e-04],\n",
              "        [ 1.56545639e-03],\n",
              "        [-8.98838043e-05],\n",
              "        [-1.67131424e-04],\n",
              "        [-5.73158264e-04],\n",
              "        [-4.52995300e-04],\n",
              "        [-3.23057175e-04],\n",
              "        [ 9.73939896e-04],\n",
              "        [-5.38825989e-04],\n",
              "        [-6.04152679e-04],\n",
              "        [ 6.66379929e-04],\n",
              "        [ 1.38211250e-03],\n",
              "        [-5.93662262e-04],\n",
              "        [-3.65257263e-04],\n",
              "        [-5.95569611e-04],\n",
              "        [ 1.27553940e-04],\n",
              "        [-1.48534775e-04],\n",
              "        [-5.82933426e-04],\n",
              "        [-4.90665436e-04],\n",
              "        [ 3.50952148e-04],\n",
              "        [ 1.10864639e-04],\n",
              "        [ 4.46796417e-04],\n",
              "        [-5.88178635e-04],\n",
              "        [-4.44173813e-04],\n",
              "        [-5.77211380e-04],\n",
              "        [ 4.22239304e-04],\n",
              "        [-6.17742538e-04],\n",
              "        [ 2.41994858e-04],\n",
              "        [-5.46455383e-04],\n",
              "        [-4.99486923e-04],\n",
              "        [-5.89132309e-04],\n",
              "        [-4.94956970e-04],\n",
              "        [-2.09093094e-04],\n",
              "        [-4.96625900e-04],\n",
              "        [-7.77244568e-05],\n",
              "        [ 2.01535225e-03],\n",
              "        [ 4.02927399e-05],\n",
              "        [-3.38792801e-04],\n",
              "        [ 5.03778458e-04],\n",
              "        [-4.95195389e-04],\n",
              "        [-1.95264816e-04],\n",
              "        [ 1.02329254e-03],\n",
              "        [ 8.44001770e-04],\n",
              "        [ 2.20537186e-04],\n",
              "        [ 4.28915024e-04],\n",
              "        [ 3.04698944e-04],\n",
              "        [ 8.07046890e-04],\n",
              "        [-2.14338303e-04],\n",
              "        [ 2.95639038e-05],\n",
              "        [ 3.62396240e-05],\n",
              "        [-2.40087509e-04],\n",
              "        [-4.75883484e-04],\n",
              "        [ 1.24001503e-03],\n",
              "        [-3.83615494e-04],\n",
              "        [-5.78641891e-04],\n",
              "        [-5.99622726e-04],\n",
              "        [-4.97579575e-04],\n",
              "        [-5.38349152e-04],\n",
              "        [-2.60353088e-04],\n",
              "        [-5.79595566e-04],\n",
              "        [-3.58819962e-04],\n",
              "        [-3.96966934e-04],\n",
              "        [-6.18696213e-04],\n",
              "        [ 1.23023987e-03],\n",
              "        [-4.69684601e-04],\n",
              "        [-1.42335892e-04],\n",
              "        [-4.55379486e-05],\n",
              "        [-5.81264496e-04],\n",
              "        [ 1.90734863e-06],\n",
              "        [-5.85079193e-04],\n",
              "        [-4.19855118e-04],\n",
              "        [ 2.17127800e-03],\n",
              "        [-7.86781311e-06],\n",
              "        [-4.57286835e-04],\n",
              "        [-5.38825989e-04],\n",
              "        [-4.13656235e-04],\n",
              "        [-4.71591949e-04],\n",
              "        [ 1.88350677e-04],\n",
              "        [ 4.62055206e-04],\n",
              "        [-1.96456909e-04],\n",
              "        [-5.26666641e-04],\n",
              "        [ 7.43865967e-05],\n",
              "        [-5.85317612e-04],\n",
              "        [ 7.08818436e-04],\n",
              "        [-1.14202499e-04],\n",
              "        [ 9.66072083e-04],\n",
              "        [ 7.93933868e-05],\n",
              "        [ 1.29222870e-04],\n",
              "        [ 1.49178505e-03],\n",
              "        [-2.46763229e-04],\n",
              "        [ 8.71896744e-04],\n",
              "        [ 7.92026520e-04],\n",
              "        [-5.92231750e-04],\n",
              "        [ 2.08139420e-04],\n",
              "        [ 1.66201591e-03],\n",
              "        [-3.71456146e-04],\n",
              "        [-5.85317612e-04],\n",
              "        [ 7.34090805e-04],\n",
              "        [-6.20126724e-04],\n",
              "        [-4.19139862e-04],\n",
              "        [-1.52587891e-05],\n",
              "        [-4.32968140e-04],\n",
              "        [ 3.77893448e-04],\n",
              "        [ 1.14154816e-03],\n",
              "        [ 1.44124031e-03],\n",
              "        [-5.80072403e-04],\n",
              "        [-9.98973846e-05],\n",
              "        [-6.07490540e-04],\n",
              "        [ 3.14474106e-04],\n",
              "        [-3.03745270e-04],\n",
              "        [ 7.14778900e-04],\n",
              "        [-5.17845154e-04],\n",
              "        [-6.14643097e-04],\n",
              "        [-5.13553619e-04],\n",
              "        [-2.83002853e-04],\n",
              "        [ 1.22785568e-04],\n",
              "        [ 8.10623169e-05],\n",
              "        [ 2.75611877e-04],\n",
              "        [-6.16312027e-04],\n",
              "        [-1.21593475e-05],\n",
              "        [-1.57594681e-04],\n",
              "        [-2.03132629e-04],\n",
              "        [-5.47409058e-04],\n",
              "        [ 6.00814819e-05],\n",
              "        [ 2.14600563e-03],\n",
              "        [-6.17742538e-04],\n",
              "        [-3.21149826e-04],\n",
              "        [-3.45706940e-05],\n",
              "        [ 1.45673752e-04],\n",
              "        [-6.17742538e-04],\n",
              "        [-6.34193420e-05],\n",
              "        [-4.38213348e-04],\n",
              "        [ 2.94923782e-04],\n",
              "        [ 1.33299828e-03],\n",
              "        [-4.99725342e-04],\n",
              "        [ 1.09696388e-03],\n",
              "        [-8.10623169e-05],\n",
              "        [-2.49147415e-04],\n",
              "        [-6.10351562e-04],\n",
              "        [-5.89609146e-04],\n",
              "        [ 1.51205063e-03],\n",
              "        [-3.11136246e-04],\n",
              "        [ 6.46114349e-05],\n",
              "        [ 1.76310539e-03],\n",
              "        [ 1.74760818e-04],\n",
              "        [-2.99692154e-04],\n",
              "        [-2.93970108e-04],\n",
              "        [ 2.19702721e-03],\n",
              "        [ 1.04904175e-04],\n",
              "        [ 1.54304504e-03],\n",
              "        [ 4.81605530e-05],\n",
              "        [-4.75645065e-04],\n",
              "        [-6.11066818e-04],\n",
              "        [ 1.55329704e-03],\n",
              "        [-6.20365143e-04],\n",
              "        [ 2.02655792e-04],\n",
              "        [-2.21729279e-04],\n",
              "        [-6.20126724e-04],\n",
              "        [-1.86920166e-04],\n",
              "        [-5.24044037e-04],\n",
              "        [-5.84363937e-04],\n",
              "        [-6.11782074e-04],\n",
              "        [ 1.00851059e-04],\n",
              "        [-3.77893448e-04],\n",
              "        [-6.15358353e-04],\n",
              "        [ 3.10182571e-04],\n",
              "        [-3.38792801e-04],\n",
              "        [-1.71899796e-04],\n",
              "        [ 9.19580460e-04],\n",
              "        [-4.77313995e-04],\n",
              "        [ 6.46829605e-04],\n",
              "        [-3.59296799e-04],\n",
              "        [-4.82559204e-04],\n",
              "        [-2.77757645e-04],\n",
              "        [-2.96115875e-04],\n",
              "        [ 2.78949738e-05],\n",
              "        [-3.15427780e-04],\n",
              "        [-2.54154205e-04],\n",
              "        [-5.17845154e-04],\n",
              "        [-2.97307968e-04],\n",
              "        [ 1.97744370e-03],\n",
              "        [ 3.36170197e-04],\n",
              "        [ 4.17232513e-04],\n",
              "        [-8.74996185e-05],\n",
              "        [ 6.01291656e-04],\n",
              "        [ 1.79672241e-03],\n",
              "        [-6.05344772e-04],\n",
              "        [-3.67879868e-04],\n",
              "        [-3.57866287e-04],\n",
              "        [-5.86748123e-04],\n",
              "        [-4.65393066e-04],\n",
              "        [-2.91109085e-04],\n",
              "        [-6.16788864e-04],\n",
              "        [-1.82151794e-04],\n",
              "        [-6.03675842e-04],\n",
              "        [-8.46385956e-05],\n",
              "        [ 9.58919525e-04],\n",
              "        [ 1.58691406e-03],\n",
              "        [-5.80072403e-04],\n",
              "        [-4.04119492e-04],\n",
              "        [ 6.87599182e-04],\n",
              "        [ 9.03129578e-04],\n",
              "        [-5.97715378e-04],\n",
              "        [ 1.47056580e-03],\n",
              "        [ 1.17588043e-03],\n",
              "        [-4.93526459e-04],\n",
              "        [-4.70399857e-04],\n",
              "        [-3.04222107e-04],\n",
              "        [-6.02006912e-04],\n",
              "        [-6.19173050e-04],\n",
              "        [-6.09159470e-04],\n",
              "        [-1.91211700e-04],\n",
              "        [-5.81502914e-04],\n",
              "        [-2.52485275e-04],\n",
              "        [ 1.50251389e-03],\n",
              "        [-9.41753387e-05],\n",
              "        [ 1.70660019e-03],\n",
              "        [ 7.98940659e-04],\n",
              "        [ 2.46047974e-04],\n",
              "        [ 2.18653679e-03],\n",
              "        [ 5.21659851e-04],\n",
              "        [-6.03199005e-04],\n",
              "        [ 8.53538513e-05],\n",
              "        [-6.10351562e-04],\n",
              "        [ 2.19106674e-04],\n",
              "        [-6.16312027e-04],\n",
              "        [-5.97238541e-04],\n",
              "        [-3.41653824e-04],\n",
              "        [-6.19649887e-04],\n",
              "        [-5.86509705e-04],\n",
              "        [-6.19888306e-04],\n",
              "        [ 4.91857529e-04],\n",
              "        [-5.77688217e-04],\n",
              "        [ 6.33716583e-04],\n",
              "        [-4.61816788e-04],\n",
              "        [ 1.74021721e-03],\n",
              "        [-4.09603119e-04],\n",
              "        [-4.36782837e-04],\n",
              "        [ 5.33342361e-04],\n",
              "        [-6.12497330e-04],\n",
              "        [ 3.67641449e-04],\n",
              "        [-1.69754028e-04],\n",
              "        [ 1.21212006e-03],\n",
              "        [-5.61237335e-04],\n",
              "        [ 3.57866287e-04],\n",
              "        [-1.66893005e-06],\n",
              "        [ 6.40392303e-04],\n",
              "        [-6.18457794e-04],\n",
              "        [-1.66893005e-06],\n",
              "        [-5.59091568e-04],\n",
              "        [-4.87565994e-04],\n",
              "        [ 1.15871429e-04],\n",
              "        [-4.81128693e-04],\n",
              "        [ 8.13722610e-04],\n",
              "        [ 1.46102905e-03],\n",
              "        [ 7.28368759e-04],\n",
              "        [-3.43084335e-04],\n",
              "        [ 9.98497009e-04],\n",
              "        [-6.08921051e-04],\n",
              "        [-3.51428986e-04],\n",
              "        [-5.24044037e-04],\n",
              "        [ 1.83272362e-03],\n",
              "        [-5.47409058e-04],\n",
              "        [ 1.52349472e-03],\n",
              "        [ 4.51564789e-04],\n",
              "        [ 1.82080269e-03],\n",
              "        [-4.24146652e-04],\n",
              "        [-4.62293625e-04],\n",
              "        [-6.09874725e-04],\n",
              "        [-1.93357468e-04],\n",
              "        [-2.25067139e-04],\n",
              "        [-6.18934631e-04],\n",
              "        [-5.22613525e-04],\n",
              "        [-2.89916992e-04],\n",
              "        [-5.82695007e-04],\n",
              "        [ 2.00390816e-03],\n",
              "        [-3.88383865e-04],\n",
              "        [-5.20229340e-04],\n",
              "        [-4.36067581e-04],\n",
              "        [-6.08444214e-04],\n",
              "        [ 1.53350830e-03],\n",
              "        [-1.90973282e-04],\n",
              "        [ 1.93023682e-03],\n",
              "        [-4.72784042e-04],\n",
              "        [-5.87463379e-04],\n",
              "        [ 1.43146515e-03],\n",
              "        [-4.06265259e-04],\n",
              "        [-5.86748123e-04],\n",
              "        [ 1.22141838e-03],\n",
              "        [-3.31640244e-04],\n",
              "        [ 7.56263733e-04],\n",
              "        [-3.01599503e-04],\n",
              "        [-1.21831894e-04],\n",
              "        [ 3.40461731e-04],\n",
              "        [-5.94854355e-04],\n",
              "        [-3.97920609e-04],\n",
              "        [ 6.60896301e-04],\n",
              "        [-1.16825104e-04],\n",
              "        [ 1.60694122e-03],\n",
              "        [-4.28199768e-04],\n",
              "        [-2.44140625e-04],\n",
              "        [-6.11543655e-04],\n",
              "        [-3.53813171e-04],\n",
              "        [ 1.65104866e-03],\n",
              "        [-1.32799149e-04],\n",
              "        [-3.32593918e-04],\n",
              "        [ 1.31130219e-04],\n",
              "        [ 1.39093399e-03],\n",
              "        [ 4.29153442e-06],\n",
              "        [-2.11000443e-04],\n",
              "        [ 4.58002090e-04],\n",
              "        [-5.79118729e-04],\n",
              "        [-6.15358353e-04],\n",
              "        [ 1.04665756e-03],\n",
              "        [-4.47750092e-04],\n",
              "        [ 2.85148621e-04],\n",
              "        [-6.20365143e-04],\n",
              "        [-1.72376633e-04],\n",
              "        [-1.06334686e-04],\n",
              "        [ 1.86872482e-03],\n",
              "        [-3.55958939e-04],\n",
              "        [-5.12599945e-05],\n",
              "        [-5.95808029e-04],\n",
              "        [-1.51395798e-04],\n",
              "        [-2.83718109e-05],\n",
              "        [-5.38587570e-04],\n",
              "        [-4.70399857e-04],\n",
              "        [ 2.12907791e-04],\n",
              "        [-1.11103058e-04],\n",
              "        [-5.92470169e-04],\n",
              "        [ 5.15937805e-04],\n",
              "        [-4.14371490e-04],\n",
              "        [-5.74827194e-04],\n",
              "        [-5.28573990e-04],\n",
              "        [-3.15904617e-04],\n",
              "        [-5.35488129e-04],\n",
              "        [-1.07288361e-04],\n",
              "        [-6.19173050e-04],\n",
              "        [ 5.71727753e-04],\n",
              "        [ 6.72340393e-05],\n",
              "        [-6.11543655e-04],\n",
              "        [-3.45706940e-04],\n",
              "        [-6.15358353e-04],\n",
              "        [-5.78641891e-04],\n",
              "        [ 7.62939453e-05],\n",
              "        [-4.50611115e-05],\n",
              "        [-3.33070755e-04],\n",
              "        [-6.05583191e-04],\n",
              "        [-2.81095505e-04],\n",
              "        [-3.36170197e-04],\n",
              "        [-5.71966171e-04],\n",
              "        [ 3.48091125e-04],\n",
              "        [ 2.35319138e-04],\n",
              "        [-6.18457794e-04],\n",
              "        [ 1.06525421e-03],\n",
              "        [-4.16994095e-04],\n",
              "        [-4.54425812e-04],\n",
              "        [-6.20841980e-04],\n",
              "        [-4.23192978e-04],\n",
              "        [-4.83989716e-04],\n",
              "        [-3.66687775e-04],\n",
              "        [-2.71797180e-04],\n",
              "        [ 2.11977959e-03],\n",
              "        [ 1.59692764e-03],\n",
              "        [-1.66654587e-04],\n",
              "        [-5.02109528e-04],\n",
              "        [ 6.21318817e-04],\n",
              "        [-5.12599945e-04],\n",
              "        [-6.01291656e-04],\n",
              "        [ 2.06661224e-03],\n",
              "        [-2.84671783e-04],\n",
              "        [ 6.53028488e-04],\n",
              "        [ 1.62863731e-03],\n",
              "        [-2.18153000e-04],\n",
              "        [-5.81741333e-04],\n",
              "        [-3.43084335e-04],\n",
              "        [-4.06980515e-04],\n",
              "        [-5.09500504e-04],\n",
              "        [ 9.82046127e-04],\n",
              "        [ 2.02965736e-03],\n",
              "        [-2.72989273e-04],\n",
              "        [-6.17742538e-04],\n",
              "        [-4.71591949e-04],\n",
              "        [ 1.24907494e-03],\n",
              "        [-5.10692596e-04],\n",
              "        [-5.15460968e-04],\n",
              "        [-5.86271286e-04],\n",
              "        [-6.07728958e-04],\n",
              "        [-9.05990601e-05],\n",
              "        [-6.07013702e-04],\n",
              "        [-6.20126724e-04],\n",
              "        [-5.44071198e-04],\n",
              "        [ 1.72615051e-04],\n",
              "        [ 5.96046448e-06],\n",
              "        [ 7.84873962e-04],\n",
              "        [-1.78813934e-04],\n",
              "        [ 1.37329102e-04],\n",
              "        [ 1.57523155e-03],\n",
              "        [-5.40733337e-04],\n",
              "        [ 7.71522522e-04],\n",
              "        [-6.19888306e-04],\n",
              "        [ 9.39369202e-05],\n",
              "        [-5.33103943e-04],\n",
              "        [-1.04188919e-04],\n",
              "        [ 5.46932220e-04],\n",
              "        [-6.14881516e-04],\n",
              "        [-5.52177429e-04],\n",
              "        [-6.16550446e-04],\n",
              "        [-3.88622284e-05],\n",
              "        [-5.68628311e-04],\n",
              "        [ 1.85751915e-03],\n",
              "        [ 1.80006027e-04],\n",
              "        [ 1.25813484e-03],\n",
              "        [-5.54800034e-04],\n",
              "        [-6.05583191e-04],\n",
              "        [ 1.08003616e-03],\n",
              "        [-5.50508499e-04],\n",
              "        [-6.77108765e-05],\n",
              "        [-4.92334366e-04],\n",
              "        [-4.67061996e-04],\n",
              "        [-6.18934631e-04],\n",
              "        [ 5.76019287e-04],\n",
              "        [-5.99384308e-04],\n",
              "        [-2.32696533e-04],\n",
              "        [-2.35795975e-04],\n",
              "        [ 1.67322159e-03],\n",
              "        [-6.12020493e-04],\n",
              "        [-1.30653381e-04],\n",
              "        [-5.93662262e-04],\n",
              "        [-6.09159470e-04],\n",
              "        [-5.35011292e-04],\n",
              "        [-1.39713287e-04],\n",
              "        [ 1.45006180e-03],\n",
              "        [-5.79833984e-04],\n",
              "        [-5.46455383e-04],\n",
              "        [ 4.22000885e-05],\n",
              "        [ 3.20434570e-04],\n",
              "        [-6.01530075e-04],\n",
              "        [-6.08682632e-04],\n",
              "        [-5.32150269e-04],\n",
              "        [-4.11510468e-04],\n",
              "        [-5.96284866e-04]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}