{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNs for 1D Burgers Equation (TF2.0).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "A1Mj-RBCn8MZ",
        "GkimJNtepkKi",
        "dOPzdkKsJzA4",
        "OTxvp1nJGDeb",
        "QGd5zVtoxAqt",
        "bXtJ5GiaxAqw",
        "fGrMDRc3w1ex",
        "rRGW4IW0w1e0"
      ],
      "include_colab_link": true
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:light",
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waltbt/ODE/blob/master/PINNs_for_1D_Burgers_Equation_(TF2_0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A1Mj-RBCn8MZ"
      },
      "source": [
        "# 0. Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Al80f-lPoJjh"
      },
      "source": [
        "## Getting the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r3iHOMsdnNiq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ea3a7ee4-4a03-49ab-92aa-53ec7bcb8524"
      },
      "source": [
        "!git clone https://github.com/maziarraissi/PINNs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PINNs'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 741 (delta 1), reused 2 (delta 1), pack-reused 736\u001b[K\n",
            "Receiving objects: 100% (741/741), 474.48 MiB | 16.96 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n",
            "Checking out files: 100% (561/561), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gtoc_dXgoOZq"
      },
      "source": [
        "## Setting up modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kNMtDjXkFHaN"
      },
      "source": [
        "TeX packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TaZCKcDsEVRP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54fffc13-d7b5-4711-e585-f3da9aed848f"
      },
      "source": [
        "!sudo apt-get -qq install texlive-fonts-recommended texlive-fonts-extra dvipng"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 86.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../04-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../05-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../06-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../07-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../08-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../09-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../10-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../11-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../12-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../13-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.12_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../14-libgs9_9.26~dfsg+0-0ubuntu0.18.04.12_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../15-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../16-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../17-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../18-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.12_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
            "Selecting previously unselected package dvipng.\n",
            "Preparing to unpack .../19-dvipng_1.15-1_amd64.deb ...\n",
            "Unpacking dvipng (1.15-1) ...\n",
            "Selecting previously unselected package fonts-adf-accanthis.\n",
            "Preparing to unpack .../20-fonts-adf-accanthis_0.20110505-1_all.deb ...\n",
            "Unpacking fonts-adf-accanthis (0.20110505-1) ...\n",
            "Selecting previously unselected package fonts-adf-berenis.\n",
            "Preparing to unpack .../21-fonts-adf-berenis_0.20110505-1_all.deb ...\n",
            "Unpacking fonts-adf-berenis (0.20110505-1) ...\n",
            "Selecting previously unselected package fonts-adf-gillius.\n",
            "Preparing to unpack .../22-fonts-adf-gillius_0.20110505-1_all.deb ...\n",
            "Unpacking fonts-adf-gillius (0.20110505-1) ...\n",
            "Selecting previously unselected package fonts-adf-universalis.\n",
            "Preparing to unpack .../23-fonts-adf-universalis_0.20110505-1_all.deb ...\n",
            "Unpacking fonts-adf-universalis (0.20110505-1) ...\n",
            "Selecting previously unselected package fonts-cabin.\n",
            "Preparing to unpack .../24-fonts-cabin_1.5-2_all.deb ...\n",
            "Unpacking fonts-cabin (1.5-2) ...\n",
            "Selecting previously unselected package fonts-comfortaa.\n",
            "Preparing to unpack .../25-fonts-comfortaa_3.001-2_all.deb ...\n",
            "Unpacking fonts-comfortaa (3.001-2) ...\n",
            "Selecting previously unselected package fonts-croscore.\n",
            "Preparing to unpack .../26-fonts-croscore_20171026-2_all.deb ...\n",
            "Unpacking fonts-croscore (20171026-2) ...\n",
            "Selecting previously unselected package fonts-crosextra-caladea.\n",
            "Preparing to unpack .../27-fonts-crosextra-caladea_20130214-2_all.deb ...\n",
            "Unpacking fonts-crosextra-caladea (20130214-2) ...\n",
            "Selecting previously unselected package fonts-crosextra-carlito.\n",
            "Preparing to unpack .../28-fonts-crosextra-carlito_20130920-1_all.deb ...\n",
            "Unpacking fonts-crosextra-carlito (20130920-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../29-fonts-dejavu-core_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../30-fonts-dejavu-extra_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
            "Selecting previously unselected package fonts-ebgaramond.\n",
            "Preparing to unpack .../31-fonts-ebgaramond_0.016-1_all.deb ...\n",
            "Unpacking fonts-ebgaramond (0.016-1) ...\n",
            "Selecting previously unselected package fonts-ebgaramond-extra.\n",
            "Preparing to unpack .../32-fonts-ebgaramond-extra_0.016-1_all.deb ...\n",
            "Unpacking fonts-ebgaramond-extra (0.016-1) ...\n",
            "Selecting previously unselected package fonts-font-awesome.\n",
            "Preparing to unpack .../33-fonts-font-awesome_4.7.0~dfsg-3_all.deb ...\n",
            "Unpacking fonts-font-awesome (4.7.0~dfsg-3) ...\n",
            "Selecting previously unselected package fonts-freefont-otf.\n",
            "Preparing to unpack .../34-fonts-freefont-otf_20120503-7_all.deb ...\n",
            "Unpacking fonts-freefont-otf (20120503-7) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../35-fonts-freefont-ttf_20120503-7_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-7) ...\n",
            "Selecting previously unselected package fonts-gfs-artemisia.\n",
            "Preparing to unpack .../36-fonts-gfs-artemisia_1.1-5_all.deb ...\n",
            "Unpacking fonts-gfs-artemisia (1.1-5) ...\n",
            "Selecting previously unselected package fonts-gfs-complutum.\n",
            "Preparing to unpack .../37-fonts-gfs-complutum_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-complutum (1.1-6) ...\n",
            "Selecting previously unselected package fonts-gfs-didot.\n",
            "Preparing to unpack .../38-fonts-gfs-didot_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-didot (1.1-6) ...\n",
            "Selecting previously unselected package fonts-gfs-neohellenic.\n",
            "Preparing to unpack .../39-fonts-gfs-neohellenic_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-neohellenic (1.1-6) ...\n",
            "Selecting previously unselected package fonts-gfs-olga.\n",
            "Preparing to unpack .../40-fonts-gfs-olga_1.1-5_all.deb ...\n",
            "Unpacking fonts-gfs-olga (1.1-5) ...\n",
            "Selecting previously unselected package fonts-gfs-solomos.\n",
            "Preparing to unpack .../41-fonts-gfs-solomos_1.1-5_all.deb ...\n",
            "Unpacking fonts-gfs-solomos (1.1-5) ...\n",
            "Selecting previously unselected package fonts-go.\n",
            "Preparing to unpack .../42-fonts-go_0~20161116-1_all.deb ...\n",
            "Unpacking fonts-go (0~20161116-1) ...\n",
            "Selecting previously unselected package fonts-junicode.\n",
            "Preparing to unpack .../43-fonts-junicode_1.001-2_all.deb ...\n",
            "Unpacking fonts-junicode (1.001-2) ...\n",
            "Selecting previously unselected package fonts-linuxlibertine.\n",
            "Preparing to unpack .../44-fonts-linuxlibertine_5.3.0-4_all.deb ...\n",
            "Unpacking fonts-linuxlibertine (5.3.0-4) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../45-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package fonts-lobster.\n",
            "Preparing to unpack .../46-fonts-lobster_2.0-2_all.deb ...\n",
            "Unpacking fonts-lobster (2.0-2) ...\n",
            "Selecting previously unselected package fonts-lobstertwo.\n",
            "Preparing to unpack .../47-fonts-lobstertwo_2.0-2_all.deb ...\n",
            "Unpacking fonts-lobstertwo (2.0-2) ...\n",
            "Selecting previously unselected package fonts-noto-hinted.\n",
            "Preparing to unpack .../48-fonts-noto-hinted_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-hinted (20171026-2) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../49-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package fonts-oflb-asana-math.\n",
            "Preparing to unpack .../50-fonts-oflb-asana-math_000.907-6_all.deb ...\n",
            "Unpacking fonts-oflb-asana-math (000.907-6) ...\n",
            "Selecting previously unselected package fonts-open-sans.\n",
            "Preparing to unpack .../51-fonts-open-sans_1.11-1_all.deb ...\n",
            "Unpacking fonts-open-sans (1.11-1) ...\n",
            "Selecting previously unselected package fonts-roboto-hinted.\n",
            "Preparing to unpack .../52-fonts-roboto-hinted_2%3a0~20160106-2_all.deb ...\n",
            "Unpacking fonts-roboto-hinted (2:0~20160106-2) ...\n",
            "Selecting previously unselected package fonts-sil-gentium.\n",
            "Preparing to unpack .../53-fonts-sil-gentium_20081126%3a1.03-2_all.deb ...\n",
            "Unpacking fonts-sil-gentium (20081126:1.03-2) ...\n",
            "Selecting previously unselected package fonts-sil-gentium-basic.\n",
            "Preparing to unpack .../54-fonts-sil-gentium-basic_1.102-1_all.deb ...\n",
            "Unpacking fonts-sil-gentium-basic (1.102-1) ...\n",
            "Selecting previously unselected package fonts-sil-gentiumplus.\n",
            "Preparing to unpack .../55-fonts-sil-gentiumplus_5.000-2_all.deb ...\n",
            "Unpacking fonts-sil-gentiumplus (5.000-2) ...\n",
            "Selecting previously unselected package fonts-sil-gentiumplus-compact.\n",
            "Preparing to unpack .../56-fonts-sil-gentiumplus-compact_5.000-2_all.deb ...\n",
            "Unpacking fonts-sil-gentiumplus-compact (5.000-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../57-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../58-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../59-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../60-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../61-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../62-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../63-ruby2.5_2.5.1-1ubuntu1.6_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.6) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../64-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../65-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../66-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../67-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../68-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../69-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../70-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../71-libruby2.5_2.5.1-1ubuntu1.6_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.6) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../72-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../73-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../74-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../75-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-fonts-extra.\n",
            "Preparing to unpack .../76-texlive-fonts-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-fonts-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package fonts-stix.\n",
            "Preparing to unpack .../77-fonts-stix_1.1.1-4_all.deb ...\n",
            "Unpacking fonts-stix (1.1.1-4) ...\n",
            "Selecting previously unselected package texlive-fonts-extra-links.\n",
            "Preparing to unpack .../78-texlive-fonts-extra-links_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-fonts-extra-links (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../79-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../80-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../81-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../82-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../83-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../84-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../85-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-gfs-neohellenic (1.1-6) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up fonts-stix (1.1.1-4) ...\n",
            "Setting up fonts-comfortaa (3.001-2) ...\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up fonts-dejavu-core (2.37-1) ...\n",
            "Setting up fonts-linuxlibertine (5.3.0-4) ...\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up fonts-oflb-asana-math (000.907-6) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up fonts-lobster (2.0-2) ...\n",
            "Setting up fonts-gfs-solomos (1.1-5) ...\n",
            "Setting up fonts-adf-accanthis (0.20110505-1) ...\n",
            "Setting up fonts-freefont-otf (20120503-7) ...\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-open-sans (1.11-1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up fonts-crosextra-carlito (20130920-1) ...\n",
            "Setting up fonts-ebgaramond-extra (0.016-1) ...\n",
            "Setting up fonts-font-awesome (4.7.0~dfsg-3) ...\n",
            "Setting up fonts-junicode (1.001-2) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up fonts-gfs-complutum (1.1-6) ...\n",
            "Setting up fonts-cabin (1.5-2) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up fonts-sil-gentiumplus-compact (5.000-2) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up fonts-adf-gillius (0.20110505-1) ...\n",
            "Setting up fonts-crosextra-caladea (20130214-2) ...\n",
            "Setting up fonts-noto-hinted (20171026-2) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up fonts-ebgaramond (0.016-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up fonts-croscore (20171026-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up fonts-adf-berenis (0.20110505-1) ...\n",
            "Setting up fonts-adf-universalis (0.20110505-1) ...\n",
            "Setting up fonts-sil-gentiumplus (5.000-2) ...\n",
            "Setting up fonts-gfs-didot (1.1-6) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up fonts-gfs-artemisia (1.1-5) ...\n",
            "Setting up fonts-dejavu-extra (2.37-1) ...\n",
            "Setting up fonts-freefont-ttf (20120503-7) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up fonts-go (0~20161116-1) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up fonts-sil-gentium (20081126:1.03-2) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
            "Setting up fonts-sil-gentium-basic (1.102-1) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-gfs-olga (1.1-5) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up fonts-roboto-hinted (2:0~20160106-2) ...\n",
            "Setting up fonts-lobstertwo (2.0-2) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up texlive-fonts-extra-links (2017.20180305-2) ...\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-fonts-extra (2017.20180305-2) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up dvipng (1.15-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.6) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.6) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Otc4Ap7qFMlf"
      },
      "source": [
        "Pip modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "srpq4aQNoQ1E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "72e85fd4-96ea-405f-ba71-51e5ec94beb0"
      },
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow-gpu \n",
        "!pip install pyDOE"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Successfully uninstalled tensorflow-2.2.0\n",
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bf/c28971266ca854a64f4b26f07c4112ddd61f30b4d1f18108b954a746f8ea/tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.30.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (47.3.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.2.0\n",
            "Collecting pyDOE\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/ac/91fe4c039e2744466621343d3b8af4a485193ed0aab53af5b1db03be0989/pyDOE-0.3.8.zip\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyDOE) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyDOE) (1.4.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-cp36-none-any.whl size=18178 sha256=998ab72d7285eec0a88251cf5ea2ec5188b903367b16a646e6998cb95e8fc39e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/c8/58/a6493bd415e8ba5735082b5e0c096d7c1f2933077a8ce34544\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ksKujMvUFRNW"
      },
      "source": [
        "## Imports, config, and utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEDn2fqlqctT",
        "lines_to_next_cell": 2,
        "colab": {}
      },
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ODGREPvZpqUz"
      },
      "source": [
        "burgersutil.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FgPvqJiYFnYG",
        "lines_to_next_cell": 0,
        "colab": {}
      },
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyDOE import lhs\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
        "repoPath = os.path.join(\".\", \"PINNs\")\n",
        "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
        "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
        "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
        "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
        "\n",
        "sys.path.insert(0, utilsPath)\n",
        "from plotting import newfig, savefig\n",
        "\n",
        "def prep_data(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
        "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
        "    data = scipy.io.loadmat(path)\n",
        "\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = data['t'].flatten()[:,None] # T x 1\n",
        "    x = data['x'].flatten()[:,None] # N x 1\n",
        "\n",
        "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.real(data['usol']).T # T x N\n",
        "\n",
        "    if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
        "      dt = t[idx_t_1] - t[idx_t_0]\n",
        "      idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "        \n",
        "      # Boudanry data\n",
        "      x_1 = np.vstack((lb, ub))\n",
        "      \n",
        "      # Test data\n",
        "      x_star = x\n",
        "      u_star = Exact_u[idx_t_1,:]\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
        "      IRK_times = tmp[q**2+q:]\n",
        "\n",
        "      return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
        "\n",
        "    # Meshing x and t in 2D (256,100)\n",
        "    X, T = np.meshgrid(x,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = Exact_u.flatten()[:,None]\n",
        "                \n",
        "    # Noiseless data TODO: add support for noisy data    \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "\n",
        "    if N_0 != None and N_1 != None:\n",
        "      Exact_u = Exact_u.T\n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "          \n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
        "      x_1 = x[idx_x,:]\n",
        "      u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
        "      u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
        "      \n",
        "      dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
        "      q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
        "      IRK_alpha = weights[0:-1,:]\n",
        "      IRK_beta = weights[-1:,:] \n",
        "      return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
        "\n",
        "    if N_f == None:\n",
        "      lb = X_star.min(axis=0)\n",
        "      ub = X_star.max(axis=0) \n",
        "      return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
        "\n",
        "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0) \n",
        "    # Getting the initial conditions (t=0)\n",
        "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "    uu1 = Exact_u[0:1,:].T\n",
        "    # Getting the lowest boundary conditions (x=-1) \n",
        "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "    uu2 = Exact_u[:,0:1]\n",
        "    # Getting the highest boundary conditions (x=1) \n",
        "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "    uu3 = Exact_u[:,-1:]\n",
        "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
        "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "    u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "    # Generating the x and t collocation points for f, with each having a N_f size\n",
        "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
        "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
        "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
        "    X_u_train = X_u_train[idx,:]\n",
        "    # Getting the corresponding u_train\n",
        "    u_train = u_train [idx,:]\n",
        "\n",
        "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb\n",
        "\n",
        "class Logger(object):\n",
        "  def __init__(self, frequency=10):\n",
        "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "    self.start_time = time.time()\n",
        "    self.frequency = frequency\n",
        "\n",
        "  def __get_elapsed(self):\n",
        "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
        "\n",
        "  def __get_error_u(self):\n",
        "    return self.error_fn()\n",
        "\n",
        "  def set_error_fn(self, error_fn):\n",
        "    self.error_fn = error_fn\n",
        "  \n",
        "  def log_train_start(self, model):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "    self.model = model\n",
        "    print(self.model.summary())\n",
        "\n",
        "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "    if epoch % self.frequency == 0:\n",
        "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "  def log_train_opt(self, name):\n",
        "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
        "    print(f\"—— Starting {name} optimization ——\")\n",
        "\n",
        "  def log_train_end(self, epoch, custom=\"\"):\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, file=None):\n",
        "\n",
        "  # Interpolating the results on the whole (x,t) domain.\n",
        "  # griddata(points, values, points at which to interpolate, method)\n",
        "  U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
        "\n",
        "  # Creating the figures\n",
        "  fig, ax = newfig(1.0, 1.1)\n",
        "  ax.axis('off')\n",
        "\n",
        "  ####### Row 0: u(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "  h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "\n",
        "  ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
        "\n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "\n",
        "  ####### Row 1: u(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(1, 3)\n",
        "  gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])\n",
        "  ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 2])\n",
        "  ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])    \n",
        "  ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  if file != None:\n",
        "    savefig(file)\n",
        "\n",
        "def plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t, file=None):\n",
        "  fig, ax = newfig(1.0, 1.2)\n",
        "  ax.axis('off')\n",
        "  \n",
        "  ####### Row 0: h(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/2 + 0.1, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "  \n",
        "  h = ax.imshow(Exact_u.T, interpolation='nearest', cmap='rainbow', \n",
        "                extent=[t.min(), t.max(), x_star.min(), x_star.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "      \n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  \n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  leg = ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "  \n",
        "  \n",
        "  ####### Row 1: h(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(1, 2)\n",
        "  gs1.update(top=1-1/2-0.05, bottom=0.15, left=0.15, right=0.85, wspace=0.5)\n",
        "  \n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[idx_t_0,:], 'b-', linewidth = 2) \n",
        "  ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')      \n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = %.2f$' % (t[idx_t_0]), fontsize = 10)\n",
        "  ax.set_xlim([lb-0.1, ub+0.1])\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.8, -0.3), ncol=2, frameon=False)\n",
        "\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x, Exact_u[idx_t_1,:], 'b-', linewidth = 2, label = 'Exact') \n",
        "  ax.plot(x_star, u_1_pred, 'r--', linewidth = 2, label = 'Prediction')      \n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = %.2f$' % (t[idx_t_1]), fontsize = 10)    \n",
        "  ax.set_xlim([lb-0.1, ub+0.1])\n",
        "  \n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.1, -0.3), ncol=2, frameon=False)\n",
        "    \n",
        "  plt.show()\n",
        "\n",
        "  if file != None:\n",
        "    savefig(file)\n",
        "\n",
        "\n",
        "def plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
        "  ub, lb, u_1_pred, Exact, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy,\n",
        "  x, t, file=None):  \n",
        "  fig, ax = newfig(1.0, 1.5)\n",
        "  ax.axis('off')\n",
        "  \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3+0.05, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "      \n",
        "  h = ax.imshow(Exact, interpolation='nearest', cmap='rainbow',\n",
        "                extent=[t_star.min(),t_star.max(), lb[0], ub[0]],\n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "  \n",
        "  line = np.linspace(x_star.min(), x_star.max(), 2)[:,None]\n",
        "  ax.plot(t_star[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1.0)\n",
        "  ax.plot(t_star[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1.0)    \n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "  \n",
        "  gs1 = gridspec.GridSpec(1, 2)\n",
        "  gs1.update(top=1-1/3-0.1, bottom=1-2/3, left=0.15, right=0.85, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x_star,Exact[:,idx_t_0][:,None], 'b', linewidth = 2, label = 'Exact')\n",
        "  ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_0], u_0.shape[0]), fontsize = 10)\n",
        "  \n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x_star,Exact[:,idx_t_1][:,None], 'b', linewidth = 2, label = 'Exact')\n",
        "  ax.plot(x_1, u_1, 'rx', linewidth = 2, label = 'Data')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_1], u_1.shape[0]), fontsize = 10)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(-0.3, -0.3), ncol=2, frameon=False)\n",
        "  \n",
        "  gs2 = gridspec.GridSpec(1, 2)\n",
        "  gs2.update(top=1-2/3-0.05, bottom=0, left=0.15, right=0.85, wspace=0.0)\n",
        "  \n",
        "  ax = plt.subplot(gs2[0, 0])\n",
        "  ax.axis('off')\n",
        "  nu = 0.01/np.pi\n",
        "  s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x + %.6f u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & ' % (nu)\n",
        "  s2 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "  s3 = r'Identified PDE (1\\% noise) & '\n",
        "  s4 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "  s5 = r'\\end{tabular}$'\n",
        "  s = s1+s2+s3+s4+s5\n",
        "  ax.text(-0.1,0.2,s)\n",
        "  plt.show()\n",
        "\n",
        "def plot_ide_cont_results(X_star, u_pred, X_u_train, u_train,\n",
        "  Exact_u, X, T, x, t, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy):\n",
        "    fig, ax = newfig(1.0, 1.4)\n",
        "    ax.axis('off')\n",
        "\n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "    \n",
        "    ####### Row 0: u(t,x) ##################    \n",
        "    gs0 = gridspec.GridSpec(1, 2)\n",
        "    gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs0[:, :])\n",
        "    \n",
        "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                  origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "    \n",
        "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
        "    \n",
        "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
        "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 1: u(t,x) slices ##################    \n",
        "    gs1 = gridspec.GridSpec(1, 3)\n",
        "    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 0])\n",
        "    ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')    \n",
        "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 1])\n",
        "    ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 2])\n",
        "    ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])    \n",
        "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 3: Identified PDE ##################    \n",
        "    gs2 = gridspec.GridSpec(1, 3)\n",
        "    gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
        "    \n",
        "    ax = plt.subplot(gs2[:, :])\n",
        "    ax.axis('off')\n",
        "    s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
        "    s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "    s3 = r'Identified PDE (1\\% noise) & '\n",
        "    s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "    s5 = r'\\end{tabular}$'\n",
        "    s = s1+s2+s3+s4+s5\n",
        "    ax.text(0.1,0.1,s)\n",
        "    plt.show()\n",
        "    # savefig('./figures/Burgers_identification')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S7Azv7DZp0M-"
      },
      "source": [
        "custom_lbfgs.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjvMe1Avpvh9",
        "lines_to_next_cell": 0,
        "colab": {}
      },
      "source": [
        "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yOT-E8C4oAJN"
      },
      "source": [
        "# 1. Continuous Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Qrt3ECzcLHp"
      },
      "source": [
        "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
        "\n",
        "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
        "\n",
        "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n8CHqrpafela"
      },
      "source": [
        "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
        "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
        "\n",
        "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
        "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
        "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C9Ko6L87J2v_"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jwWhiecUqbAo",
        "lines_to_next_cell": 2,
        "colab": {}
      },
      "source": [
        "\n",
        "# Data size on the solution u\n",
        "N_u = 52\n",
        "# Collocation points size, where we’ll check for f = 0\n",
        "N_f = 10024\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 100\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate=0.1,\n",
        "  beta_1=0.99,\n",
        "  epsilon=1e-1)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 2000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GkimJNtepkKi"
      },
      "source": [
        "## PINN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j3wUjV9oe7V9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KVm9UCvvlyY_",
        "colab": {}
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
        "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
        "    self.u_model = tf.keras.Sequential()\n",
        "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.u_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.nu = nu\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    # Separating the collocation coordinates\n",
        "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
        "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
        "    \n",
        "  # Defining custom loss\n",
        "  def __loss(self, u, u_pred):\n",
        "    f_pred = self.f_model()\n",
        "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
        "      tf.reduce_mean(tf.square(f_pred))\n",
        "\n",
        "  def __grad(self, X, u):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(u, self.u_model(X))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.u_model.trainable_variables\n",
        "    return var\n",
        "\n",
        "  # The actual PINN\n",
        "  def f_model(self):\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(self.x_f)\n",
        "      tape.watch(self.t_f)\n",
        "      # Packing together the inputs\n",
        "      X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
        "\n",
        "      # Getting the prediction\n",
        "      u = self.u_model(X_f)\n",
        "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
        "      u_x = tape.gradient(u, self.x_f)\n",
        "    \n",
        "    # Getting the other derivatives\n",
        "    u_xx = tape.gradient(u_x, self.x_f)\n",
        "    u_t = tape.gradient(u, self.t_f)\n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "\n",
        "    nu = self.get_params(numpy=True)\n",
        "\n",
        "    # Buidling the PINNs\n",
        "    return u_t + u*u_x - nu*u_xx\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    return self.nu\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = []\n",
        "    for layer in self.u_model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  def summary(self):\n",
        "    return self.u_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(X_u, u)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
        "      self.logger.log_train_epoch(epoch, loss_value)\n",
        "    \n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.__loss(u, self.u_model(X_u))\n",
        "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "\n",
        "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    u_star = self.u_model(X_star)\n",
        "    f_star = self.f_model()\n",
        "    return u_star, f_star"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8FzMd65dpoHo"
      },
      "source": [
        "## Training and plotting the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q7Z36HUmzib",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   x is the (256,1) it is the dsicrete x axis - numbers range from -1 to 1\n",
        "*   t is the (100,10) and is the time axis - numbers range from 0 to 1\n",
        "*   X is (100,256) is a mesh of the values of x and t - Why?\n",
        "*   T is (100,256) is a mesh of the values of x and t - Why?\n",
        "*   Exact_u is (100,256) - it is the solution directly from the data file\n",
        "*   X_star is (25600, 2) - is made of pairs of x and t for every point in the meshes\n",
        "*   u_star is (25600, 1) - flattened Exact_u\n",
        "*   X_u_train is (50, 2) - (N_u,2) - it is built from X_star - N_u randomly selected points\n",
        "*   u_train is (50, 1) - (N_u,1) - it is built from u_star - N_u randomly selected points\n",
        "*   X_f is (10000, 2) -  (N_f,2) - x and t pairs selected using latin hyper cube...  These are are colocation points\n",
        "*   ub is [ub of x, ub of t]\n",
        "*   lb is [lb of x, lb of t]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "668In9OdbnxJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b20c025a-48f0-4330-8a2a-ba77a4187d4c"
      },
      "source": [
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "  X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
        "print(x.shape)\n",
        "# print(x)\n",
        "print(t.shape)\n",
        "# print(t)\n",
        "print(X.shape)\n",
        "# print(X)\n",
        "print(T.shape)\n",
        "# print(T)\n",
        "print(Exact_u.shape)\n",
        "print(X_star.shape)\n",
        "print(u_star.shape)\n",
        "print(X_u_train.shape)\n",
        "print(u_train.shape)\n",
        "print(X_f.shape)\n",
        "print(ub)\n",
        "print(lb)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 1)\n",
            "(100, 1)\n",
            "(100, 256)\n",
            "(100, 256)\n",
            "(100, 256)\n",
            "(25600, 2)\n",
            "(25600, 1)\n",
            "(52, 2)\n",
            "(52, 1)\n",
            "(10024, 2)\n",
            "[1.   0.99]\n",
            "[-1.  0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SEKkpHvApf46",
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22491dfd-47a5-494c-9855-7d974041246d"
      },
      "source": [
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
        "def error():\n",
        "  u_pred, _ = pinn.predict(X_star)\n",
        "  return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_pred, f_pred = pinn.predict(X_star)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "50\n",
            "10000\n",
            "100\n",
            "2\n",
            "TensorFlow version: 2.2.0\n",
            "Eager execution: True\n",
            "WARNING:tensorflow:From <ipython-input-5-2616730e37e2>:131: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 20)                60        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 3,021\n",
            "Trainable params: 3,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "tf_epoch =      0  elapsed = 00:10  loss = 2.9103e-01  error = 8.8958e-01  \n",
            "tf_epoch =     10  elapsed = 00:11  loss = 2.1053e-01  error = 7.8298e-01  \n",
            "tf_epoch =     20  elapsed = 00:11  loss = 1.8826e-01  error = 6.7527e-01  \n",
            "tf_epoch =     30  elapsed = 00:11  loss = 1.6335e-01  error = 6.6386e-01  \n",
            "tf_epoch =     40  elapsed = 00:12  loss = 1.5234e-01  error = 6.1549e-01  \n",
            "tf_epoch =     50  elapsed = 00:12  loss = 1.5271e-01  error = 5.5059e-01  \n",
            "tf_epoch =     60  elapsed = 00:13  loss = 1.5121e-01  error = 6.1824e-01  \n",
            "tf_epoch =     70  elapsed = 00:13  loss = 1.4983e-01  error = 5.9295e-01  \n",
            "tf_epoch =     80  elapsed = 00:13  loss = 1.4985e-01  error = 6.3547e-01  \n",
            "tf_epoch =     90  elapsed = 00:14  loss = 1.4213e-01  error = 5.9022e-01  \n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 00:15  loss = 1.0505e-01  error = 5.0906e-01  \n",
            "nt_epoch =     20  elapsed = 00:15  loss = 8.8612e-02  error = 5.1689e-01  \n",
            "nt_epoch =     30  elapsed = 00:16  loss = 7.3398e-02  error = 4.9323e-01  \n",
            "nt_epoch =     40  elapsed = 00:17  loss = 6.4595e-02  error = 4.6714e-01  \n",
            "nt_epoch =     50  elapsed = 00:17  loss = 6.1894e-02  error = 4.5988e-01  \n",
            "nt_epoch =     60  elapsed = 00:18  loss = 5.7564e-02  error = 4.4973e-01  \n",
            "nt_epoch =     70  elapsed = 00:19  loss = 5.4513e-02  error = 4.4184e-01  \n",
            "nt_epoch =     80  elapsed = 00:19  loss = 4.9475e-02  error = 4.2403e-01  \n",
            "nt_epoch =     90  elapsed = 00:20  loss = 4.4024e-02  error = 3.8747e-01  \n",
            "nt_epoch =    100  elapsed = 00:21  loss = 4.0810e-02  error = 3.7187e-01  \n",
            "nt_epoch =    110  elapsed = 00:21  loss = 3.5824e-02  error = 3.4513e-01  \n",
            "nt_epoch =    120  elapsed = 00:22  loss = 3.2404e-02  error = 3.1742e-01  \n",
            "nt_epoch =    130  elapsed = 00:23  loss = 2.9551e-02  error = 2.9874e-01  \n",
            "nt_epoch =    140  elapsed = 00:23  loss = 2.7433e-02  error = 2.9271e-01  \n",
            "nt_epoch =    150  elapsed = 00:24  loss = 2.5423e-02  error = 2.8217e-01  \n",
            "nt_epoch =    160  elapsed = 00:25  loss = 2.2883e-02  error = 2.5823e-01  \n",
            "nt_epoch =    170  elapsed = 00:25  loss = 1.9170e-02  error = 2.1558e-01  \n",
            "nt_epoch =    180  elapsed = 00:26  loss = 1.5662e-02  error = 1.9111e-01  \n",
            "nt_epoch =    190  elapsed = 00:27  loss = 1.3222e-02  error = 1.7944e-01  \n",
            "nt_epoch =    200  elapsed = 00:27  loss = 1.1648e-02  error = 1.7604e-01  \n",
            "nt_epoch =    210  elapsed = 00:28  loss = 1.0277e-02  error = 1.6847e-01  \n",
            "nt_epoch =    220  elapsed = 00:29  loss = 9.4894e-03  error = 1.6703e-01  \n",
            "nt_epoch =    230  elapsed = 00:29  loss = 8.8035e-03  error = 1.6749e-01  \n",
            "nt_epoch =    240  elapsed = 00:30  loss = 8.4246e-03  error = 1.5995e-01  \n",
            "nt_epoch =    250  elapsed = 00:31  loss = 7.6004e-03  error = 1.4725e-01  \n",
            "nt_epoch =    260  elapsed = 00:31  loss = 7.2133e-03  error = 1.4762e-01  \n",
            "nt_epoch =    270  elapsed = 00:32  loss = 6.8092e-03  error = 1.5090e-01  \n",
            "nt_epoch =    280  elapsed = 00:33  loss = 5.9512e-03  error = 1.3903e-01  \n",
            "nt_epoch =    290  elapsed = 00:34  loss = 5.4524e-03  error = 1.3307e-01  \n",
            "nt_epoch =    300  elapsed = 00:34  loss = 5.1420e-03  error = 1.3108e-01  \n",
            "nt_epoch =    310  elapsed = 00:35  loss = 4.7812e-03  error = 1.2986e-01  \n",
            "nt_epoch =    320  elapsed = 00:36  loss = 4.4667e-03  error = 1.2852e-01  \n",
            "nt_epoch =    330  elapsed = 00:36  loss = 4.1944e-03  error = 1.2731e-01  \n",
            "nt_epoch =    340  elapsed = 00:37  loss = 3.9601e-03  error = 1.2483e-01  \n",
            "nt_epoch =    350  elapsed = 00:38  loss = 3.7756e-03  error = 1.2464e-01  \n",
            "nt_epoch =    360  elapsed = 00:39  loss = 3.5880e-03  error = 1.2334e-01  \n",
            "nt_epoch =    370  elapsed = 00:39  loss = 3.4102e-03  error = 1.2311e-01  \n",
            "nt_epoch =    380  elapsed = 00:40  loss = 3.3009e-03  error = 1.1944e-01  \n",
            "nt_epoch =    390  elapsed = 00:41  loss = 3.1232e-03  error = 1.0642e-01  \n",
            "nt_epoch =    400  elapsed = 00:41  loss = 2.8631e-03  error = 9.1195e-02  \n",
            "nt_epoch =    410  elapsed = 00:42  loss = 2.6054e-03  error = 8.5383e-02  \n",
            "nt_epoch =    420  elapsed = 00:43  loss = 2.4291e-03  error = 7.7459e-02  \n",
            "nt_epoch =    430  elapsed = 00:43  loss = 2.2934e-03  error = 7.6229e-02  \n",
            "nt_epoch =    440  elapsed = 00:44  loss = 2.0948e-03  error = 7.4807e-02  \n",
            "nt_epoch =    450  elapsed = 00:45  loss = 2.0118e-03  error = 7.1188e-02  \n",
            "nt_epoch =    460  elapsed = 00:45  loss = 1.9286e-03  error = 7.0180e-02  \n",
            "nt_epoch =    470  elapsed = 00:46  loss = 1.8614e-03  error = 6.9215e-02  \n",
            "nt_epoch =    480  elapsed = 00:47  loss = 1.7924e-03  error = 6.7903e-02  \n",
            "nt_epoch =    490  elapsed = 00:47  loss = 1.7178e-03  error = 6.7000e-02  \n",
            "nt_epoch =    500  elapsed = 00:48  loss = 1.6495e-03  error = 6.4928e-02  \n",
            "nt_epoch =    510  elapsed = 00:49  loss = 1.5335e-03  error = 6.3241e-02  \n",
            "nt_epoch =    520  elapsed = 00:50  loss = 1.4482e-03  error = 6.2661e-02  \n",
            "nt_epoch =    530  elapsed = 00:50  loss = 1.3891e-03  error = 6.1839e-02  \n",
            "nt_epoch =    540  elapsed = 00:51  loss = 1.3040e-03  error = 6.1327e-02  \n",
            "nt_epoch =    550  elapsed = 00:52  loss = 1.2458e-03  error = 6.1152e-02  \n",
            "nt_epoch =    560  elapsed = 00:52  loss = 1.1861e-03  error = 6.1180e-02  \n",
            "nt_epoch =    570  elapsed = 00:53  loss = 1.1259e-03  error = 6.0384e-02  \n",
            "nt_epoch =    580  elapsed = 00:54  loss = 1.0903e-03  error = 5.9249e-02  \n",
            "nt_epoch =    590  elapsed = 00:54  loss = 1.0370e-03  error = 5.7217e-02  \n",
            "nt_epoch =    600  elapsed = 00:55  loss = 9.8736e-04  error = 5.5837e-02  \n",
            "nt_epoch =    610  elapsed = 00:56  loss = 9.5506e-04  error = 5.4187e-02  \n",
            "nt_epoch =    620  elapsed = 00:57  loss = 9.2219e-04  error = 5.2384e-02  \n",
            "nt_epoch =    630  elapsed = 00:57  loss = 8.7850e-04  error = 4.9219e-02  \n",
            "nt_epoch =    640  elapsed = 00:58  loss = 8.4998e-04  error = 4.8348e-02  \n",
            "nt_epoch =    650  elapsed = 00:59  loss = 8.1036e-04  error = 4.6932e-02  \n",
            "nt_epoch =    660  elapsed = 00:59  loss = 7.8237e-04  error = 4.6102e-02  \n",
            "nt_epoch =    670  elapsed = 01:00  loss = 7.4714e-04  error = 4.3959e-02  \n",
            "nt_epoch =    680  elapsed = 01:01  loss = 7.1922e-04  error = 4.2440e-02  \n",
            "nt_epoch =    690  elapsed = 01:01  loss = 6.8739e-04  error = 3.8603e-02  \n",
            "nt_epoch =    700  elapsed = 01:02  loss = 6.3928e-04  error = 3.3386e-02  \n",
            "nt_epoch =    710  elapsed = 01:03  loss = 5.9436e-04  error = 3.7234e-02  \n",
            "nt_epoch =    720  elapsed = 01:03  loss = 5.5805e-04  error = 3.5497e-02  \n",
            "nt_epoch =    730  elapsed = 01:04  loss = 4.9693e-04  error = 3.9192e-02  \n",
            "nt_epoch =    740  elapsed = 01:05  loss = 4.6938e-04  error = 3.9849e-02  \n",
            "nt_epoch =    750  elapsed = 01:05  loss = 4.3625e-04  error = 3.9035e-02  \n",
            "nt_epoch =    760  elapsed = 01:06  loss = 4.1256e-04  error = 4.1617e-02  \n",
            "nt_epoch =    770  elapsed = 01:07  loss = 3.8653e-04  error = 4.2583e-02  \n",
            "nt_epoch =    780  elapsed = 01:08  loss = 3.6468e-04  error = 4.2757e-02  \n",
            "nt_epoch =    790  elapsed = 01:08  loss = 3.5055e-04  error = 3.8608e-02  \n",
            "nt_epoch =    800  elapsed = 01:09  loss = 3.3505e-04  error = 4.0168e-02  \n",
            "nt_epoch =    810  elapsed = 01:10  loss = 3.2124e-04  error = 3.8176e-02  \n",
            "nt_epoch =    820  elapsed = 01:11  loss = 3.0391e-04  error = 3.6990e-02  \n",
            "nt_epoch =    830  elapsed = 01:11  loss = 2.8930e-04  error = 3.5336e-02  \n",
            "nt_epoch =    840  elapsed = 01:12  loss = 2.7766e-04  error = 3.3789e-02  \n",
            "nt_epoch =    850  elapsed = 01:13  loss = 2.6548e-04  error = 3.1507e-02  \n",
            "nt_epoch =    860  elapsed = 01:13  loss = 2.5381e-04  error = 3.1856e-02  \n",
            "nt_epoch =    870  elapsed = 01:14  loss = 2.4659e-04  error = 3.0710e-02  \n",
            "nt_epoch =    880  elapsed = 01:15  loss = 2.3872e-04  error = 2.8850e-02  \n",
            "nt_epoch =    890  elapsed = 01:15  loss = 2.2950e-04  error = 2.8742e-02  \n",
            "nt_epoch =    900  elapsed = 01:16  loss = 2.1986e-04  error = 2.6951e-02  \n",
            "nt_epoch =    910  elapsed = 01:17  loss = 2.1079e-04  error = 2.7601e-02  \n",
            "nt_epoch =    920  elapsed = 01:18  loss = 2.0906e-04  error = 2.5631e-02  \n",
            "nt_epoch =    930  elapsed = 01:18  loss = 1.9613e-04  error = 2.5648e-02  \n",
            "nt_epoch =    940  elapsed = 01:19  loss = 1.8986e-04  error = 2.4449e-02  \n",
            "nt_epoch =    950  elapsed = 01:20  loss = 1.8235e-04  error = 2.4956e-02  \n",
            "nt_epoch =    960  elapsed = 01:20  loss = 1.7634e-04  error = 2.3086e-02  \n",
            "nt_epoch =    970  elapsed = 01:21  loss = 1.6724e-04  error = 2.1159e-02  \n",
            "nt_epoch =    980  elapsed = 01:22  loss = 1.6119e-04  error = 1.9266e-02  \n",
            "nt_epoch =    990  elapsed = 01:22  loss = 1.5348e-04  error = 1.6521e-02  \n",
            "nt_epoch =   1000  elapsed = 01:23  loss = 1.4862e-04  error = 1.7078e-02  \n",
            "nt_epoch =   1010  elapsed = 01:24  loss = 1.4324e-04  error = 1.5463e-02  \n",
            "nt_epoch =   1020  elapsed = 01:25  loss = 1.3863e-04  error = 1.4465e-02  \n",
            "nt_epoch =   1030  elapsed = 01:25  loss = 1.3328e-04  error = 1.4305e-02  \n",
            "nt_epoch =   1040  elapsed = 01:26  loss = 1.3029e-04  error = 1.4462e-02  \n",
            "nt_epoch =   1050  elapsed = 01:27  loss = 1.2789e-04  error = 1.3599e-02  \n",
            "nt_epoch =   1060  elapsed = 01:27  loss = 1.2338e-04  error = 1.3206e-02  \n",
            "nt_epoch =   1070  elapsed = 01:28  loss = 1.1944e-04  error = 1.2869e-02  \n",
            "nt_epoch =   1080  elapsed = 01:29  loss = 1.1752e-04  error = 1.2693e-02  \n",
            "nt_epoch =   1090  elapsed = 01:30  loss = 1.1497e-04  error = 1.2534e-02  \n",
            "nt_epoch =   1100  elapsed = 01:30  loss = 1.1081e-04  error = 1.2554e-02  \n",
            "nt_epoch =   1110  elapsed = 01:31  loss = 1.0635e-04  error = 1.2134e-02  \n",
            "nt_epoch =   1120  elapsed = 01:32  loss = 1.0263e-04  error = 1.1819e-02  \n",
            "nt_epoch =   1130  elapsed = 01:33  loss = 9.9835e-05  error = 1.1508e-02  \n",
            "nt_epoch =   1140  elapsed = 01:33  loss = 9.5846e-05  error = 1.0953e-02  \n",
            "nt_epoch =   1150  elapsed = 01:34  loss = 9.3546e-05  error = 1.0911e-02  \n",
            "nt_epoch =   1160  elapsed = 01:35  loss = 9.1580e-05  error = 1.1305e-02  \n",
            "nt_epoch =   1170  elapsed = 01:35  loss = 8.9542e-05  error = 1.1428e-02  \n",
            "nt_epoch =   1180  elapsed = 01:36  loss = 8.6877e-05  error = 1.1630e-02  \n",
            "nt_epoch =   1190  elapsed = 01:37  loss = 8.4676e-05  error = 1.1690e-02  \n",
            "nt_epoch =   1200  elapsed = 01:37  loss = 8.3439e-05  error = 1.1799e-02  \n",
            "nt_epoch =   1210  elapsed = 01:38  loss = 8.1960e-05  error = 1.1744e-02  \n",
            "nt_epoch =   1220  elapsed = 01:39  loss = 8.0733e-05  error = 1.1534e-02  \n",
            "nt_epoch =   1230  elapsed = 01:40  loss = 7.9675e-05  error = 1.1224e-02  \n",
            "nt_epoch =   1240  elapsed = 01:40  loss = 7.7835e-05  error = 1.0751e-02  \n",
            "nt_epoch =   1250  elapsed = 01:41  loss = 7.5135e-05  error = 1.0390e-02  \n",
            "nt_epoch =   1260  elapsed = 01:42  loss = 7.3638e-05  error = 1.0313e-02  \n",
            "nt_epoch =   1270  elapsed = 01:43  loss = 7.2018e-05  error = 1.0135e-02  \n",
            "nt_epoch =   1280  elapsed = 01:43  loss = 7.0797e-05  error = 1.0226e-02  \n",
            "nt_epoch =   1290  elapsed = 01:44  loss = 6.9789e-05  error = 1.0405e-02  \n",
            "nt_epoch =   1300  elapsed = 01:45  loss = 6.8597e-05  error = 1.0631e-02  \n",
            "nt_epoch =   1310  elapsed = 01:45  loss = 6.7959e-05  error = 1.0591e-02  \n",
            "nt_epoch =   1320  elapsed = 01:46  loss = 6.6729e-05  error = 1.0828e-02  \n",
            "nt_epoch =   1330  elapsed = 01:47  loss = 6.5471e-05  error = 1.0814e-02  \n",
            "nt_epoch =   1340  elapsed = 01:47  loss = 6.4349e-05  error = 1.0520e-02  \n",
            "nt_epoch =   1350  elapsed = 01:48  loss = 6.3643e-05  error = 1.0186e-02  \n",
            "nt_epoch =   1360  elapsed = 01:49  loss = 6.2030e-05  error = 1.0064e-02  \n",
            "nt_epoch =   1370  elapsed = 01:49  loss = 6.0920e-05  error = 1.0326e-02  \n",
            "nt_epoch =   1380  elapsed = 01:50  loss = 5.9605e-05  error = 1.0770e-02  \n",
            "nt_epoch =   1390  elapsed = 01:51  loss = 5.8638e-05  error = 1.1119e-02  \n",
            "nt_epoch =   1400  elapsed = 01:51  loss = 5.7544e-05  error = 1.0766e-02  \n",
            "nt_epoch =   1410  elapsed = 01:52  loss = 5.6727e-05  error = 1.1171e-02  \n",
            "nt_epoch =   1420  elapsed = 01:53  loss = 5.5921e-05  error = 1.0944e-02  \n",
            "nt_epoch =   1430  elapsed = 01:53  loss = 5.5413e-05  error = 1.1794e-02  \n",
            "nt_epoch =   1440  elapsed = 01:54  loss = 5.4173e-05  error = 1.1373e-02  \n",
            "nt_epoch =   1450  elapsed = 01:55  loss = 5.2974e-05  error = 1.1446e-02  \n",
            "nt_epoch =   1460  elapsed = 01:55  loss = 5.2015e-05  error = 1.1604e-02  \n",
            "nt_epoch =   1470  elapsed = 01:56  loss = 5.0897e-05  error = 1.1569e-02  \n",
            "nt_epoch =   1480  elapsed = 01:57  loss = 5.0208e-05  error = 1.1198e-02  \n",
            "nt_epoch =   1490  elapsed = 01:57  loss = 4.9438e-05  error = 1.0891e-02  \n",
            "nt_epoch =   1500  elapsed = 01:58  loss = 4.8588e-05  error = 1.0895e-02  \n",
            "nt_epoch =   1510  elapsed = 01:59  loss = 4.7937e-05  error = 1.0503e-02  \n",
            "nt_epoch =   1520  elapsed = 02:00  loss = 4.6938e-05  error = 1.0482e-02  \n",
            "nt_epoch =   1530  elapsed = 02:00  loss = 4.6323e-05  error = 1.0464e-02  \n",
            "nt_epoch =   1540  elapsed = 02:01  loss = 4.5833e-05  error = 9.6228e-03  \n",
            "nt_epoch =   1550  elapsed = 02:02  loss = 4.5479e-05  error = 9.8082e-03  \n",
            "nt_epoch =   1560  elapsed = 02:02  loss = 4.5005e-05  error = 9.6368e-03  \n",
            "nt_epoch =   1570  elapsed = 02:03  loss = 4.5117e-05  error = 9.2110e-03  \n",
            "nt_epoch =   1580  elapsed = 02:04  loss = 4.3892e-05  error = 9.8108e-03  \n",
            "nt_epoch =   1590  elapsed = 02:04  loss = 4.3648e-05  error = 9.1712e-03  \n",
            "nt_epoch =   1600  elapsed = 02:05  loss = 4.3406e-05  error = 9.6791e-03  \n",
            "nt_epoch =   1610  elapsed = 02:06  loss = 4.3075e-05  error = 9.7824e-03  \n",
            "nt_epoch =   1620  elapsed = 02:07  loss = 4.2847e-05  error = 1.0388e-02  \n",
            "nt_epoch =   1630  elapsed = 02:07  loss = 4.2437e-05  error = 1.0295e-02  \n",
            "nt_epoch =   1640  elapsed = 02:08  loss = 4.1892e-05  error = 1.0504e-02  \n",
            "nt_epoch =   1650  elapsed = 02:09  loss = 4.1563e-05  error = 1.0468e-02  \n",
            "nt_epoch =   1660  elapsed = 02:10  loss = 4.1193e-05  error = 1.0187e-02  \n",
            "nt_epoch =   1670  elapsed = 02:10  loss = 4.0858e-05  error = 1.0092e-02  \n",
            "nt_epoch =   1680  elapsed = 02:11  loss = 4.0555e-05  error = 1.0223e-02  \n",
            "nt_epoch =   1690  elapsed = 02:12  loss = 4.0142e-05  error = 1.0176e-02  \n",
            "nt_epoch =   1700  elapsed = 02:13  loss = 3.9882e-05  error = 9.9833e-03  \n",
            "nt_epoch =   1710  elapsed = 02:13  loss = 3.9269e-05  error = 1.0373e-02  \n",
            "nt_epoch =   1720  elapsed = 02:14  loss = 3.8949e-05  error = 9.7981e-03  \n",
            "nt_epoch =   1730  elapsed = 02:15  loss = 3.8700e-05  error = 9.5827e-03  \n",
            "nt_epoch =   1740  elapsed = 02:15  loss = 3.8228e-05  error = 9.8259e-03  \n",
            "nt_epoch =   1750  elapsed = 02:16  loss = 3.7771e-05  error = 9.9605e-03  \n",
            "nt_epoch =   1760  elapsed = 02:17  loss = 3.7374e-05  error = 1.0157e-02  \n",
            "nt_epoch =   1770  elapsed = 02:18  loss = 3.7136e-05  error = 1.0067e-02  \n",
            "nt_epoch =   1780  elapsed = 02:18  loss = 3.6807e-05  error = 1.0081e-02  \n",
            "nt_epoch =   1790  elapsed = 02:19  loss = 3.6444e-05  error = 9.9044e-03  \n",
            "nt_epoch =   1800  elapsed = 02:20  loss = 3.6326e-05  error = 9.6350e-03  \n",
            "nt_epoch =   1810  elapsed = 02:21  loss = 3.6042e-05  error = 9.9286e-03  \n",
            "nt_epoch =   1820  elapsed = 02:21  loss = 3.5787e-05  error = 1.0130e-02  \n",
            "nt_epoch =   1830  elapsed = 02:22  loss = 3.5463e-05  error = 9.4826e-03  \n",
            "nt_epoch =   1840  elapsed = 02:23  loss = 3.5192e-05  error = 9.9089e-03  \n",
            "nt_epoch =   1850  elapsed = 02:23  loss = 3.4920e-05  error = 9.7712e-03  \n",
            "nt_epoch =   1860  elapsed = 02:24  loss = 3.4555e-05  error = 9.8164e-03  \n",
            "nt_epoch =   1870  elapsed = 02:25  loss = 3.4246e-05  error = 1.0246e-02  \n",
            "nt_epoch =   1880  elapsed = 02:26  loss = 3.3803e-05  error = 9.1212e-03  \n",
            "nt_epoch =   1890  elapsed = 02:27  loss = 3.3549e-05  error = 9.9702e-03  \n",
            "nt_epoch =   1900  elapsed = 02:27  loss = 3.3279e-05  error = 9.3759e-03  \n",
            "nt_epoch =   1910  elapsed = 02:28  loss = 3.3064e-05  error = 8.5308e-03  \n",
            "nt_epoch =   1920  elapsed = 02:29  loss = 3.2655e-05  error = 8.3407e-03  \n",
            "nt_epoch =   1930  elapsed = 02:29  loss = 3.2351e-05  error = 7.8839e-03  \n",
            "nt_epoch =   1940  elapsed = 02:30  loss = 3.1956e-05  error = 8.2010e-03  \n",
            "nt_epoch =   1950  elapsed = 02:31  loss = 3.1805e-05  error = 8.2778e-03  \n",
            "nt_epoch =   1960  elapsed = 02:31  loss = 3.1566e-05  error = 8.2256e-03  \n",
            "nt_epoch =   1970  elapsed = 02:32  loss = 3.1308e-05  error = 7.4457e-03  \n",
            "nt_epoch =   1980  elapsed = 02:33  loss = 3.1111e-05  error = 7.6017e-03  \n",
            "nt_epoch =   1990  elapsed = 02:34  loss = 3.0884e-05  error = 7.8003e-03  \n",
            "==================\n",
            "Training finished (epoch 2100): duration = 02:34  error = 7.9419e-03  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wf1QXaK5PlUh",
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "c92a0e41-6d0b-4191-fd3e-ad31c29436a1"
      },
      "source": [
        "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n",
        "  Exact_u, X, T, x, t)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4d1e3c4d904c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m   Exact_u, X, T, x, t)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_inf_cont_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T-Dt42ItxAqr"
      },
      "source": [
        "# 2. Discrete Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U1oPFjYDxAqs"
      },
      "source": [
        "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
        "\n",
        "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
        "\n",
        "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P4z7cQz_xAqt"
      },
      "source": [
        "Approximating $u(t,x)$ with a deep NN, we define the PINN: (TODO)\n",
        "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
        "\n",
        "We train the shared parameters between the deep NN and the PINN minimizing the loss: (TODO)\n",
        "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
        "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGd5zVtoxAqt"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wKrkV1ChxAqu",
        "lines_to_next_cell": 2,
        "colab": {}
      },
      "source": [
        "\n",
        "# Data size on initial condition on u\n",
        "N_n = 250\n",
        "# Number of RK stages\n",
        "q = 500\n",
        "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q+1-sized output [u_1^n(x), ..., u_{q+1}^n(x)]\n",
        "layers = [1, 50, 50, 50, q + 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 200\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  lr=0.001,\n",
        "  beta_1=0.9,\n",
        "  beta_2=0.999,\n",
        "  epsilon=1e-08)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 1000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bXtJ5GiaxAqw"
      },
      "source": [
        "## PINN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KiPSoTyPxAqw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2acHqmorxAqx",
        "colab": {}
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times):\n",
        "    self.lb = lb\n",
        "    self.ub = ub\n",
        "    self.nu = nu\n",
        "\n",
        "    self.dt = dt\n",
        "\n",
        "    self.q = max(q,1)\n",
        "    self.IRK_weights = IRK_weights\n",
        "    self.IRK_times = IRK_times\n",
        "\n",
        "    # Descriptive Keras model [2, 50, …, 50, q+1]\n",
        "    self.U_1_model = tf.keras.Sequential()\n",
        "    self.U_1_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.U_1_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.U_1_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    self.x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "  def U_0_model(self, x):\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(x)\n",
        "      tape.watch(self.dummy_x0_tf)\n",
        "\n",
        "      # Getting the prediction, and removing the last item (q+1)\n",
        "      U_1 = self.U_1_model(x) # shape=(len(x), q+1)\n",
        "      U = U_1[:, :-1] # shape=(len(x), q)\n",
        "\n",
        "      # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
        "      g_U = tape.gradient(U, x, output_gradients=self.dummy_x0_tf)\n",
        "      U_x = tape.gradient(g_U, self.dummy_x0_tf)\n",
        "      g_U_x = tape.gradient(U_x, x, output_gradients=self.dummy_x0_tf)\n",
        "    \n",
        "    # Doing the last one outside the with, to optimize performance\n",
        "    # Impossible to do for the earlier grad, because they’re needed after\n",
        "    U_xx = tape.gradient(g_U_x, self.dummy_x0_tf)\n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "\n",
        "    # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
        "    nu = self.get_params(numpy=True)\n",
        "    N = U*U_x - nu*U_xx # shape=(len(x), q)\n",
        "    return U_1 + self.dt*tf.matmul(N, self.IRK_weights.T)\n",
        "\n",
        "  # Defining custom loss\n",
        "  def __loss(self, u_0, u_0_pred):\n",
        "    u_1_pred = self.U_1_model(self.x_1)\n",
        "    return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
        "      tf.reduce_sum(tf.square(u_1_pred))\n",
        "\n",
        "  def __grad(self, x_0, u_0):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.U_1_model.trainable_variables\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = []\n",
        "    for layer in self.U_1_model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.U_1_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    return self.nu\n",
        "\n",
        "  def summary(self):\n",
        "    return self.U_1_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, x_0, u_0, tf_epochs, nt_config):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
        "    u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
        "\n",
        "    # Creating dummy tensors for the gradients\n",
        "    self.dummy_x0_tf = tf.ones([x_0.shape[0], self.q], dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(x_0, u_0)\n",
        "      self.optimizer.apply_gradients(\n",
        "        zip(grads, self.__wrap_training_variables()))\n",
        "      self.logger.log_train_epoch(epoch, loss_value)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
        "      grad = tape.gradient(loss_value, self.U_1_model.trainable_variables)\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "    \n",
        "    self.logger.log_train_end(tf_epochs)\n",
        "\n",
        "  def predict(self, x_star):\n",
        "    u_star = self.U_1_model(x_star)[:, -1]\n",
        "    return u_star"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XIq8U_a_xAqy"
      },
      "source": [
        "## Training and plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d4ICbcZhxAqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63c94c49-8301-43f3-8c55-c22cba8f1bcc"
      },
      "source": [
        "\n",
        "# Setup\n",
        "lb = np.array([-1.0])\n",
        "ub = np.array([1.0])\n",
        "idx_t_0 = 10\n",
        "idx_t_1 = 90\n",
        "nu = 0.01/np.pi\n",
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x, t, dt, \\\n",
        "  Exact_u, x_0, u_0, x_1, x_star, u_star, \\\n",
        "  IRK_weights, IRK_times = prep_data(path, N_n=N_n, q=q, lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times)\n",
        "def error():\n",
        "  u_pred = pinn.predict(x_star)\n",
        "  return np.linalg.norm(u_pred - u_star, 2) / np.linalg.norm(u_star, 2)\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(x_0, u_0, tf_epochs, nt_config)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_1_pred = pinn.predict(x_star)\n",
        "\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_1_pred = pinn.predict(x_star)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0805 21:25:48.731011 140462172571520 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "W0805 21:25:48.739773 140462172571520 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "W0805 21:25:48.751665 140462172571520 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.0.0-beta1\n",
            "Eager execution: True\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 50)                100       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 501)               25551     \n",
            "=================================================================\n",
            "Total params: 30,751\n",
            "Trainable params: 30,751\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "tf_epoch =      0  elapsed = 00:00  loss = 6.1188e+04  error = 9.9183e-01  \n",
            "tf_epoch =     10  elapsed = 00:00  loss = 5.2999e+04  error = 9.1168e-01  \n",
            "tf_epoch =     20  elapsed = 00:01  loss = 4.0422e+04  error = 8.3942e-01  \n",
            "tf_epoch =     30  elapsed = 00:02  loss = 3.0798e+04  error = 1.0075e+00  \n",
            "tf_epoch =     40  elapsed = 00:02  loss = 2.6585e+04  error = 1.2057e+00  \n",
            "tf_epoch =     50  elapsed = 00:03  loss = 2.3989e+04  error = 1.3011e+00  \n",
            "tf_epoch =     60  elapsed = 00:04  loss = 2.1985e+04  error = 1.3448e+00  \n",
            "tf_epoch =     70  elapsed = 00:04  loss = 2.0261e+04  error = 1.3670e+00  \n",
            "tf_epoch =     80  elapsed = 00:05  loss = 1.8731e+04  error = 1.3788e+00  \n",
            "tf_epoch =     90  elapsed = 00:05  loss = 1.7442e+04  error = 1.3852e+00  \n",
            "tf_epoch =    100  elapsed = 00:06  loss = 1.6428e+04  error = 1.3889e+00  \n",
            "tf_epoch =    110  elapsed = 00:07  loss = 1.5687e+04  error = 1.3913e+00  \n",
            "tf_epoch =    120  elapsed = 00:07  loss = 1.5193e+04  error = 1.3929e+00  \n",
            "tf_epoch =    130  elapsed = 00:08  loss = 1.4885e+04  error = 1.3941e+00  \n",
            "tf_epoch =    140  elapsed = 00:09  loss = 1.4671e+04  error = 1.3951e+00  \n",
            "tf_epoch =    150  elapsed = 00:09  loss = 1.4462e+04  error = 1.3959e+00  \n",
            "tf_epoch =    160  elapsed = 00:10  loss = 1.4274e+04  error = 1.3966e+00  \n",
            "tf_epoch =    170  elapsed = 00:11  loss = 1.4054e+04  error = 1.3972e+00  \n",
            "tf_epoch =    180  elapsed = 00:11  loss = 1.3774e+04  error = 1.3978e+00  \n",
            "tf_epoch =    190  elapsed = 00:12  loss = 1.3375e+04  error = 1.3985e+00  \n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 00:13  loss = 1.0260e+04  error = 1.3989e+00  \n",
            "nt_epoch =     20  elapsed = 00:14  loss = 8.0025e+03  error = 1.3987e+00  \n",
            "nt_epoch =     30  elapsed = 00:15  loss = 6.5610e+03  error = 1.3953e+00  \n",
            "nt_epoch =     40  elapsed = 00:16  loss = 5.3492e+03  error = 1.3838e+00  \n",
            "nt_epoch =     50  elapsed = 00:17  loss = 4.0831e+03  error = 1.3269e+00  \n",
            "nt_epoch =     60  elapsed = 00:17  loss = 3.0797e+03  error = 5.6137e-01  \n",
            "nt_epoch =     70  elapsed = 00:18  loss = 2.3537e+03  error = 3.5572e-01  \n",
            "nt_epoch =     80  elapsed = 00:19  loss = 1.7021e+03  error = 1.8447e-01  \n",
            "nt_epoch =     90  elapsed = 00:20  loss = 1.2382e+03  error = 2.6041e-01  \n",
            "nt_epoch =    100  elapsed = 00:21  loss = 9.5778e+02  error = 2.6782e-01  \n",
            "nt_epoch =    110  elapsed = 00:22  loss = 5.9612e+02  error = 2.3209e-01  \n",
            "nt_epoch =    120  elapsed = 00:23  loss = 4.3253e+02  error = 1.7717e-01  \n",
            "nt_epoch =    130  elapsed = 00:24  loss = 3.7643e+02  error = 1.7225e-01  \n",
            "nt_epoch =    140  elapsed = 00:25  loss = 3.2437e+02  error = 1.4322e-01  \n",
            "nt_epoch =    150  elapsed = 00:26  loss = 2.4386e+02  error = 1.4102e-01  \n",
            "nt_epoch =    160  elapsed = 00:27  loss = 1.7156e+02  error = 1.5808e-01  \n",
            "nt_epoch =    170  elapsed = 00:28  loss = 1.3841e+02  error = 1.5474e-01  \n",
            "nt_epoch =    180  elapsed = 00:29  loss = 1.1642e+02  error = 1.3585e-01  \n",
            "nt_epoch =    190  elapsed = 00:30  loss = 1.0751e+02  error = 1.3060e-01  \n",
            "nt_epoch =    200  elapsed = 00:31  loss = 1.0277e+02  error = 1.2505e-01  \n",
            "nt_epoch =    210  elapsed = 00:32  loss = 9.7123e+01  error = 1.2389e-01  \n",
            "nt_epoch =    220  elapsed = 00:33  loss = 9.0848e+01  error = 1.2139e-01  \n",
            "nt_epoch =    230  elapsed = 00:34  loss = 8.0729e+01  error = 1.0536e-01  \n",
            "nt_epoch =    240  elapsed = 00:35  loss = 7.4309e+01  error = 9.7123e-02  \n",
            "nt_epoch =    250  elapsed = 00:36  loss = 6.6305e+01  error = 9.2456e-02  \n",
            "nt_epoch =    260  elapsed = 00:37  loss = 6.2808e+01  error = 8.8131e-02  \n",
            "nt_epoch =    270  elapsed = 00:38  loss = 5.9841e+01  error = 7.8305e-02  \n",
            "nt_epoch =    280  elapsed = 00:39  loss = 5.6641e+01  error = 7.3515e-02  \n",
            "nt_epoch =    290  elapsed = 00:40  loss = 5.4772e+01  error = 7.0167e-02  \n",
            "nt_epoch =    300  elapsed = 00:41  loss = 5.1820e+01  error = 6.9559e-02  \n",
            "nt_epoch =    310  elapsed = 00:42  loss = 4.8778e+01  error = 6.2209e-02  \n",
            "nt_epoch =    320  elapsed = 00:43  loss = 4.6680e+01  error = 6.2261e-02  \n",
            "nt_epoch =    330  elapsed = 00:44  loss = 4.3339e+01  error = 6.1597e-02  \n",
            "nt_epoch =    340  elapsed = 00:45  loss = 4.0787e+01  error = 6.3107e-02  \n",
            "nt_epoch =    350  elapsed = 00:46  loss = 3.8937e+01  error = 6.5166e-02  \n",
            "nt_epoch =    360  elapsed = 00:47  loss = 3.6545e+01  error = 6.7040e-02  \n",
            "nt_epoch =    370  elapsed = 00:47  loss = 3.5034e+01  error = 6.6152e-02  \n",
            "nt_epoch =    380  elapsed = 00:48  loss = 3.3942e+01  error = 6.8988e-02  \n",
            "nt_epoch =    390  elapsed = 00:49  loss = 3.2148e+01  error = 6.8608e-02  \n",
            "nt_epoch =    400  elapsed = 00:50  loss = 2.9968e+01  error = 6.9662e-02  \n",
            "nt_epoch =    410  elapsed = 00:51  loss = 2.8058e+01  error = 6.6027e-02  \n",
            "nt_epoch =    420  elapsed = 00:52  loss = 2.7008e+01  error = 6.5506e-02  \n",
            "nt_epoch =    430  elapsed = 00:53  loss = 2.6284e+01  error = 6.5002e-02  \n",
            "nt_epoch =    440  elapsed = 00:54  loss = 2.5651e+01  error = 6.3251e-02  \n",
            "nt_epoch =    450  elapsed = 00:55  loss = 2.4570e+01  error = 6.1640e-02  \n",
            "nt_epoch =    460  elapsed = 00:56  loss = 2.3681e+01  error = 6.0461e-02  \n",
            "nt_epoch =    470  elapsed = 00:57  loss = 2.3017e+01  error = 5.9235e-02  \n",
            "nt_epoch =    480  elapsed = 00:58  loss = 2.2296e+01  error = 5.7410e-02  \n",
            "nt_epoch =    490  elapsed = 00:59  loss = 2.1354e+01  error = 5.3423e-02  \n",
            "nt_epoch =    500  elapsed = 01:00  loss = 2.0496e+01  error = 5.2898e-02  \n",
            "nt_epoch =    510  elapsed = 01:01  loss = 1.9511e+01  error = 5.1763e-02  \n",
            "nt_epoch =    520  elapsed = 01:02  loss = 1.8708e+01  error = 4.9333e-02  \n",
            "nt_epoch =    530  elapsed = 01:03  loss = 1.8047e+01  error = 4.9318e-02  \n",
            "nt_epoch =    540  elapsed = 01:04  loss = 1.7367e+01  error = 4.8382e-02  \n",
            "nt_epoch =    550  elapsed = 01:05  loss = 1.6835e+01  error = 4.7556e-02  \n",
            "nt_epoch =    560  elapsed = 01:06  loss = 1.6030e+01  error = 4.5138e-02  \n",
            "nt_epoch =    570  elapsed = 01:07  loss = 1.5672e+01  error = 4.3811e-02  \n",
            "nt_epoch =    580  elapsed = 01:08  loss = 1.4913e+01  error = 4.2593e-02  \n",
            "nt_epoch =    590  elapsed = 01:09  loss = 1.4461e+01  error = 4.1103e-02  \n",
            "nt_epoch =    600  elapsed = 01:10  loss = 1.3996e+01  error = 3.9832e-02  \n",
            "nt_epoch =    610  elapsed = 01:11  loss = 1.3314e+01  error = 4.0016e-02  \n",
            "nt_epoch =    620  elapsed = 01:12  loss = 1.2898e+01  error = 3.9564e-02  \n",
            "nt_epoch =    630  elapsed = 01:13  loss = 1.2469e+01  error = 3.6838e-02  \n",
            "nt_epoch =    640  elapsed = 01:14  loss = 1.2242e+01  error = 3.6251e-02  \n",
            "nt_epoch =    650  elapsed = 01:15  loss = 1.1776e+01  error = 3.6203e-02  \n",
            "nt_epoch =    660  elapsed = 01:16  loss = 1.1366e+01  error = 3.5112e-02  \n",
            "nt_epoch =    670  elapsed = 01:16  loss = 1.1178e+01  error = 3.4941e-02  \n",
            "nt_epoch =    680  elapsed = 01:17  loss = 1.0874e+01  error = 3.4121e-02  \n",
            "nt_epoch =    690  elapsed = 01:18  loss = 1.1036e+01  error = 3.1702e-02  \n",
            "nt_epoch =    700  elapsed = 01:19  loss = 1.0241e+01  error = 3.1255e-02  \n",
            "nt_epoch =    710  elapsed = 01:20  loss = 9.9218e+00  error = 3.1102e-02  \n",
            "nt_epoch =    720  elapsed = 01:21  loss = 9.5941e+00  error = 3.1420e-02  \n",
            "nt_epoch =    730  elapsed = 01:22  loss = 9.2719e+00  error = 3.0510e-02  \n",
            "nt_epoch =    740  elapsed = 01:23  loss = 9.0276e+00  error = 3.0447e-02  \n",
            "nt_epoch =    750  elapsed = 01:24  loss = 8.8384e+00  error = 2.9893e-02  \n",
            "nt_epoch =    760  elapsed = 01:25  loss = 8.5528e+00  error = 2.9247e-02  \n",
            "nt_epoch =    770  elapsed = 01:26  loss = 8.3017e+00  error = 2.8684e-02  \n",
            "nt_epoch =    780  elapsed = 01:27  loss = 8.0868e+00  error = 3.0357e-02  \n",
            "nt_epoch =    790  elapsed = 01:28  loss = 7.7973e+00  error = 3.1152e-02  \n",
            "nt_epoch =    800  elapsed = 01:29  loss = 7.6803e+00  error = 3.1273e-02  \n",
            "nt_epoch =    810  elapsed = 01:30  loss = 7.3932e+00  error = 3.1176e-02  \n",
            "nt_epoch =    820  elapsed = 01:31  loss = 7.2232e+00  error = 3.0331e-02  \n",
            "nt_epoch =    830  elapsed = 01:32  loss = 7.0624e+00  error = 3.0471e-02  \n",
            "nt_epoch =    840  elapsed = 01:33  loss = 6.8274e+00  error = 3.0537e-02  \n",
            "nt_epoch =    850  elapsed = 01:34  loss = 6.7052e+00  error = 3.0474e-02  \n",
            "nt_epoch =    860  elapsed = 01:35  loss = 6.5483e+00  error = 3.0797e-02  \n",
            "nt_epoch =    870  elapsed = 01:36  loss = 6.4209e+00  error = 3.1281e-02  \n",
            "nt_epoch =    880  elapsed = 01:37  loss = 6.2477e+00  error = 3.1229e-02  \n",
            "nt_epoch =    890  elapsed = 01:38  loss = 6.0582e+00  error = 3.1409e-02  \n",
            "nt_epoch =    900  elapsed = 01:39  loss = 5.9251e+00  error = 3.0850e-02  \n",
            "nt_epoch =    910  elapsed = 01:40  loss = 5.8344e+00  error = 3.0446e-02  \n",
            "nt_epoch =    920  elapsed = 01:41  loss = 5.7018e+00  error = 3.0456e-02  \n",
            "nt_epoch =    930  elapsed = 01:42  loss = 5.5640e+00  error = 3.0581e-02  \n",
            "nt_epoch =    940  elapsed = 01:43  loss = 5.3976e+00  error = 3.0329e-02  \n",
            "nt_epoch =    950  elapsed = 01:44  loss = 5.3060e+00  error = 3.0085e-02  \n",
            "nt_epoch =    960  elapsed = 01:45  loss = 5.2103e+00  error = 3.0336e-02  \n",
            "nt_epoch =    970  elapsed = 01:46  loss = 5.1273e+00  error = 3.1041e-02  \n",
            "nt_epoch =    980  elapsed = 01:47  loss = 5.0356e+00  error = 3.0310e-02  \n",
            "nt_epoch =    990  elapsed = 01:48  loss = 4.9440e+00  error = 2.9466e-02  \n",
            "==================\n",
            "Training finished (epoch 200): duration = 01:49  error = 2.9393e-02  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g9aFDcGpxAq2",
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "c1461dcd-8bff-4bb6-90b6-17ad557acce0"
      },
      "source": [
        "plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0805 21:27:37.908822 140462172571520 legend.py:1289] No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEsCAYAAABg9mDTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 388.543x288.159 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IoN7tCmSGDeO"
      },
      "source": [
        "# 3. Continuous Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BkpuORIsGDeQ"
      },
      "source": [
        "$$u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0$$\n",
        "\n",
        "With $\\lambda_1$ and $\\lambda_2$ real parameters of the differential operator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lpUoecjeGDeR"
      },
      "source": [
        "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
        "$$f := u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx}.$$\n",
        "\n",
        "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
        "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_u}|f(t_u^i,x_u^i)|^2,$$\n",
        "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ respectively the trainring data on $u(t,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dOPzdkKsJzA4"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hKRYhyXqGDeT",
        "lines_to_next_cell": 2,
        "colab": {}
      },
      "source": [
        "\n",
        "# Data size on the solution u\n",
        "N_u = 2000\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 100\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate=0.001)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 1000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OTxvp1nJGDeb"
      },
      "source": [
        "## PINN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O9grEA3wGDed"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLOwP1UfGDee",
        "lines_to_next_cell": 2,
        "colab": {}
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, ub, lb):\n",
        "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
        "    self.u_model = tf.keras.Sequential()\n",
        "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.u_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    # Defining the two additional trainable variables for identification\n",
        "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
        "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
        "    \n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "  # The actual PINN\n",
        "  def __f_model(self, X_u):\n",
        "    l1, l2 = self.get_params()\n",
        "    # Separating the collocation coordinates\n",
        "    x_f = tf.convert_to_tensor(X_u[:, 0:1], dtype=self.dtype)\n",
        "    t_f = tf.convert_to_tensor(X_u[:, 1:2], dtype=self.dtype)\n",
        "\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(x_f)\n",
        "      tape.watch(t_f)\n",
        "      # Packing together the inputs\n",
        "      X_f = tf.stack([x_f[:,0], t_f[:,0]], axis=1)\n",
        "\n",
        "\n",
        "      # Getting the prediction\n",
        "      u = self.u_model(X_f)\n",
        "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
        "      u_x = tape.gradient(u, x_f)\n",
        "    \n",
        "    # Getting the other derivatives\n",
        "    u_xx = tape.gradient(u_x, x_f)\n",
        "    u_t = tape.gradient(u, t_f)\n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "\n",
        "    # Buidling the PINNs\n",
        "    return u_t + l1*u*u_x - l2*u_xx\n",
        "\n",
        "  # Defining custom loss\n",
        "  def __loss(self, X_u, u, u_pred):\n",
        "    f_pred = self.__f_model(X_u)\n",
        "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
        "      tf.reduce_mean(tf.square(f_pred))\n",
        "\n",
        "  def __grad(self, X, u):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(X, u, self.u_model(X))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.u_model.trainable_variables\n",
        "    var.extend([self.lambda_1, self.lambda_2])\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "      w = []\n",
        "      for layer in self.u_model.layers[1:]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "      w.extend(self.lambda_1.numpy())\n",
        "      w.extend(self.lambda_2.numpy())\n",
        "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "    self.lambda_1.assign([w[-2]])\n",
        "    self.lambda_2.assign([w[-1]])\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    if numpy:\n",
        "      return l1.numpy()[0], l2.numpy()[0]\n",
        "    return l1, l2\n",
        "\n",
        "  def summary(self):\n",
        "    return self.u_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, X_u, u, tf_epochs, nt_config):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
        "\n",
        "    def log_train_epoch(epoch, loss, is_iter):\n",
        "      l1, l2 = self.get_params(numpy=True)\n",
        "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
        "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(X_u, u)\n",
        "      self.optimizer.apply_gradients(\n",
        "        zip(grads, self.__wrap_training_variables()))\n",
        "      log_train_epoch(epoch, loss_value, False)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        tape.watch(self.lambda_1)\n",
        "        tape.watch(self.lambda_2)\n",
        "        loss_value = self.__loss(X_u, u, self.u_model(X_u))\n",
        "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True, log_train_epoch)\n",
        "    \n",
        "    l1, l2 = self.get_params(numpy=True)\n",
        "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    u_star = self.u_model(X_star)\n",
        "    f_star = self.__f_model(X_star)\n",
        "    return u_star.numpy(), f_star.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-rWEI708GDei"
      },
      "source": [
        "## Training and plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yxkeBW46GDek",
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31c8bd18-432c-4628-8b71-4695892b0229"
      },
      "source": [
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "  X_u_train, u_train, ub, lb = prep_data(path, N_u, noise=0.0)\n",
        "lambdas_star = (1.0, 0.01/np.pi)\n",
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
        "def error():\n",
        "  l1, l2 = pinn.get_params(numpy=True)\n",
        "  l1_star, l2_star = lambdas_star\n",
        "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
        "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
        "  return (error_lambda_1 + error_lambda_2) / 2\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_pred, f_pred = pinn.predict(X_star)\n",
        "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
        "\n",
        "# Noise case\n",
        "x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "  X_u_train, u_train, ub, lb = prep_data(path, N_u, noise=0.01)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
        "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
        "lambda_1_pred_noise, lambda_2_pred_noise = pinn.get_params(numpy=True)\n",
        "\n",
        "print(\"l1: \", lambda_1_pred)\n",
        "print(\"l2: \", lambda_2_pred)\n",
        "print(\"l1_noise: \", lambda_1_pred_noise)\n",
        "print(\"l2_noise: \", lambda_2_pred_noise)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0805 22:05:58.320646 140462172571520 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.0.0-beta1\n",
            "Eager execution: True\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_6 (Lambda)            (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 20)                60        \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 3,021\n",
            "Trainable params: 3,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "tf_epoch =      0  elapsed = 00:00  loss = 4.5547e-01  error = 6.1091e-01  l1 = -0.000783  l2 = 0.002480\n",
            "tf_epoch =     10  elapsed = 00:01  loss = 2.6218e-01  error = 6.1337e-01  l1 = -0.006820  l2 = 0.002483\n",
            "tf_epoch =     20  elapsed = 00:02  loss = 2.1888e-01  error = 6.1472e-01  l1 = -0.017007  l2 = 0.002507\n",
            "tf_epoch =     30  elapsed = 00:03  loss = 2.0441e-01  error = 6.1597e-01  l1 = -0.028668  l2 = 0.002536\n",
            "tf_epoch =     40  elapsed = 00:04  loss = 1.8307e-01  error = 6.1695e-01  l1 = -0.038174  l2 = 0.002560\n",
            "tf_epoch =     50  elapsed = 00:05  loss = 1.5691e-01  error = 6.1771e-01  l1 = -0.043576  l2 = 0.002572\n",
            "tf_epoch =     60  elapsed = 00:06  loss = 1.2616e-01  error = 6.2007e-01  l1 = -0.041468  l2 = 0.002551\n",
            "tf_epoch =     70  elapsed = 00:07  loss = 1.0271e-01  error = 6.2017e-01  l1 = -0.027952  l2 = 0.002507\n",
            "tf_epoch =     80  elapsed = 00:08  loss = 8.8514e-02  error = 6.1846e-01  l1 = -0.009309  l2 = 0.002459\n",
            "tf_epoch =     90  elapsed = 00:09  loss = 7.8031e-02  error = 6.1632e-01  l1 = 0.010664  l2 = 0.002409\n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 00:11  loss = 4.6388e-02  error = 5.7011e-01  l1 = 0.118674  l2 = 0.002359\n",
            "nt_epoch =     20  elapsed = 00:12  loss = 3.0207e-02  error = 5.6609e-01  l1 = 0.122968  l2 = 0.002371\n",
            "nt_epoch =     30  elapsed = 00:13  loss = 2.7134e-02  error = 5.4988e-01  l1 = 0.152543  l2 = 0.002380\n",
            "nt_epoch =     40  elapsed = 00:14  loss = 2.3084e-02  error = 5.2059e-01  l1 = 0.198827  l2 = 0.002419\n",
            "nt_epoch =     50  elapsed = 00:15  loss = 2.1455e-02  error = 4.8902e-01  l1 = 0.244015  l2 = 0.002476\n",
            "nt_epoch =     60  elapsed = 00:16  loss = 1.9514e-02  error = 4.5810e-01  l1 = 0.281306  l2 = 0.002554\n",
            "nt_epoch =     70  elapsed = 00:17  loss = 1.7115e-02  error = 3.9137e-01  l1 = 0.358043  l2 = 0.002735\n",
            "nt_epoch =     80  elapsed = 00:18  loss = 1.6210e-02  error = 3.7468e-01  l1 = 0.367946  l2 = 0.002810\n",
            "nt_epoch =     90  elapsed = 00:20  loss = 1.5033e-02  error = 3.4473e-01  l1 = 0.398276  l2 = 0.002904\n",
            "nt_epoch =    100  elapsed = 00:21  loss = 1.4720e-02  error = 3.1895e-01  l1 = 0.410808  l2 = 0.003028\n",
            "nt_epoch =    110  elapsed = 00:22  loss = 1.3573e-02  error = 3.2674e-01  l1 = 0.430982  l2 = 0.003452\n",
            "nt_epoch =    120  elapsed = 00:23  loss = 1.2996e-02  error = 3.8987e-01  l1 = 0.479184  l2 = 0.004007\n",
            "nt_epoch =    130  elapsed = 00:24  loss = 1.1865e-02  error = 3.7349e-01  l1 = 0.469203  l2 = 0.003871\n",
            "nt_epoch =    140  elapsed = 00:25  loss = 1.0908e-02  error = 4.3078e-01  l1 = 0.500988  l2 = 0.004337\n",
            "nt_epoch =    150  elapsed = 00:27  loss = 9.3850e-03  error = 4.8405e-01  l1 = 0.565851  l2 = 0.004883\n",
            "nt_epoch =    160  elapsed = 00:28  loss = 8.1922e-03  error = 5.2559e-01  l1 = 0.621218  l2 = 0.005323\n",
            "nt_epoch =    170  elapsed = 00:29  loss = 6.6335e-03  error = 5.6620e-01  l1 = 0.704882  l2 = 0.005848\n",
            "nt_epoch =    180  elapsed = 00:30  loss = 6.1406e-03  error = 5.9871e-01  l1 = 0.724151  l2 = 0.006117\n",
            "nt_epoch =    190  elapsed = 00:31  loss = 5.5447e-03  error = 6.2428e-01  l1 = 0.760905  l2 = 0.006396\n",
            "nt_epoch =    200  elapsed = 00:33  loss = 5.0648e-03  error = 6.3345e-01  l1 = 0.786835  l2 = 0.006537\n",
            "nt_epoch =    210  elapsed = 00:34  loss = 4.4839e-03  error = 6.4229e-01  l1 = 0.817985  l2 = 0.006693\n",
            "nt_epoch =    220  elapsed = 00:35  loss = 4.0833e-03  error = 6.6417e-01  l1 = 0.860075  l2 = 0.006966\n",
            "nt_epoch =    230  elapsed = 00:36  loss = 3.6562e-03  error = 6.7634e-01  l1 = 0.870518  l2 = 0.007077\n",
            "nt_epoch =    240  elapsed = 00:37  loss = 3.3530e-03  error = 6.9604e-01  l1 = 0.886854  l2 = 0.007254\n",
            "nt_epoch =    250  elapsed = 00:39  loss = 3.0241e-03  error = 6.8186e-01  l1 = 0.913283  l2 = 0.007248\n",
            "nt_epoch =    260  elapsed = 00:40  loss = 2.7792e-03  error = 6.5136e-01  l1 = 0.930029  l2 = 0.007107\n",
            "nt_epoch =    270  elapsed = 00:41  loss = 2.5468e-03  error = 6.1248e-01  l1 = 0.924810  l2 = 0.006843\n",
            "nt_epoch =    280  elapsed = 00:42  loss = 2.3568e-03  error = 5.7254e-01  l1 = 0.924378  l2 = 0.006587\n",
            "nt_epoch =    290  elapsed = 00:43  loss = 2.2290e-03  error = 5.5689e-01  l1 = 0.927062  l2 = 0.006496\n",
            "nt_epoch =    300  elapsed = 00:44  loss = 2.0926e-03  error = 5.3285e-01  l1 = 0.919142  l2 = 0.006318\n",
            "nt_epoch =    310  elapsed = 00:46  loss = 1.9134e-03  error = 5.0649e-01  l1 = 0.917415  l2 = 0.006145\n",
            "nt_epoch =    320  elapsed = 00:47  loss = 1.8455e-03  error = 4.8682e-01  l1 = 0.914484  l2 = 0.006010\n",
            "nt_epoch =    330  elapsed = 00:48  loss = 1.7572e-03  error = 4.6955e-01  l1 = 0.908829  l2 = 0.005882\n",
            "nt_epoch =    340  elapsed = 00:49  loss = 1.6866e-03  error = 4.5410e-01  l1 = 0.905959  l2 = 0.005775\n",
            "nt_epoch =    350  elapsed = 00:50  loss = 1.5247e-03  error = 4.0378e-01  l1 = 0.925295  l2 = 0.005516\n",
            "nt_epoch =    360  elapsed = 00:51  loss = 1.3841e-03  error = 3.6355e-01  l1 = 0.939808  l2 = 0.005306\n",
            "nt_epoch =    370  elapsed = 00:52  loss = 1.3024e-03  error = 3.5553e-01  l1 = 0.940323  l2 = 0.005256\n",
            "nt_epoch =    380  elapsed = 00:54  loss = 1.2065e-03  error = 3.5196e-01  l1 = 0.947363  l2 = 0.005256\n",
            "nt_epoch =    390  elapsed = 00:55  loss = 1.1563e-03  error = 3.2859e-01  l1 = 0.942368  l2 = 0.005091\n",
            "nt_epoch =    400  elapsed = 00:56  loss = 1.0685e-03  error = 3.0424e-01  l1 = 0.946365  l2 = 0.004949\n",
            "nt_epoch =    410  elapsed = 00:57  loss = 1.0135e-03  error = 2.8243e-01  l1 = 0.954490  l2 = 0.004836\n",
            "nt_epoch =    420  elapsed = 00:58  loss = 9.2407e-04  error = 2.6625e-01  l1 = 0.963087  l2 = 0.004761\n",
            "nt_epoch =    430  elapsed = 00:59  loss = 8.8538e-04  error = 2.4908e-01  l1 = 0.964432  l2 = 0.004656\n",
            "nt_epoch =    440  elapsed = 01:01  loss = 8.4309e-04  error = 2.3792e-01  l1 = 0.956942  l2 = 0.004561\n",
            "nt_epoch =    450  elapsed = 01:02  loss = 8.0340e-04  error = 2.2458e-01  l1 = 0.955214  l2 = 0.004470\n",
            "nt_epoch =    460  elapsed = 01:03  loss = 7.5371e-04  error = 2.1174e-01  l1 = 0.950283  l2 = 0.004373\n",
            "nt_epoch =    470  elapsed = 01:04  loss = 7.2626e-04  error = 2.0422e-01  l1 = 0.945856  l2 = 0.004311\n",
            "nt_epoch =    480  elapsed = 01:05  loss = 7.0752e-04  error = 2.0217e-01  l1 = 0.948346  l2 = 0.004306\n",
            "nt_epoch =    490  elapsed = 01:06  loss = 6.8443e-04  error = 1.9477e-01  l1 = 0.957499  l2 = 0.004288\n",
            "nt_epoch =    500  elapsed = 01:07  loss = 6.4415e-04  error = 1.8741e-01  l1 = 0.965458  l2 = 0.004266\n",
            "nt_epoch =    510  elapsed = 01:09  loss = 6.0834e-04  error = 1.8464e-01  l1 = 0.966649  l2 = 0.004252\n",
            "nt_epoch =    520  elapsed = 01:10  loss = 5.7773e-04  error = 1.7019e-01  l1 = 0.967768  l2 = 0.004164\n",
            "nt_epoch =    530  elapsed = 01:11  loss = 5.4784e-04  error = 1.6286e-01  l1 = 0.962064  l2 = 0.004099\n",
            "nt_epoch =    540  elapsed = 01:12  loss = 5.2209e-04  error = 1.5228e-01  l1 = 0.968599  l2 = 0.004053\n",
            "nt_epoch =    550  elapsed = 01:13  loss = 5.0727e-04  error = 1.4176e-01  l1 = 0.967531  l2 = 0.003982\n",
            "nt_epoch =    560  elapsed = 01:14  loss = 4.7850e-04  error = 1.2735e-01  l1 = 0.971758  l2 = 0.003904\n",
            "nt_epoch =    570  elapsed = 01:16  loss = 4.6200e-04  error = 1.2696e-01  l1 = 0.969199  l2 = 0.003893\n",
            "nt_epoch =    580  elapsed = 01:17  loss = 4.4489e-04  error = 1.2292e-01  l1 = 0.969392  l2 = 0.003868\n",
            "nt_epoch =    590  elapsed = 01:18  loss = 4.3311e-04  error = 1.1420e-01  l1 = 0.965402  l2 = 0.003800\n",
            "nt_epoch =    600  elapsed = 01:19  loss = 4.2703e-04  error = 1.1338e-01  l1 = 0.968025  l2 = 0.003803\n",
            "nt_epoch =    610  elapsed = 01:20  loss = 4.1820e-04  error = 1.0822e-01  l1 = 0.970085  l2 = 0.003777\n",
            "nt_epoch =    620  elapsed = 01:21  loss = 4.1113e-04  error = 1.0510e-01  l1 = 0.971414  l2 = 0.003761\n",
            "nt_epoch =    630  elapsed = 01:22  loss = 4.0120e-04  error = 1.0171e-01  l1 = 0.968403  l2 = 0.003730\n",
            "nt_epoch =    640  elapsed = 01:24  loss = 3.9197e-04  error = 9.7866e-02  l1 = 0.970155  l2 = 0.003711\n",
            "nt_epoch =    650  elapsed = 01:25  loss = 3.8516e-04  error = 9.8245e-02  l1 = 0.971486  l2 = 0.003718\n",
            "nt_epoch =    660  elapsed = 01:26  loss = 3.7878e-04  error = 9.3661e-02  l1 = 0.973963  l2 = 0.003696\n",
            "nt_epoch =    670  elapsed = 01:27  loss = 3.7338e-04  error = 9.0888e-02  l1 = 0.969097  l2 = 0.003663\n",
            "nt_epoch =    680  elapsed = 01:28  loss = 3.6224e-04  error = 8.5894e-02  l1 = 0.967396  l2 = 0.003626\n",
            "nt_epoch =    690  elapsed = 01:29  loss = 3.4632e-04  error = 7.5356e-02  l1 = 0.966055  l2 = 0.003555\n",
            "nt_epoch =    700  elapsed = 01:31  loss = 3.3898e-04  error = 7.5324e-02  l1 = 0.966495  l2 = 0.003556\n",
            "nt_epoch =    710  elapsed = 01:32  loss = 3.2593e-04  error = 6.8791e-02  l1 = 0.967516  l2 = 0.003518\n",
            "nt_epoch =    720  elapsed = 01:33  loss = 3.1708e-04  error = 6.6337e-02  l1 = 0.968170  l2 = 0.003504\n",
            "nt_epoch =    730  elapsed = 01:34  loss = 3.0607e-04  error = 6.2293e-02  l1 = 0.966960  l2 = 0.003475\n",
            "nt_epoch =    740  elapsed = 01:35  loss = 2.9664e-04  error = 6.1627e-02  l1 = 0.970627  l2 = 0.003482\n",
            "nt_epoch =    750  elapsed = 01:37  loss = 2.9056e-04  error = 5.7331e-02  l1 = 0.972344  l2 = 0.003460\n",
            "nt_epoch =    760  elapsed = 01:38  loss = 2.8196e-04  error = 5.5612e-02  l1 = 0.974392  l2 = 0.003456\n",
            "nt_epoch =    770  elapsed = 01:39  loss = 2.7242e-04  error = 5.4966e-02  l1 = 0.976286  l2 = 0.003458\n",
            "nt_epoch =    780  elapsed = 01:40  loss = 2.6532e-04  error = 4.9907e-02  l1 = 0.976196  l2 = 0.003425\n",
            "nt_epoch =    790  elapsed = 01:41  loss = 2.5841e-04  error = 4.6427e-02  l1 = 0.976959  l2 = 0.003405\n",
            "nt_epoch =    800  elapsed = 01:42  loss = 2.5628e-04  error = 4.6708e-02  l1 = 0.976560  l2 = 0.003406\n",
            "nt_epoch =    810  elapsed = 01:44  loss = 2.5336e-04  error = 4.5637e-02  l1 = 0.976174  l2 = 0.003398\n",
            "nt_epoch =    820  elapsed = 01:45  loss = 2.4862e-04  error = 4.4082e-02  l1 = 0.977858  l2 = 0.003393\n",
            "nt_epoch =    830  elapsed = 01:46  loss = 2.4352e-04  error = 3.9727e-02  l1 = 0.979376  l2 = 0.003370\n",
            "nt_epoch =    840  elapsed = 01:47  loss = 2.3865e-04  error = 3.9482e-02  l1 = 0.979070  l2 = 0.003368\n",
            "nt_epoch =    850  elapsed = 01:48  loss = 2.3638e-04  error = 4.1141e-02  l1 = 0.977261  l2 = 0.003373\n",
            "nt_epoch =    860  elapsed = 01:49  loss = 2.3133e-04  error = 4.0585e-02  l1 = 0.976038  l2 = 0.003365\n",
            "nt_epoch =    870  elapsed = 01:50  loss = 2.3197e-04  error = 3.6408e-02  l1 = 0.978740  l2 = 0.003347\n",
            "nt_epoch =    880  elapsed = 01:52  loss = 2.2169e-04  error = 3.5966e-02  l1 = 0.978617  l2 = 0.003344\n",
            "nt_epoch =    890  elapsed = 01:53  loss = 2.1828e-04  error = 3.5176e-02  l1 = 0.976814  l2 = 0.003333\n",
            "nt_epoch =    900  elapsed = 01:54  loss = 2.1357e-04  error = 3.3021e-02  l1 = 0.976694  l2 = 0.003319\n",
            "nt_epoch =    910  elapsed = 01:55  loss = 2.1006e-04  error = 3.1735e-02  l1 = 0.978034  l2 = 0.003315\n",
            "nt_epoch =    920  elapsed = 01:56  loss = 2.0643e-04  error = 3.1324e-02  l1 = 0.978150  l2 = 0.003313\n",
            "nt_epoch =    930  elapsed = 01:57  loss = 2.0262e-04  error = 3.0225e-02  l1 = 0.978716  l2 = 0.003308\n",
            "nt_epoch =    940  elapsed = 01:59  loss = 1.9924e-04  error = 2.9259e-02  l1 = 0.978330  l2 = 0.003300\n",
            "nt_epoch =    950  elapsed = 02:00  loss = 1.9626e-04  error = 2.8498e-02  l1 = 0.978416  l2 = 0.003296\n",
            "nt_epoch =    960  elapsed = 02:01  loss = 1.9336e-04  error = 2.7538e-02  l1 = 0.979151  l2 = 0.003292\n",
            "nt_epoch =    970  elapsed = 02:02  loss = 1.9189e-04  error = 2.7689e-02  l1 = 0.980030  l2 = 0.003296\n",
            "nt_epoch =    980  elapsed = 02:03  loss = 1.8845e-04  error = 2.6561e-02  l1 = 0.980922  l2 = 0.003291\n",
            "nt_epoch =    990  elapsed = 02:04  loss = 1.8622e-04  error = 2.6297e-02  l1 = 0.980903  l2 = 0.003290\n",
            "==================\n",
            "Training finished (epoch 100): duration = 02:05  error = 2.6637e-02  l1 = 0.980819  l2 = 0.003292\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_7 (Lambda)            (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 20)                60        \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 3,021\n",
            "Trainable params: 3,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "tf_epoch =      0  elapsed = 02:06  loss = 5.3112e-01  error = 6.0989e-01  l1 = 0.000953  l2 = 0.002480\n",
            "tf_epoch =     10  elapsed = 02:07  loss = 2.3350e-01  error = 6.0913e-01  l1 = -0.010057  l2 = 0.002520\n",
            "tf_epoch =     20  elapsed = 02:07  loss = 1.9911e-01  error = 6.1103e-01  l1 = -0.031261  l2 = 0.002576\n",
            "tf_epoch =     30  elapsed = 02:08  loss = 1.5744e-01  error = 6.1332e-01  l1 = -0.048395  l2 = 0.002616\n",
            "tf_epoch =     40  elapsed = 02:09  loss = 1.3018e-01  error = 6.1905e-01  l1 = -0.049670  l2 = 0.002583\n",
            "tf_epoch =     50  elapsed = 02:10  loss = 1.0898e-01  error = 6.2125e-01  l1 = -0.033856  l2 = 0.002519\n",
            "tf_epoch =     60  elapsed = 02:11  loss = 9.1190e-02  error = 6.1961e-01  l1 = -0.009289  l2 = 0.002451\n",
            "tf_epoch =     70  elapsed = 02:12  loss = 7.5517e-02  error = 6.1659e-01  l1 = 0.017538  l2 = 0.002385\n",
            "tf_epoch =     80  elapsed = 02:13  loss = 6.2048e-02  error = 6.1315e-01  l1 = 0.043264  l2 = 0.002325\n",
            "tf_epoch =     90  elapsed = 02:14  loss = 5.1829e-02  error = 6.0986e-01  l1 = 0.064958  l2 = 0.002277\n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 02:16  loss = 3.4789e-02  error = 5.8552e-01  l1 = 0.123846  l2 = 0.002244\n",
            "nt_epoch =     20  elapsed = 02:17  loss = 3.1380e-02  error = 5.8328e-01  l1 = 0.124780  l2 = 0.002256\n",
            "nt_epoch =     30  elapsed = 02:18  loss = 2.8160e-02  error = 5.7078e-01  l1 = 0.140075  l2 = 0.002287\n",
            "nt_epoch =     40  elapsed = 02:19  loss = 2.4317e-02  error = 5.2633e-01  l1 = 0.202051  l2 = 0.002372\n",
            "nt_epoch =     50  elapsed = 02:20  loss = 2.2570e-02  error = 5.0601e-01  l1 = 0.224975  l2 = 0.002429\n",
            "nt_epoch =     60  elapsed = 02:21  loss = 2.0501e-02  error = 4.7094e-01  l1 = 0.267087  l2 = 0.002518\n",
            "nt_epoch =     70  elapsed = 02:22  loss = 1.9115e-02  error = 4.2796e-01  l1 = 0.322862  l2 = 0.002614\n",
            "nt_epoch =     80  elapsed = 02:23  loss = 1.8261e-02  error = 4.1072e-01  l1 = 0.343873  l2 = 0.002657\n",
            "nt_epoch =     90  elapsed = 02:25  loss = 1.7433e-02  error = 3.9804e-01  l1 = 0.353039  l2 = 0.002708\n",
            "nt_epoch =    100  elapsed = 02:26  loss = 1.6774e-02  error = 3.7785e-01  l1 = 0.368630  l2 = 0.002787\n",
            "nt_epoch =    110  elapsed = 02:27  loss = 1.6362e-02  error = 3.5654e-01  l1 = 0.372552  l2 = 0.002911\n",
            "nt_epoch =    120  elapsed = 02:28  loss = 1.5758e-02  error = 3.1992e-01  l1 = 0.412476  l2 = 0.003017\n",
            "nt_epoch =    130  elapsed = 02:29  loss = 1.4944e-02  error = 2.8398e-01  l1 = 0.444952  l2 = 0.003224\n",
            "nt_epoch =    140  elapsed = 02:30  loss = 1.4588e-02  error = 2.9381e-01  l1 = 0.452767  l2 = 0.003312\n",
            "nt_epoch =    150  elapsed = 02:32  loss = 1.4301e-02  error = 3.0585e-01  l1 = 0.471338  l2 = 0.003447\n",
            "nt_epoch =    160  elapsed = 02:33  loss = 1.3828e-02  error = 3.2956e-01  l1 = 0.462828  l2 = 0.003571\n",
            "nt_epoch =    170  elapsed = 02:34  loss = 1.3335e-02  error = 3.6350e-01  l1 = 0.488216  l2 = 0.003868\n",
            "nt_epoch =    180  elapsed = 02:35  loss = 1.2830e-02  error = 4.2774e-01  l1 = 0.518257  l2 = 0.004373\n",
            "nt_epoch =    190  elapsed = 02:36  loss = 1.2420e-02  error = 4.7917e-01  l1 = 0.540407  l2 = 0.004771\n",
            "nt_epoch =    200  elapsed = 02:37  loss = 1.1719e-02  error = 5.6209e-01  l1 = 0.580504  l2 = 0.005426\n",
            "nt_epoch =    210  elapsed = 02:38  loss = 1.0709e-02  error = 5.7909e-01  l1 = 0.606428  l2 = 0.005617\n",
            "nt_epoch =    220  elapsed = 02:40  loss = 9.7500e-03  error = 6.1351e-01  l1 = 0.641118  l2 = 0.005946\n",
            "nt_epoch =    230  elapsed = 02:41  loss = 8.7207e-03  error = 5.7742e-01  l1 = 0.640565  l2 = 0.005715\n",
            "nt_epoch =    240  elapsed = 02:42  loss = 7.8460e-03  error = 5.8637e-01  l1 = 0.676503  l2 = 0.005886\n",
            "nt_epoch =    250  elapsed = 02:43  loss = 7.2991e-03  error = 5.9771e-01  l1 = 0.693592  l2 = 0.006013\n",
            "nt_epoch =    260  elapsed = 02:44  loss = 6.9033e-03  error = 6.0232e-01  l1 = 0.718055  l2 = 0.006120\n",
            "nt_epoch =    270  elapsed = 02:45  loss = 6.3858e-03  error = 6.1833e-01  l1 = 0.750046  l2 = 0.006324\n",
            "nt_epoch =    280  elapsed = 02:47  loss = 5.9734e-03  error = 6.0738e-01  l1 = 0.783206  l2 = 0.006360\n",
            "nt_epoch =    290  elapsed = 02:48  loss = 5.5363e-03  error = 6.2464e-01  l1 = 0.777594  l2 = 0.006452\n",
            "nt_epoch =    300  elapsed = 02:49  loss = 4.9390e-03  error = 6.4307e-01  l1 = 0.811936  l2 = 0.006678\n",
            "nt_epoch =    310  elapsed = 02:50  loss = 4.5822e-03  error = 6.5004e-01  l1 = 0.820310  l2 = 0.006749\n",
            "nt_epoch =    320  elapsed = 02:51  loss = 4.1630e-03  error = 6.4719e-01  l1 = 0.837444  l2 = 0.006786\n",
            "nt_epoch =    330  elapsed = 02:52  loss = 3.8043e-03  error = 6.3223e-01  l1 = 0.845277  l2 = 0.006715\n",
            "nt_epoch =    340  elapsed = 02:54  loss = 3.4169e-03  error = 6.4547e-01  l1 = 0.868115  l2 = 0.006872\n",
            "nt_epoch =    350  elapsed = 02:55  loss = 3.1316e-03  error = 6.5234e-01  l1 = 0.883926  l2 = 0.006967\n",
            "nt_epoch =    360  elapsed = 02:56  loss = 2.8485e-03  error = 6.4175e-01  l1 = 0.907531  l2 = 0.006974\n",
            "nt_epoch =    370  elapsed = 02:57  loss = 2.6941e-03  error = 6.4568e-01  l1 = 0.920546  l2 = 0.007041\n",
            "nt_epoch =    380  elapsed = 02:58  loss = 2.5401e-03  error = 6.3447e-01  l1 = 0.942858  l2 = 0.007040\n",
            "nt_epoch =    390  elapsed = 02:59  loss = 2.3545e-03  error = 6.2577e-01  l1 = 0.965312  l2 = 0.007056\n",
            "nt_epoch =    400  elapsed = 03:01  loss = 2.2150e-03  error = 6.1584e-01  l1 = 0.970546  l2 = 0.007010\n",
            "nt_epoch =    410  elapsed = 03:02  loss = 2.1212e-03  error = 6.0058e-01  l1 = 0.973210  l2 = 0.006921\n",
            "nt_epoch =    420  elapsed = 03:03  loss = 1.9489e-03  error = 5.7872e-01  l1 = 0.974917  l2 = 0.006788\n",
            "nt_epoch =    430  elapsed = 03:04  loss = 1.8226e-03  error = 5.5815e-01  l1 = 0.965612  l2 = 0.006627\n",
            "nt_epoch =    440  elapsed = 03:05  loss = 1.6765e-03  error = 5.3173e-01  l1 = 0.957432  l2 = 0.006433\n",
            "nt_epoch =    450  elapsed = 03:06  loss = 1.5792e-03  error = 5.1428e-01  l1 = 0.965126  l2 = 0.006346\n",
            "nt_epoch =    460  elapsed = 03:07  loss = 1.5200e-03  error = 4.9978e-01  l1 = 0.961149  l2 = 0.006241\n",
            "nt_epoch =    470  elapsed = 03:09  loss = 1.4515e-03  error = 4.7699e-01  l1 = 0.957164  l2 = 0.006083\n",
            "nt_epoch =    480  elapsed = 03:10  loss = 1.3737e-03  error = 4.5344e-01  l1 = 0.955194  l2 = 0.005927\n",
            "nt_epoch =    490  elapsed = 03:11  loss = 1.3348e-03  error = 4.4458e-01  l1 = 0.966645  l2 = 0.005907\n",
            "nt_epoch =    500  elapsed = 03:12  loss = 1.2751e-03  error = 4.2943e-01  l1 = 0.975850  l2 = 0.005840\n",
            "nt_epoch =    510  elapsed = 03:13  loss = 1.2096e-03  error = 4.0376e-01  l1 = 0.978477  l2 = 0.005685\n",
            "nt_epoch =    520  elapsed = 03:14  loss = 1.1418e-03  error = 3.8214e-01  l1 = 0.982095  l2 = 0.005559\n",
            "nt_epoch =    530  elapsed = 03:15  loss = 1.0261e-03  error = 3.4897e-01  l1 = 0.968225  l2 = 0.005304\n",
            "nt_epoch =    540  elapsed = 03:17  loss = 9.7767e-04  error = 3.2998e-01  l1 = 0.959293  l2 = 0.005154\n",
            "nt_epoch =    550  elapsed = 03:18  loss = 9.4200e-04  error = 3.1896e-01  l1 = 0.962152  l2 = 0.005093\n",
            "nt_epoch =    560  elapsed = 03:19  loss = 8.9928e-04  error = 3.0573e-01  l1 = 0.962817  l2 = 0.005011\n",
            "nt_epoch =    570  elapsed = 03:20  loss = 8.6621e-04  error = 2.8544e-01  l1 = 0.964267  l2 = 0.004887\n",
            "nt_epoch =    580  elapsed = 03:21  loss = 8.3094e-04  error = 2.7076e-01  l1 = 0.964051  l2 = 0.004792\n",
            "nt_epoch =    590  elapsed = 03:22  loss = 8.0726e-04  error = 2.6283e-01  l1 = 0.965158  l2 = 0.004745\n",
            "nt_epoch =    600  elapsed = 03:23  loss = 7.8002e-04  error = 2.5822e-01  l1 = 0.965667  l2 = 0.004718\n",
            "nt_epoch =    610  elapsed = 03:25  loss = 7.4952e-04  error = 2.4636e-01  l1 = 0.962230  l2 = 0.004631\n",
            "nt_epoch =    620  elapsed = 03:26  loss = 7.3318e-04  error = 2.3886e-01  l1 = 0.967654  l2 = 0.004601\n",
            "nt_epoch =    630  elapsed = 03:27  loss = 7.0806e-04  error = 2.3472e-01  l1 = 0.966326  l2 = 0.004570\n",
            "nt_epoch =    640  elapsed = 03:28  loss = 6.7937e-04  error = 2.1475e-01  l1 = 0.964458  l2 = 0.004437\n",
            "nt_epoch =    650  elapsed = 03:29  loss = 6.2907e-04  error = 2.0257e-01  l1 = 0.970783  l2 = 0.004380\n",
            "nt_epoch =    660  elapsed = 03:30  loss = 5.8949e-04  error = 1.9478e-01  l1 = 0.963752  l2 = 0.004308\n",
            "nt_epoch =    670  elapsed = 03:32  loss = 5.6067e-04  error = 1.8338e-01  l1 = 0.966920  l2 = 0.004245\n",
            "nt_epoch =    680  elapsed = 03:33  loss = 5.2638e-04  error = 1.7096e-01  l1 = 0.977806  l2 = 0.004201\n",
            "nt_epoch =    690  elapsed = 03:34  loss = 5.0443e-04  error = 1.7229e-01  l1 = 0.983268  l2 = 0.004227\n",
            "nt_epoch =    700  elapsed = 03:35  loss = 4.7387e-04  error = 1.5640e-01  l1 = 0.984007  l2 = 0.004128\n",
            "nt_epoch =    710  elapsed = 03:36  loss = 4.4169e-04  error = 1.4664e-01  l1 = 0.984739  l2 = 0.004068\n",
            "nt_epoch =    720  elapsed = 03:37  loss = 4.1785e-04  error = 1.3704e-01  l1 = 0.987923  l2 = 0.004017\n",
            "nt_epoch =    730  elapsed = 03:38  loss = 3.9392e-04  error = 1.2932e-01  l1 = 0.989543  l2 = 0.003973\n",
            "nt_epoch =    740  elapsed = 03:39  loss = 3.7277e-04  error = 1.2522e-01  l1 = 0.992388  l2 = 0.003956\n",
            "nt_epoch =    750  elapsed = 03:41  loss = 3.5740e-04  error = 1.1806e-01  l1 = 0.992574  l2 = 0.003911\n",
            "nt_epoch =    760  elapsed = 03:42  loss = 3.4894e-04  error = 1.1250e-01  l1 = 0.991604  l2 = 0.003873\n",
            "nt_epoch =    770  elapsed = 03:43  loss = 3.3899e-04  error = 1.0895e-01  l1 = 0.989738  l2 = 0.003844\n",
            "nt_epoch =    780  elapsed = 03:44  loss = 3.2471e-04  error = 1.0372e-01  l1 = 0.988364  l2 = 0.003806\n",
            "nt_epoch =    790  elapsed = 03:45  loss = 3.1829e-04  error = 1.0304e-01  l1 = 0.987924  l2 = 0.003801\n",
            "nt_epoch =    800  elapsed = 03:46  loss = 3.0776e-04  error = 9.9617e-02  l1 = 0.986270  l2 = 0.003774\n",
            "nt_epoch =    810  elapsed = 03:48  loss = 3.0003e-04  error = 9.8536e-02  l1 = 0.986249  l2 = 0.003767\n",
            "nt_epoch =    820  elapsed = 03:49  loss = 2.8791e-04  error = 9.2661e-02  l1 = 0.988537  l2 = 0.003737\n",
            "nt_epoch =    830  elapsed = 03:50  loss = 2.8004e-04  error = 9.0399e-02  l1 = 0.989118  l2 = 0.003724\n",
            "nt_epoch =    840  elapsed = 03:51  loss = 2.7457e-04  error = 8.8433e-02  l1 = 0.990015  l2 = 0.003714\n",
            "nt_epoch =    850  elapsed = 03:52  loss = 2.7102e-04  error = 8.6346e-02  l1 = 0.990101  l2 = 0.003701\n",
            "nt_epoch =    860  elapsed = 03:53  loss = 2.6618e-04  error = 8.3846e-02  l1 = 0.991621  l2 = 0.003690\n",
            "nt_epoch =    870  elapsed = 03:55  loss = 2.6021e-04  error = 8.1116e-02  l1 = 0.992198  l2 = 0.003675\n",
            "nt_epoch =    880  elapsed = 03:56  loss = 2.5450e-04  error = 7.9881e-02  l1 = 0.993493  l2 = 0.003671\n",
            "nt_epoch =    890  elapsed = 03:57  loss = 2.4862e-04  error = 7.6633e-02  l1 = 0.992534  l2 = 0.003647\n",
            "nt_epoch =    900  elapsed = 03:58  loss = 2.4101e-04  error = 7.2747e-02  l1 = 0.992647  l2 = 0.003623\n",
            "nt_epoch =    910  elapsed = 03:59  loss = 2.3697e-04  error = 7.1985e-02  l1 = 0.993399  l2 = 0.003620\n",
            "nt_epoch =    920  elapsed = 04:00  loss = 2.3026e-04  error = 6.7384e-02  l1 = 0.993050  l2 = 0.003590\n",
            "nt_epoch =    930  elapsed = 04:02  loss = 2.2598e-04  error = 6.4905e-02  l1 = 0.993305  l2 = 0.003575\n",
            "nt_epoch =    940  elapsed = 04:03  loss = 2.2171e-04  error = 6.2661e-02  l1 = 0.992913  l2 = 0.003559\n",
            "nt_epoch =    950  elapsed = 04:04  loss = 2.1806e-04  error = 6.0430e-02  l1 = 0.993354  l2 = 0.003547\n",
            "nt_epoch =    960  elapsed = 04:05  loss = 2.1318e-04  error = 5.8935e-02  l1 = 0.994080  l2 = 0.003539\n",
            "nt_epoch =    970  elapsed = 04:06  loss = 2.1126e-04  error = 5.7587e-02  l1 = 0.993881  l2 = 0.003530\n",
            "nt_epoch =    980  elapsed = 04:07  loss = 2.0829e-04  error = 5.6364e-02  l1 = 0.993899  l2 = 0.003523\n",
            "nt_epoch =    990  elapsed = 04:09  loss = 2.0582e-04  error = 5.3578e-02  l1 = 0.993495  l2 = 0.003503\n",
            "==================\n",
            "Training finished (epoch 100): duration = 04:10  error = 5.1054e-02  l1 = 0.993525  l2 = 0.003488\n",
            "l1:  0.98081917\n",
            "l2:  0.0032916232\n",
            "l1_noise:  0.99352545\n",
            "l2_noise:  0.0034875113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y-y6mkKpP5M8",
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "5d7db632-b1b5-481a-d5b2-ac26a77acdd1"
      },
      "source": [
        "plot_ide_cont_results(X_star, u_pred, X_u_train, u_train,\n",
        "  Exact_u, X, T, x, t, lambda_1_pred, lambda_1_pred_noise, lambda_2_pred, lambda_2_pred_noise)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAFnCAYAAAD3+Q2cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0BioiIgoaAxUREQUtP8Hr1sn2l8xokQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 388.543x336.186 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bcY8EfT1w1ev"
      },
      "source": [
        "# 4. Discrete Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SFBJUhYqw1ew"
      },
      "source": [
        "$$u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0$$\n",
        "\n",
        "With $\\lambda_1$ and $\\lambda_2$ real parameters of the differential operator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lKxgAhrTw1ex"
      },
      "source": [
        "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
        "$$f := u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx}.$$\n",
        "\n",
        "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
        "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_u}|f(t_u^i,x_u^i)|^2,$$\n",
        "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ respectively the trainring data on $u(t,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fGrMDRc3w1ex"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yOuJ6DV1w1ey",
        "lines_to_next_cell": 2,
        "colab": {}
      },
      "source": [
        "\n",
        "# Data size on initial condition on u\n",
        "N_0 = 199\n",
        "N_1 = 201\n",
        "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q-sized output defined later [u_1^n(x), ..., u_{q+1}^n(x)]\n",
        "layers = [1, 50, 50, 50, 0]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 100\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  lr=0.001,\n",
        "  beta_1=0.9,\n",
        "  beta_2=0.999,\n",
        "  epsilon=1e-08)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 2000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rRGW4IW0w1e0"
      },
      "source": [
        "## PINN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TYMIf2_Uw1e0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5yiB3TOYw1e1",
        "colab": {}
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta):\n",
        "    self.lb = lb\n",
        "    self.ub = ub\n",
        "\n",
        "    self.dt = dt\n",
        "\n",
        "    self.q = max(q,1)\n",
        "    self.IRK_alpha = IRK_alpha\n",
        "    self.IRK_beta = IRK_beta\n",
        "\n",
        "    # Descriptive Keras model [2, 50, …, 50, q+1]\n",
        "    self.U_model = tf.keras.Sequential()\n",
        "    self.U_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.U_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.U_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "  def __autograd(self, U, x, dummy):\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(x)\n",
        "      tape.watch(dummy)\n",
        "\n",
        "      # Getting the prediction\n",
        "      U = self.U_model(x) # shape=(len(x), q)\n",
        "\n",
        "      # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
        "      g_U = tape.gradient(U, x, output_gradients=dummy)\n",
        "      U_x = tape.gradient(g_U, dummy)\n",
        "      g_U_x = tape.gradient(U_x, x, output_gradients=dummy)\n",
        "    \n",
        "    # Doing the last one outside the with, to optimize performance\n",
        "    # Impossible to do for the earlier grad, because they’re needed after\n",
        "    U_xx = tape.gradient(g_U_x, dummy)\n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "    return U_x, U_xx\n",
        "\n",
        "  def U_0_model(self, x, customDummy=None):\n",
        "    U = self.U_model(x)\n",
        "    if customDummy != None:\n",
        "      dummy = customDummy\n",
        "    else:\n",
        "      dummy = self.dummy_x_0\n",
        "    U_x, U_xx = self.__autograd(U, x, dummy)\n",
        "\n",
        "    # Buidling the PINNs\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    N = l1*U*U_x - l2*U_xx # shape=(len(x), q)\n",
        "    return U + self.dt*tf.matmul(N, self.IRK_alpha.T)\n",
        "\n",
        "  def U_1_model(self, x, customDummy=None):\n",
        "    U = self.U_model(x)\n",
        "    #dummy = customDummy or self.dummy_x_1\n",
        "    if customDummy != None:\n",
        "      dummy = customDummy\n",
        "    else:\n",
        "      dummy = self.dummy_x_1\n",
        "    U_x, U_xx = self.__autograd(U, x, dummy)\n",
        "\n",
        "    # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    N = -l1*U*U_x + l2*U_xx # shape=(len(x), q)\n",
        "    return U + self.dt*tf.matmul(N, (self.IRK_beta - self.IRK_alpha).T)\n",
        "\n",
        "  # Defining custom loss\n",
        "  def __loss(self, x_0, u_0, x_1, u_1):\n",
        "    u_0_pred = self.U_0_model(x_0)\n",
        "    u_1_pred = self.U_1_model(x_1)\n",
        "    return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
        "      tf.reduce_sum(tf.square(u_1_pred - u_1))\n",
        "\n",
        "  def __grad(self, x_0, u_0, x_1, u_1):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(x_0, u_0, x_1, u_1)\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.U_model.trainable_variables\n",
        "    var.extend([self.lambda_1, self.lambda_2])\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "      w = []\n",
        "      for layer in self.U_model.layers[1:]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "      w.extend(self.lambda_1.numpy())\n",
        "      w.extend(self.lambda_2.numpy())\n",
        "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.U_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "    self.lambda_1.assign([w[-2]])\n",
        "    self.lambda_2.assign([w[-1]])\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    if numpy:\n",
        "      return l1.numpy()[0], l2.numpy()[0]\n",
        "    return l1, l2\n",
        "\n",
        "  def summary(self):\n",
        "    return self.U_model.summary()\n",
        "\n",
        "  def __createDummy(self, x):\n",
        "    return tf.ones([x.shape[0], self.q], dtype=self.dtype)\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, x_0, u_0, x_1, u_1, tf_epochs=1):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
        "    u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
        "    x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
        "    u_1 = tf.convert_to_tensor(u_1, dtype=self.dtype)\n",
        "\n",
        "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
        "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
        "\n",
        "    # Creating dummy tensors for the gradients\n",
        "    self.dummy_x_0 = self.__createDummy(x_0)\n",
        "    self.dummy_x_1 = self.__createDummy(x_1)\n",
        "\n",
        "    def log_train_epoch(epoch, loss, is_iter):\n",
        "      l1, l2 = self.get_params(numpy=True)\n",
        "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
        "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(x_0, u_0, x_1, u_1)\n",
        "      self.optimizer.apply_gradients(\n",
        "        zip(grads, self.__wrap_training_variables()))\n",
        "      log_train_epoch(epoch, loss_value, False)\n",
        "    \n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        tape.watch(self.lambda_1)\n",
        "        tape.watch(self.lambda_2)\n",
        "        loss_value = self.__loss(x_0, u_0, x_1, u_1)\n",
        "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True, log_train_epoch)\n",
        "    \n",
        "    l1, l2 = self.get_params(numpy=True)\n",
        "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
        "\n",
        "  def predict(self, x_star):\n",
        "    x_star = tf.convert_to_tensor(x_star, dtype=self.dtype)\n",
        "    dummy = self.__createDummy(x_star)\n",
        "    U_0_star = self.U_0_model(x_star, dummy)\n",
        "    U_1_star = self.U_1_model(x_star, dummy)\n",
        "    return U_0_star, U_1_star"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JFS_u67iw1e2"
      },
      "source": [
        "## Training and plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sU5VfXKsw1e3",
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4725688b-b927-4216-dc2a-4d1460006225"
      },
      "source": [
        "\n",
        "# Setup\n",
        "lb = np.array([-1.0])\n",
        "ub = np.array([1.0])\n",
        "idx_t_0 = 10\n",
        "skip = 80\n",
        "idx_t_1 = idx_t_0 + skip\n",
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x_0, u_0, x_1, u_1, x_star, t_star, dt, q, \\\n",
        "  Exact_u, IRK_alpha, IRK_beta = prep_data(path, N_0=N_0, N_1=N_1,\n",
        "  lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
        "lambdas_star = (1.0, 0.01/np.pi)\n",
        "\n",
        "# Setting the output layer dynamically\n",
        "layers[-1] = q\n",
        " \n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta)\n",
        "def error():\n",
        "  l1, l2 = pinn.get_params(numpy=True)\n",
        "  l1_star, l2_star = lambdas_star\n",
        "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
        "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
        "  return (error_lambda_1 + error_lambda_2) / 2\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(x_0, u_0, x_1, u_1, tf_epochs)\n",
        "\n",
        "# Getting the model predictions\n",
        "U_0_pred, U_1_pred = pinn.predict(x_star)\n",
        "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
        "\n",
        "# Noisy case (same as before with a different noise)\n",
        "x_0, u_0, x_1, u_1, x_star, t_star, dt, q, \\\n",
        "  Exact_u, IRK_alpha, IRK_beta = prep_data(path, N_0=N_0, N_1=N_1,\n",
        "  lb=lb, ub=ub, noise=0.01, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
        "layers[-1] = q\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta)\n",
        "pinn.fit(x_0, u_0, x_1, u_1, tf_epochs)\n",
        "U_0_pred, U_1_pred = pinn.predict(x_star)\n",
        "lambda_1_pred_noisy, lambda_2_pred_noisy = pinn.get_params(numpy=True)\n",
        "\n",
        "print(\"l1: \", lambda_1_pred)\n",
        "print(\"l2: \", lambda_2_pred)\n",
        "print(\"noisy l1: \", lambda_1_pred_noisy)\n",
        "print(\"noisy l2: \", lambda_2_pred_noisy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0805 21:29:55.733261 140462172571520 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "W0805 21:29:55.741581 140462172571520 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "W0805 21:29:55.757947 140462172571520 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.0.0-beta1\n",
            "Eager execution: True\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_4 (Lambda)            (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 50)                100       \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 81)                4131      \n",
            "=================================================================\n",
            "Total params: 9,331\n",
            "Trainable params: 9,331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "tf_epoch =      0  elapsed = 00:00  loss = 1.1813e+04  error = 6.1075e-01  l1 = -0.001000  l2 = 0.002481\n",
            "tf_epoch =     10  elapsed = 00:01  loss = 9.9691e+03  error = 6.1178e-01  l1 = -0.011066  l2 = 0.002507\n",
            "tf_epoch =     20  elapsed = 00:02  loss = 7.9099e+03  error = 6.1293e-01  l1 = -0.022113  l2 = 0.002535\n",
            "tf_epoch =     30  elapsed = 00:03  loss = 6.3321e+03  error = 6.1418e-01  l1 = -0.034568  l2 = 0.002566\n",
            "tf_epoch =     40  elapsed = 00:04  loss = 5.8366e+03  error = 6.1542e-01  l1 = -0.048344  l2 = 0.002602\n",
            "tf_epoch =     50  elapsed = 00:05  loss = 5.5025e+03  error = 6.1640e-01  l1 = -0.062319  l2 = 0.002640\n",
            "tf_epoch =     60  elapsed = 00:07  loss = 5.0937e+03  error = 6.1690e-01  l1 = -0.075224  l2 = 0.002678\n",
            "tf_epoch =     70  elapsed = 00:08  loss = 4.5730e+03  error = 6.1672e-01  l1 = -0.086180  l2 = 0.002714\n",
            "tf_epoch =     80  elapsed = 00:09  loss = 3.9713e+03  error = 6.1589e-01  l1 = -0.094153  l2 = 0.002745\n",
            "tf_epoch =     90  elapsed = 00:10  loss = 3.4099e+03  error = 6.1530e-01  l1 = -0.097546  l2 = 0.002760\n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 00:12  loss = 1.2870e+03  error = 3.2565e-01  l1 = 0.364494  l2 = 0.003233\n",
            "nt_epoch =     20  elapsed = 00:13  loss = 6.7773e+02  error = 8.2816e-01  l1 = 0.469211  l2 = 0.006766\n",
            "nt_epoch =     30  elapsed = 00:15  loss = 4.7834e+02  error = 2.4539e+00  l1 = 0.729530  l2 = 0.017944\n",
            "nt_epoch =     40  elapsed = 00:16  loss = 3.5564e+02  error = 2.7200e+00  l1 = 0.984380  l2 = 0.020449\n",
            "nt_epoch =     50  elapsed = 00:18  loss = 2.9623e+02  error = 2.4826e+00  l1 = 0.995968  l2 = 0.018975\n",
            "nt_epoch =     60  elapsed = 00:19  loss = 2.3519e+02  error = 2.3837e+00  l1 = 1.006340  l2 = 0.018338\n",
            "nt_epoch =     70  elapsed = 00:20  loss = 1.7752e+02  error = 1.9880e+00  l1 = 0.971393  l2 = 0.015748\n",
            "nt_epoch =     80  elapsed = 00:22  loss = 1.4324e+02  error = 1.6854e+00  l1 = 0.937183  l2 = 0.013713\n",
            "nt_epoch =     90  elapsed = 00:23  loss = 1.2228e+02  error = 1.5410e+00  l1 = 0.948462  l2 = 0.012829\n",
            "nt_epoch =    100  elapsed = 00:25  loss = 1.0669e+02  error = 1.4480e+00  l1 = 0.974106  l2 = 0.012319\n",
            "nt_epoch =    110  elapsed = 00:26  loss = 9.1946e+01  error = 1.3136e+00  l1 = 0.990890  l2 = 0.011517\n",
            "nt_epoch =    120  elapsed = 00:28  loss = 7.6001e+01  error = 1.2365e+00  l1 = 0.994493  l2 = 0.011038\n",
            "nt_epoch =    130  elapsed = 00:29  loss = 6.4507e+01  error = 1.1971e+00  l1 = 1.000269  l2 = 0.010803\n",
            "nt_epoch =    140  elapsed = 00:31  loss = 5.5918e+01  error = 1.1813e+00  l1 = 1.006957  l2 = 0.010682\n",
            "nt_epoch =    150  elapsed = 00:32  loss = 4.8202e+01  error = 1.0281e+00  l1 = 1.002090  l2 = 0.009721\n",
            "nt_epoch =    160  elapsed = 00:34  loss = 4.3994e+01  error = 9.9356e-01  l1 = 1.007602  l2 = 0.009484\n",
            "nt_epoch =    170  elapsed = 00:35  loss = 3.9932e+01  error = 9.2204e-01  l1 = 0.998968  l2 = 0.009050\n",
            "nt_epoch =    180  elapsed = 00:37  loss = 3.6425e+01  error = 8.9369e-01  l1 = 0.994200  l2 = 0.008854\n",
            "nt_epoch =    190  elapsed = 00:38  loss = 3.3725e+01  error = 8.6143e-01  l1 = 0.998479  l2 = 0.008662\n",
            "nt_epoch =    200  elapsed = 00:39  loss = 3.0683e+01  error = 8.1752e-01  l1 = 0.988564  l2 = 0.008351\n",
            "nt_epoch =    210  elapsed = 00:41  loss = 2.7897e+01  error = 7.8483e-01  l1 = 0.987215  l2 = 0.008139\n",
            "nt_epoch =    220  elapsed = 00:42  loss = 2.4941e+01  error = 7.3155e-01  l1 = 0.984964  l2 = 0.007792\n",
            "nt_epoch =    230  elapsed = 00:44  loss = 2.2577e+01  error = 6.9378e-01  l1 = 0.980762  l2 = 0.007539\n",
            "nt_epoch =    240  elapsed = 00:45  loss = 2.1197e+01  error = 6.7455e-01  l1 = 0.985058  l2 = 0.007430\n",
            "nt_epoch =    250  elapsed = 00:47  loss = 1.9865e+01  error = 6.4613e-01  l1 = 0.978922  l2 = 0.007229\n",
            "nt_epoch =    260  elapsed = 00:48  loss = 1.9040e+01  error = 6.3535e-01  l1 = 0.982749  l2 = 0.007173\n",
            "nt_epoch =    270  elapsed = 00:50  loss = 1.8037e+01  error = 6.2022e-01  l1 = 0.981052  l2 = 0.007071\n",
            "nt_epoch =    280  elapsed = 00:51  loss = 1.7216e+01  error = 6.0933e-01  l1 = 0.980408  l2 = 0.007000\n",
            "nt_epoch =    290  elapsed = 00:53  loss = 1.6463e+01  error = 5.9877e-01  l1 = 0.982428  l2 = 0.006939\n",
            "nt_epoch =    300  elapsed = 00:54  loss = 1.5756e+01  error = 5.8959e-01  l1 = 0.984641  l2 = 0.006888\n",
            "nt_epoch =    310  elapsed = 00:56  loss = 1.4968e+01  error = 5.7482e-01  l1 = 0.981601  l2 = 0.006784\n",
            "nt_epoch =    320  elapsed = 00:57  loss = 1.4316e+01  error = 5.7950e-01  l1 = 0.984629  l2 = 0.006823\n",
            "nt_epoch =    330  elapsed = 00:58  loss = 1.3615e+01  error = 5.6916e-01  l1 = 0.980470  l2 = 0.006744\n",
            "nt_epoch =    340  elapsed = 01:00  loss = 1.3004e+01  error = 5.6093e-01  l1 = 0.985142  l2 = 0.006707\n",
            "nt_epoch =    350  elapsed = 01:01  loss = 1.2404e+01  error = 5.5615e-01  l1 = 0.983455  l2 = 0.006671\n",
            "nt_epoch =    360  elapsed = 01:03  loss = 1.1788e+01  error = 5.4870e-01  l1 = 0.982986  l2 = 0.006622\n",
            "nt_epoch =    370  elapsed = 01:04  loss = 1.1310e+01  error = 5.3179e-01  l1 = 0.981203  l2 = 0.006509\n",
            "nt_epoch =    380  elapsed = 01:06  loss = 1.0892e+01  error = 5.2246e-01  l1 = 0.980921  l2 = 0.006448\n",
            "nt_epoch =    390  elapsed = 01:07  loss = 1.0416e+01  error = 5.1746e-01  l1 = 0.981611  l2 = 0.006419\n",
            "nt_epoch =    400  elapsed = 01:09  loss = 9.9818e+00  error = 5.0646e-01  l1 = 0.982039  l2 = 0.006350\n",
            "nt_epoch =    410  elapsed = 01:10  loss = 9.6122e+00  error = 4.9305e-01  l1 = 0.980863  l2 = 0.006261\n",
            "nt_epoch =    420  elapsed = 01:11  loss = 9.2317e+00  error = 4.7748e-01  l1 = 0.980146  l2 = 0.006160\n",
            "nt_epoch =    430  elapsed = 01:13  loss = 8.7165e+00  error = 4.6033e-01  l1 = 0.980470  l2 = 0.006051\n",
            "nt_epoch =    440  elapsed = 01:14  loss = 8.2752e+00  error = 4.3906e-01  l1 = 0.983291  l2 = 0.005925\n",
            "nt_epoch =    450  elapsed = 01:16  loss = 7.8998e+00  error = 4.2935e-01  l1 = 0.982174  l2 = 0.005860\n",
            "nt_epoch =    460  elapsed = 01:17  loss = 7.6693e+00  error = 4.1701e-01  l1 = 0.980339  l2 = 0.005775\n",
            "nt_epoch =    470  elapsed = 01:19  loss = 7.2655e+00  error = 4.0856e-01  l1 = 0.982653  l2 = 0.005729\n",
            "nt_epoch =    480  elapsed = 01:20  loss = 6.9439e+00  error = 3.8906e-01  l1 = 0.981064  l2 = 0.005600\n",
            "nt_epoch =    490  elapsed = 01:22  loss = 6.6608e+00  error = 3.7876e-01  l1 = 0.981603  l2 = 0.005536\n",
            "nt_epoch =    500  elapsed = 01:23  loss = 6.3776e+00  error = 3.6545e-01  l1 = 0.982252  l2 = 0.005453\n",
            "nt_epoch =    510  elapsed = 01:25  loss = 6.1208e+00  error = 3.6399e-01  l1 = 0.981270  l2 = 0.005441\n",
            "nt_epoch =    520  elapsed = 01:26  loss = 5.8380e+00  error = 3.5217e-01  l1 = 0.983180  l2 = 0.005372\n",
            "nt_epoch =    530  elapsed = 01:28  loss = 5.5861e+00  error = 3.4196e-01  l1 = 0.982357  l2 = 0.005304\n",
            "nt_epoch =    540  elapsed = 01:29  loss = 5.3498e+00  error = 3.3594e-01  l1 = 0.984317  l2 = 0.005272\n",
            "nt_epoch =    550  elapsed = 01:31  loss = 5.1774e+00  error = 3.2639e-01  l1 = 0.982962  l2 = 0.005207\n",
            "nt_epoch =    560  elapsed = 01:32  loss = 5.0155e+00  error = 3.2338e-01  l1 = 0.983032  l2 = 0.005188\n",
            "nt_epoch =    570  elapsed = 01:33  loss = 4.8551e+00  error = 3.1400e-01  l1 = 0.982553  l2 = 0.005127\n",
            "nt_epoch =    580  elapsed = 01:35  loss = 4.7118e+00  error = 3.1093e-01  l1 = 0.984131  l2 = 0.005112\n",
            "nt_epoch =    590  elapsed = 01:36  loss = 4.5106e+00  error = 3.0599e-01  l1 = 0.982567  l2 = 0.005076\n",
            "nt_epoch =    600  elapsed = 01:38  loss = 4.5038e+00  error = 2.9348e-01  l1 = 0.982905  l2 = 0.004997\n",
            "nt_epoch =    610  elapsed = 01:39  loss = 4.2388e+00  error = 2.9332e-01  l1 = 0.981765  l2 = 0.004992\n",
            "nt_epoch =    620  elapsed = 01:41  loss = 4.1205e+00  error = 2.9032e-01  l1 = 0.982229  l2 = 0.004975\n",
            "nt_epoch =    630  elapsed = 01:42  loss = 4.0014e+00  error = 2.8697e-01  l1 = 0.981998  l2 = 0.004953\n",
            "nt_epoch =    640  elapsed = 01:44  loss = 3.8779e+00  error = 2.8511e-01  l1 = 0.980104  l2 = 0.004935\n",
            "nt_epoch =    650  elapsed = 01:45  loss = 3.7428e+00  error = 2.8299e-01  l1 = 0.981499  l2 = 0.004926\n",
            "nt_epoch =    660  elapsed = 01:46  loss = 3.6601e+00  error = 2.8273e-01  l1 = 0.980301  l2 = 0.004920\n",
            "nt_epoch =    670  elapsed = 01:48  loss = 3.6144e+00  error = 2.8139e-01  l1 = 0.980256  l2 = 0.004912\n",
            "nt_epoch =    680  elapsed = 01:49  loss = 3.5577e+00  error = 2.8401e-01  l1 = 0.978957  l2 = 0.004924\n",
            "nt_epoch =    690  elapsed = 01:51  loss = 3.4748e+00  error = 2.8186e-01  l1 = 0.980814  l2 = 0.004916\n",
            "nt_epoch =    700  elapsed = 01:52  loss = 3.3991e+00  error = 2.7965e-01  l1 = 0.979456  l2 = 0.004898\n",
            "nt_epoch =    710  elapsed = 01:54  loss = 3.3333e+00  error = 2.8078e-01  l1 = 0.979964  l2 = 0.004907\n",
            "nt_epoch =    720  elapsed = 01:55  loss = 3.2731e+00  error = 2.7806e-01  l1 = 0.980107  l2 = 0.004890\n",
            "nt_epoch =    730  elapsed = 01:56  loss = 3.2102e+00  error = 2.7937e-01  l1 = 0.979400  l2 = 0.004896\n",
            "nt_epoch =    740  elapsed = 01:58  loss = 3.1395e+00  error = 2.7962e-01  l1 = 0.980064  l2 = 0.004900\n",
            "nt_epoch =    750  elapsed = 01:59  loss = 3.0756e+00  error = 2.8008e-01  l1 = 0.980802  l2 = 0.004905\n",
            "nt_epoch =    760  elapsed = 02:01  loss = 3.0047e+00  error = 2.8261e-01  l1 = 0.979912  l2 = 0.004918\n",
            "nt_epoch =    770  elapsed = 02:02  loss = 2.9442e+00  error = 2.8387e-01  l1 = 0.981034  l2 = 0.004930\n",
            "nt_epoch =    780  elapsed = 02:04  loss = 2.8865e+00  error = 2.8232e-01  l1 = 0.981512  l2 = 0.004922\n",
            "nt_epoch =    790  elapsed = 02:05  loss = 2.8251e+00  error = 2.8203e-01  l1 = 0.981696  l2 = 0.004920\n",
            "nt_epoch =    800  elapsed = 02:06  loss = 2.7568e+00  error = 2.7896e-01  l1 = 0.982755  l2 = 0.004904\n",
            "nt_epoch =    810  elapsed = 02:08  loss = 2.7093e+00  error = 2.7606e-01  l1 = 0.983128  l2 = 0.004887\n",
            "nt_epoch =    820  elapsed = 02:09  loss = 2.6728e+00  error = 2.7635e-01  l1 = 0.983099  l2 = 0.004889\n",
            "nt_epoch =    830  elapsed = 02:11  loss = 2.6205e+00  error = 2.7495e-01  l1 = 0.984050  l2 = 0.004883\n",
            "nt_epoch =    840  elapsed = 02:12  loss = 2.5675e+00  error = 2.7257e-01  l1 = 0.982841  l2 = 0.004864\n",
            "nt_epoch =    850  elapsed = 02:14  loss = 2.5339e+00  error = 2.6951e-01  l1 = 0.984028  l2 = 0.004848\n",
            "nt_epoch =    860  elapsed = 02:15  loss = 2.4994e+00  error = 2.6765e-01  l1 = 0.984052  l2 = 0.004836\n",
            "nt_epoch =    870  elapsed = 02:16  loss = 2.4682e+00  error = 2.6556e-01  l1 = 0.983585  l2 = 0.004821\n",
            "nt_epoch =    880  elapsed = 02:18  loss = 2.4283e+00  error = 2.6312e-01  l1 = 0.984150  l2 = 0.004808\n",
            "nt_epoch =    890  elapsed = 02:19  loss = 2.3836e+00  error = 2.6051e-01  l1 = 0.984242  l2 = 0.004791\n",
            "nt_epoch =    900  elapsed = 02:21  loss = 2.3437e+00  error = 2.5786e-01  l1 = 0.984477  l2 = 0.004775\n",
            "nt_epoch =    910  elapsed = 02:22  loss = 2.3015e+00  error = 2.5636e-01  l1 = 0.984475  l2 = 0.004766\n",
            "nt_epoch =    920  elapsed = 02:24  loss = 2.2732e+00  error = 2.5646e-01  l1 = 0.984714  l2 = 0.004767\n",
            "nt_epoch =    930  elapsed = 02:25  loss = 2.2259e+00  error = 2.5437e-01  l1 = 0.984968  l2 = 0.004755\n",
            "nt_epoch =    940  elapsed = 02:27  loss = 2.1708e+00  error = 2.5111e-01  l1 = 0.985133  l2 = 0.004734\n",
            "nt_epoch =    950  elapsed = 02:28  loss = 2.1326e+00  error = 2.5003e-01  l1 = 0.985617  l2 = 0.004729\n",
            "nt_epoch =    960  elapsed = 02:30  loss = 2.1003e+00  error = 2.4752e-01  l1 = 0.986266  l2 = 0.004715\n",
            "nt_epoch =    970  elapsed = 02:31  loss = 2.0698e+00  error = 2.4538e-01  l1 = 0.986517  l2 = 0.004702\n",
            "nt_epoch =    980  elapsed = 02:33  loss = 2.0376e+00  error = 2.4290e-01  l1 = 0.986308  l2 = 0.004686\n",
            "nt_epoch =    990  elapsed = 02:34  loss = 2.0140e+00  error = 2.4100e-01  l1 = 0.986927  l2 = 0.004676\n",
            "nt_epoch =   1000  elapsed = 02:36  loss = 1.9837e+00  error = 2.3782e-01  l1 = 0.986883  l2 = 0.004655\n",
            "nt_epoch =   1010  elapsed = 02:37  loss = 1.9632e+00  error = 2.3694e-01  l1 = 0.986752  l2 = 0.004649\n",
            "nt_epoch =   1020  elapsed = 02:39  loss = 1.9437e+00  error = 2.3592e-01  l1 = 0.987598  l2 = 0.004646\n",
            "nt_epoch =   1030  elapsed = 02:40  loss = 1.9210e+00  error = 2.3282e-01  l1 = 0.987518  l2 = 0.004626\n",
            "nt_epoch =   1040  elapsed = 02:42  loss = 1.8990e+00  error = 2.3094e-01  l1 = 0.987191  l2 = 0.004613\n",
            "nt_epoch =   1050  elapsed = 02:43  loss = 1.8843e+00  error = 2.2965e-01  l1 = 0.987708  l2 = 0.004606\n",
            "nt_epoch =   1060  elapsed = 02:45  loss = 1.8653e+00  error = 2.2845e-01  l1 = 0.987327  l2 = 0.004597\n",
            "nt_epoch =   1070  elapsed = 02:46  loss = 1.8545e+00  error = 2.2630e-01  l1 = 0.987603  l2 = 0.004584\n",
            "nt_epoch =   1080  elapsed = 02:48  loss = 1.8393e+00  error = 2.2482e-01  l1 = 0.988261  l2 = 0.004577\n",
            "nt_epoch =   1090  elapsed = 02:49  loss = 1.8214e+00  error = 2.2260e-01  l1 = 0.988172  l2 = 0.004563\n",
            "nt_epoch =   1100  elapsed = 02:51  loss = 1.7973e+00  error = 2.1753e-01  l1 = 0.988351  l2 = 0.004531\n",
            "nt_epoch =   1110  elapsed = 02:52  loss = 1.7792e+00  error = 2.1794e-01  l1 = 0.988297  l2 = 0.004533\n",
            "nt_epoch =   1120  elapsed = 02:53  loss = 1.7589e+00  error = 2.1456e-01  l1 = 0.988445  l2 = 0.004512\n",
            "nt_epoch =   1130  elapsed = 02:55  loss = 1.7439e+00  error = 2.1395e-01  l1 = 0.988194  l2 = 0.004508\n",
            "nt_epoch =   1140  elapsed = 02:56  loss = 1.7262e+00  error = 2.1158e-01  l1 = 0.989040  l2 = 0.004495\n",
            "nt_epoch =   1150  elapsed = 02:58  loss = 1.7138e+00  error = 2.0777e-01  l1 = 0.987479  l2 = 0.004466\n",
            "nt_epoch =   1160  elapsed = 02:59  loss = 1.6880e+00  error = 2.0627e-01  l1 = 0.988296  l2 = 0.004459\n",
            "nt_epoch =   1170  elapsed = 03:01  loss = 1.6776e+00  error = 2.0597e-01  l1 = 0.988464  l2 = 0.004458\n",
            "nt_epoch =   1180  elapsed = 03:02  loss = 1.6539e+00  error = 2.0123e-01  l1 = 0.988734  l2 = 0.004428\n",
            "nt_epoch =   1190  elapsed = 03:04  loss = 1.6385e+00  error = 2.0211e-01  l1 = 0.988796  l2 = 0.004434\n",
            "nt_epoch =   1200  elapsed = 03:05  loss = 1.6175e+00  error = 1.9975e-01  l1 = 0.988140  l2 = 0.004417\n",
            "nt_epoch =   1210  elapsed = 03:07  loss = 1.6050e+00  error = 1.9987e-01  l1 = 0.988035  l2 = 0.004417\n",
            "nt_epoch =   1220  elapsed = 03:08  loss = 1.5929e+00  error = 1.9762e-01  l1 = 0.988199  l2 = 0.004404\n",
            "nt_epoch =   1230  elapsed = 03:09  loss = 1.5775e+00  error = 1.9676e-01  l1 = 0.987910  l2 = 0.004397\n",
            "nt_epoch =   1240  elapsed = 03:11  loss = 1.5616e+00  error = 1.9532e-01  l1 = 0.988591  l2 = 0.004390\n",
            "nt_epoch =   1250  elapsed = 03:12  loss = 1.5512e+00  error = 1.9523e-01  l1 = 0.988301  l2 = 0.004389\n",
            "nt_epoch =   1260  elapsed = 03:14  loss = 1.5433e+00  error = 1.9380e-01  l1 = 0.988073  l2 = 0.004379\n",
            "nt_epoch =   1270  elapsed = 03:15  loss = 1.5344e+00  error = 1.9382e-01  l1 = 0.988197  l2 = 0.004379\n",
            "nt_epoch =   1280  elapsed = 03:17  loss = 1.5254e+00  error = 1.9325e-01  l1 = 0.988190  l2 = 0.004376\n",
            "nt_epoch =   1290  elapsed = 03:18  loss = 1.5124e+00  error = 1.9097e-01  l1 = 0.988184  l2 = 0.004361\n",
            "nt_epoch =   1300  elapsed = 03:20  loss = 1.5012e+00  error = 1.9033e-01  l1 = 0.988254  l2 = 0.004357\n",
            "nt_epoch =   1310  elapsed = 03:21  loss = 1.4859e+00  error = 1.8963e-01  l1 = 0.988438  l2 = 0.004353\n",
            "nt_epoch =   1320  elapsed = 03:22  loss = 1.4679e+00  error = 1.8987e-01  l1 = 0.987531  l2 = 0.004352\n",
            "nt_epoch =   1330  elapsed = 03:24  loss = 1.4532e+00  error = 1.8941e-01  l1 = 0.987888  l2 = 0.004350\n",
            "nt_epoch =   1340  elapsed = 03:25  loss = 1.4415e+00  error = 1.8934e-01  l1 = 0.987352  l2 = 0.004348\n",
            "nt_epoch =   1350  elapsed = 03:27  loss = 1.4343e+00  error = 1.8887e-01  l1 = 0.987553  l2 = 0.004346\n",
            "nt_epoch =   1360  elapsed = 03:28  loss = 1.4252e+00  error = 1.8892e-01  l1 = 0.987669  l2 = 0.004347\n",
            "nt_epoch =   1370  elapsed = 03:30  loss = 1.4144e+00  error = 1.8781e-01  l1 = 0.986946  l2 = 0.004337\n",
            "nt_epoch =   1380  elapsed = 03:31  loss = 1.4040e+00  error = 1.8801e-01  l1 = 0.987331  l2 = 0.004340\n",
            "nt_epoch =   1390  elapsed = 03:33  loss = 1.3939e+00  error = 1.8776e-01  l1 = 0.987417  l2 = 0.004338\n",
            "nt_epoch =   1400  elapsed = 03:34  loss = 1.3842e+00  error = 1.8661e-01  l1 = 0.987222  l2 = 0.004330\n",
            "nt_epoch =   1410  elapsed = 03:36  loss = 1.3740e+00  error = 1.8545e-01  l1 = 0.987135  l2 = 0.004323\n",
            "nt_epoch =   1420  elapsed = 03:37  loss = 1.3646e+00  error = 1.8436e-01  l1 = 0.987631  l2 = 0.004317\n",
            "nt_epoch =   1430  elapsed = 03:39  loss = 1.3545e+00  error = 1.8364e-01  l1 = 0.986924  l2 = 0.004311\n",
            "nt_epoch =   1440  elapsed = 03:40  loss = 1.3450e+00  error = 1.8282e-01  l1 = 0.987095  l2 = 0.004306\n",
            "nt_epoch =   1450  elapsed = 03:41  loss = 1.3359e+00  error = 1.8232e-01  l1 = 0.986943  l2 = 0.004302\n",
            "nt_epoch =   1460  elapsed = 03:43  loss = 1.3273e+00  error = 1.8126e-01  l1 = 0.987366  l2 = 0.004297\n",
            "nt_epoch =   1470  elapsed = 03:44  loss = 1.3176e+00  error = 1.8013e-01  l1 = 0.986575  l2 = 0.004287\n",
            "nt_epoch =   1480  elapsed = 03:46  loss = 1.3080e+00  error = 1.7912e-01  l1 = 0.987455  l2 = 0.004283\n",
            "nt_epoch =   1490  elapsed = 03:47  loss = 1.3004e+00  error = 1.7871e-01  l1 = 0.987576  l2 = 0.004281\n",
            "nt_epoch =   1500  elapsed = 03:49  loss = 1.2883e+00  error = 1.7707e-01  l1 = 0.987428  l2 = 0.004270\n",
            "nt_epoch =   1510  elapsed = 03:50  loss = 1.2800e+00  error = 1.7661e-01  l1 = 0.987495  l2 = 0.004268\n",
            "nt_epoch =   1520  elapsed = 03:52  loss = 1.2689e+00  error = 1.7579e-01  l1 = 0.987301  l2 = 0.004262\n",
            "nt_epoch =   1530  elapsed = 03:53  loss = 1.2554e+00  error = 1.7496e-01  l1 = 0.988037  l2 = 0.004259\n",
            "nt_epoch =   1540  elapsed = 03:55  loss = 1.2466e+00  error = 1.7440e-01  l1 = 0.988193  l2 = 0.004256\n",
            "nt_epoch =   1550  elapsed = 03:56  loss = 1.2368e+00  error = 1.7403e-01  l1 = 0.987664  l2 = 0.004252\n",
            "nt_epoch =   1560  elapsed = 03:57  loss = 1.2289e+00  error = 1.7372e-01  l1 = 0.988254  l2 = 0.004252\n",
            "nt_epoch =   1570  elapsed = 03:59  loss = 1.2175e+00  error = 1.7248e-01  l1 = 0.988070  l2 = 0.004243\n",
            "nt_epoch =   1580  elapsed = 04:00  loss = 1.2075e+00  error = 1.7186e-01  l1 = 0.988365  l2 = 0.004240\n",
            "nt_epoch =   1590  elapsed = 04:02  loss = 1.1939e+00  error = 1.7057e-01  l1 = 0.988390  l2 = 0.004232\n",
            "nt_epoch =   1600  elapsed = 04:03  loss = 1.1841e+00  error = 1.6960e-01  l1 = 0.988587  l2 = 0.004226\n",
            "nt_epoch =   1610  elapsed = 04:05  loss = 1.1763e+00  error = 1.6859e-01  l1 = 0.988878  l2 = 0.004221\n",
            "nt_epoch =   1620  elapsed = 04:06  loss = 1.1666e+00  error = 1.6861e-01  l1 = 0.988486  l2 = 0.004220\n",
            "nt_epoch =   1630  elapsed = 04:08  loss = 1.1558e+00  error = 1.6852e-01  l1 = 0.989130  l2 = 0.004221\n",
            "nt_epoch =   1640  elapsed = 04:09  loss = 1.1445e+00  error = 1.6819e-01  l1 = 0.988940  l2 = 0.004219\n",
            "nt_epoch =   1650  elapsed = 04:10  loss = 1.1362e+00  error = 1.6757e-01  l1 = 0.989245  l2 = 0.004216\n",
            "nt_epoch =   1660  elapsed = 04:12  loss = 1.1251e+00  error = 1.6634e-01  l1 = 0.989516  l2 = 0.004209\n",
            "nt_epoch =   1670  elapsed = 04:13  loss = 1.1154e+00  error = 1.6599e-01  l1 = 0.988958  l2 = 0.004205\n",
            "nt_epoch =   1680  elapsed = 04:15  loss = 1.1065e+00  error = 1.6502e-01  l1 = 0.989650  l2 = 0.004201\n",
            "nt_epoch =   1690  elapsed = 04:16  loss = 1.0972e+00  error = 1.6426e-01  l1 = 0.989356  l2 = 0.004195\n",
            "nt_epoch =   1700  elapsed = 04:18  loss = 1.0901e+00  error = 1.6394e-01  l1 = 0.989525  l2 = 0.004193\n",
            "nt_epoch =   1710  elapsed = 04:19  loss = 1.0842e+00  error = 1.6321e-01  l1 = 0.989633  l2 = 0.004189\n",
            "nt_epoch =   1720  elapsed = 04:20  loss = 1.0777e+00  error = 1.6324e-01  l1 = 0.989977  l2 = 0.004190\n",
            "nt_epoch =   1730  elapsed = 04:22  loss = 1.0701e+00  error = 1.6361e-01  l1 = 0.989718  l2 = 0.004192\n",
            "nt_epoch =   1740  elapsed = 04:23  loss = 1.0606e+00  error = 1.6333e-01  l1 = 0.989797  l2 = 0.004190\n",
            "nt_epoch =   1750  elapsed = 04:25  loss = 1.0532e+00  error = 1.6321e-01  l1 = 0.990038  l2 = 0.004190\n",
            "nt_epoch =   1760  elapsed = 04:26  loss = 1.0476e+00  error = 1.6287e-01  l1 = 0.989783  l2 = 0.004187\n",
            "nt_epoch =   1770  elapsed = 04:28  loss = 1.0425e+00  error = 1.6265e-01  l1 = 0.989692  l2 = 0.004186\n",
            "nt_epoch =   1780  elapsed = 04:29  loss = 1.0378e+00  error = 1.6276e-01  l1 = 0.989866  l2 = 0.004187\n",
            "nt_epoch =   1790  elapsed = 04:30  loss = 1.0323e+00  error = 1.6227e-01  l1 = 0.990168  l2 = 0.004185\n",
            "nt_epoch =   1800  elapsed = 04:32  loss = 1.0252e+00  error = 1.6125e-01  l1 = 0.990048  l2 = 0.004178\n",
            "nt_epoch =   1810  elapsed = 04:33  loss = 1.0166e+00  error = 1.6129e-01  l1 = 0.990225  l2 = 0.004179\n",
            "nt_epoch =   1820  elapsed = 04:35  loss = 1.0093e+00  error = 1.6130e-01  l1 = 0.990541  l2 = 0.004180\n",
            "nt_epoch =   1830  elapsed = 04:36  loss = 9.9968e-01  error = 1.6134e-01  l1 = 0.990157  l2 = 0.004179\n",
            "nt_epoch =   1840  elapsed = 04:38  loss = 9.9227e-01  error = 1.6108e-01  l1 = 0.990943  l2 = 0.004180\n",
            "nt_epoch =   1850  elapsed = 04:39  loss = 9.8708e-01  error = 1.6098e-01  l1 = 0.990681  l2 = 0.004178\n",
            "nt_epoch =   1860  elapsed = 04:41  loss = 9.8043e-01  error = 1.6084e-01  l1 = 0.990812  l2 = 0.004178\n",
            "nt_epoch =   1870  elapsed = 04:42  loss = 9.7207e-01  error = 1.6027e-01  l1 = 0.991381  l2 = 0.004176\n",
            "nt_epoch =   1880  elapsed = 04:43  loss = 9.6781e-01  error = 1.5963e-01  l1 = 0.991263  l2 = 0.004172\n",
            "nt_epoch =   1890  elapsed = 04:45  loss = 9.5852e-01  error = 1.5870e-01  l1 = 0.991411  l2 = 0.004166\n",
            "nt_epoch =   1900  elapsed = 04:46  loss = 9.4784e-01  error = 1.5773e-01  l1 = 0.991784  l2 = 0.004161\n",
            "nt_epoch =   1910  elapsed = 04:48  loss = 9.3941e-01  error = 1.5693e-01  l1 = 0.991339  l2 = 0.004155\n",
            "nt_epoch =   1920  elapsed = 04:49  loss = 9.3368e-01  error = 1.5638e-01  l1 = 0.991556  l2 = 0.004152\n",
            "nt_epoch =   1930  elapsed = 04:51  loss = 9.2914e-01  error = 1.5595e-01  l1 = 0.991946  l2 = 0.004150\n",
            "nt_epoch =   1940  elapsed = 04:52  loss = 9.2272e-01  error = 1.5505e-01  l1 = 0.991931  l2 = 0.004145\n",
            "nt_epoch =   1950  elapsed = 04:53  loss = 9.1393e-01  error = 1.5389e-01  l1 = 0.991959  l2 = 0.004137\n",
            "nt_epoch =   1960  elapsed = 04:55  loss = 9.0897e-01  error = 1.5377e-01  l1 = 0.992015  l2 = 0.004137\n",
            "nt_epoch =   1970  elapsed = 04:56  loss = 9.0220e-01  error = 1.5289e-01  l1 = 0.992125  l2 = 0.004131\n",
            "nt_epoch =   1980  elapsed = 04:58  loss = 8.9507e-01  error = 1.5217e-01  l1 = 0.992123  l2 = 0.004127\n",
            "nt_epoch =   1990  elapsed = 04:59  loss = 8.8714e-01  error = 1.5156e-01  l1 = 0.992306  l2 = 0.004123\n",
            "==================\n",
            "Training finished (epoch 100): duration = 05:01  error = 1.5059e-01  l1 = 0.992138  l2 = 0.004117\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_5 (Lambda)            (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 50)                100       \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 81)                4131      \n",
            "=================================================================\n",
            "Total params: 9,331\n",
            "Trainable params: 9,331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "tf_epoch =      0  elapsed = 05:01  loss = 1.2375e+04  error = 6.1151e-01  l1 = -0.000980  l2 = 0.002476\n",
            "tf_epoch =     10  elapsed = 05:03  loss = 9.4425e+03  error = 6.2203e-01  l1 = -0.018428  l2 = 0.002465\n",
            "tf_epoch =     20  elapsed = 05:04  loss = 6.4197e+03  error = 6.2544e-01  l1 = -0.040787  l2 = 0.002514\n",
            "tf_epoch =     30  elapsed = 05:05  loss = 5.9004e+03  error = 6.2803e-01  l1 = -0.065218  l2 = 0.002576\n",
            "tf_epoch =     40  elapsed = 05:06  loss = 5.1127e+03  error = 6.2973e-01  l1 = -0.088590  l2 = 0.002639\n",
            "tf_epoch =     50  elapsed = 05:08  loss = 4.3025e+03  error = 6.2985e-01  l1 = -0.106919  l2 = 0.002697\n",
            "tf_epoch =     60  elapsed = 05:09  loss = 3.4788e+03  error = 6.2903e-01  l1 = -0.117281  l2 = 0.002735\n",
            "tf_epoch =     70  elapsed = 05:10  loss = 2.8653e+03  error = 6.3099e-01  l1 = -0.115937  l2 = 0.002718\n",
            "tf_epoch =     80  elapsed = 05:11  loss = 2.4799e+03  error = 6.3410e-01  l1 = -0.101854  l2 = 0.002654\n",
            "tf_epoch =     90  elapsed = 05:12  loss = 2.2136e+03  error = 6.3474e-01  l1 = -0.080093  l2 = 0.002580\n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 05:14  loss = 9.1052e+02  error = 3.9100e-01  l1 = 0.299946  l2 = 0.002922\n",
            "nt_epoch =     20  elapsed = 05:16  loss = 6.5346e+02  error = 6.7552e-01  l1 = 0.441912  l2 = 0.005707\n",
            "nt_epoch =     30  elapsed = 05:17  loss = 4.6638e+02  error = 2.8034e+00  l1 = 0.839351  l2 = 0.020519\n",
            "nt_epoch =     40  elapsed = 05:18  loss = 3.3248e+02  error = 2.3077e+00  l1 = 0.887256  l2 = 0.017516\n",
            "nt_epoch =     50  elapsed = 05:20  loss = 2.7659e+02  error = 2.5355e+00  l1 = 0.979377  l2 = 0.019259\n",
            "nt_epoch =     60  elapsed = 05:21  loss = 2.2904e+02  error = 2.5740e+00  l1 = 1.022532  l2 = 0.019498\n",
            "nt_epoch =     70  elapsed = 05:23  loss = 1.7298e+02  error = 2.2133e+00  l1 = 0.985821  l2 = 0.017228\n",
            "nt_epoch =     80  elapsed = 05:24  loss = 1.3476e+02  error = 1.8490e+00  l1 = 0.957157  l2 = 0.014818\n",
            "nt_epoch =     90  elapsed = 05:25  loss = 1.1935e+02  error = 1.7067e+00  l1 = 0.960036  l2 = 0.013921\n",
            "nt_epoch =    100  elapsed = 05:27  loss = 1.0035e+02  error = 1.6875e+00  l1 = 0.975445  l2 = 0.013848\n",
            "nt_epoch =    110  elapsed = 05:28  loss = 8.6219e+01  error = 1.6762e+00  l1 = 0.986587  l2 = 0.013811\n",
            "nt_epoch =    120  elapsed = 05:30  loss = 7.6445e+01  error = 1.6216e+00  l1 = 0.991943  l2 = 0.013481\n",
            "nt_epoch =    130  elapsed = 05:31  loss = 6.7425e+01  error = 1.4478e+00  l1 = 0.988927  l2 = 0.012365\n",
            "nt_epoch =    140  elapsed = 05:32  loss = 6.1783e+01  error = 1.3346e+00  l1 = 0.985831  l2 = 0.011634\n",
            "nt_epoch =    150  elapsed = 05:34  loss = 5.6698e+01  error = 1.2642e+00  l1 = 0.996995  l2 = 0.011222\n",
            "nt_epoch =    160  elapsed = 05:35  loss = 5.0897e+01  error = 1.1785e+00  l1 = 0.997859  l2 = 0.010679\n",
            "nt_epoch =    170  elapsed = 05:37  loss = 4.6084e+01  error = 1.0708e+00  l1 = 0.998287  l2 = 0.009994\n",
            "nt_epoch =    180  elapsed = 05:38  loss = 4.2660e+01  error = 9.5266e-01  l1 = 0.996247  l2 = 0.009236\n",
            "nt_epoch =    190  elapsed = 05:40  loss = 3.9221e+01  error = 9.1690e-01  l1 = 1.010366  l2 = 0.008987\n",
            "nt_epoch =    200  elapsed = 05:41  loss = 3.6825e+01  error = 8.1434e-01  l1 = 0.994865  l2 = 0.008351\n",
            "nt_epoch =    210  elapsed = 05:43  loss = 3.3894e+01  error = 7.7300e-01  l1 = 0.997136  l2 = 0.008095\n",
            "nt_epoch =    220  elapsed = 05:44  loss = 3.1747e+01  error = 7.4573e-01  l1 = 0.990612  l2 = 0.007901\n",
            "nt_epoch =    230  elapsed = 05:45  loss = 2.9377e+01  error = 7.1902e-01  l1 = 0.990143  l2 = 0.007729\n",
            "nt_epoch =    240  elapsed = 05:47  loss = 2.6872e+01  error = 7.1508e-01  l1 = 0.985139  l2 = 0.007688\n",
            "nt_epoch =    250  elapsed = 05:48  loss = 2.5530e+01  error = 7.1099e-01  l1 = 0.985545  l2 = 0.007663\n",
            "nt_epoch =    260  elapsed = 05:50  loss = 2.4071e+01  error = 7.0604e-01  l1 = 0.976619  l2 = 0.007603\n",
            "nt_epoch =    270  elapsed = 05:51  loss = 2.2901e+01  error = 7.0674e-01  l1 = 0.982473  l2 = 0.007627\n",
            "nt_epoch =    280  elapsed = 05:53  loss = 2.1926e+01  error = 7.3069e-01  l1 = 0.980621  l2 = 0.007773\n",
            "nt_epoch =    290  elapsed = 05:54  loss = 2.1246e+01  error = 7.2854e-01  l1 = 0.981635  l2 = 0.007763\n",
            "nt_epoch =    300  elapsed = 05:55  loss = 2.0617e+01  error = 7.3374e-01  l1 = 0.985266  l2 = 0.007807\n",
            "nt_epoch =    310  elapsed = 05:57  loss = 1.9888e+01  error = 7.1673e-01  l1 = 0.987139  l2 = 0.007705\n",
            "nt_epoch =    320  elapsed = 05:58  loss = 1.9186e+01  error = 7.0999e-01  l1 = 0.987250  l2 = 0.007662\n",
            "nt_epoch =    330  elapsed = 06:00  loss = 1.8593e+01  error = 6.8324e-01  l1 = 0.988003  l2 = 0.007495\n",
            "nt_epoch =    340  elapsed = 06:01  loss = 1.7900e+01  error = 6.6910e-01  l1 = 0.986318  l2 = 0.007399\n",
            "nt_epoch =    350  elapsed = 06:03  loss = 1.7086e+01  error = 6.5334e-01  l1 = 0.985884  l2 = 0.007297\n",
            "nt_epoch =    360  elapsed = 06:04  loss = 1.6255e+01  error = 6.3924e-01  l1 = 0.988448  l2 = 0.007216\n",
            "nt_epoch =    370  elapsed = 06:06  loss = 1.5403e+01  error = 6.2896e-01  l1 = 0.987947  l2 = 0.007149\n",
            "nt_epoch =    380  elapsed = 06:07  loss = 1.4398e+01  error = 5.8714e-01  l1 = 0.984063  l2 = 0.006870\n",
            "nt_epoch =    390  elapsed = 06:09  loss = 1.3611e+01  error = 5.5787e-01  l1 = 0.982858  l2 = 0.006680\n",
            "nt_epoch =    400  elapsed = 06:10  loss = 1.2917e+01  error = 5.2944e-01  l1 = 0.983034  l2 = 0.006500\n",
            "nt_epoch =    410  elapsed = 06:11  loss = 1.2326e+01  error = 5.0861e-01  l1 = 0.982497  l2 = 0.006365\n",
            "nt_epoch =    420  elapsed = 06:13  loss = 1.1814e+01  error = 4.8586e-01  l1 = 0.978108  l2 = 0.006207\n",
            "nt_epoch =    430  elapsed = 06:14  loss = 1.1390e+01  error = 4.8083e-01  l1 = 0.983404  l2 = 0.006191\n",
            "nt_epoch =    440  elapsed = 06:16  loss = 1.0994e+01  error = 4.6760e-01  l1 = 0.979914  l2 = 0.006096\n",
            "nt_epoch =    450  elapsed = 06:17  loss = 1.0584e+01  error = 4.5800e-01  l1 = 0.980342  l2 = 0.006036\n",
            "nt_epoch =    460  elapsed = 06:19  loss = 1.0083e+01  error = 4.4964e-01  l1 = 0.977135  l2 = 0.005973\n",
            "nt_epoch =    470  elapsed = 06:20  loss = 9.5992e+00  error = 4.4329e-01  l1 = 0.979977  l2 = 0.005941\n",
            "nt_epoch =    480  elapsed = 06:21  loss = 9.2303e+00  error = 4.3702e-01  l1 = 0.976155  l2 = 0.005889\n",
            "nt_epoch =    490  elapsed = 06:23  loss = 8.8740e+00  error = 4.2571e-01  l1 = 0.980277  l2 = 0.005830\n",
            "nt_epoch =    500  elapsed = 06:24  loss = 8.6849e+00  error = 4.2407e-01  l1 = 0.980870  l2 = 0.005822\n",
            "nt_epoch =    510  elapsed = 06:26  loss = 8.2775e+00  error = 4.1275e-01  l1 = 0.978785  l2 = 0.005743\n",
            "nt_epoch =    520  elapsed = 06:27  loss = 7.9131e+00  error = 4.0650e-01  l1 = 0.977400  l2 = 0.005699\n",
            "nt_epoch =    530  elapsed = 06:29  loss = 7.6322e+00  error = 4.0177e-01  l1 = 0.979587  l2 = 0.005676\n",
            "nt_epoch =    540  elapsed = 06:30  loss = 7.4403e+00  error = 3.9769e-01  l1 = 0.978533  l2 = 0.005647\n",
            "nt_epoch =    550  elapsed = 06:31  loss = 7.2869e+00  error = 3.9387e-01  l1 = 0.980435  l2 = 0.005628\n",
            "nt_epoch =    560  elapsed = 06:33  loss = 7.0286e+00  error = 3.8509e-01  l1 = 0.978019  l2 = 0.005565\n",
            "nt_epoch =    570  elapsed = 06:34  loss = 6.7969e+00  error = 3.7834e-01  l1 = 0.980438  l2 = 0.005529\n",
            "nt_epoch =    580  elapsed = 06:36  loss = 6.5548e+00  error = 3.7214e-01  l1 = 0.979196  l2 = 0.005486\n",
            "nt_epoch =    590  elapsed = 06:37  loss = 6.3867e+00  error = 3.6646e-01  l1 = 0.978832  l2 = 0.005449\n",
            "nt_epoch =    600  elapsed = 06:39  loss = 6.2502e+00  error = 3.6139e-01  l1 = 0.979171  l2 = 0.005418\n",
            "nt_epoch =    610  elapsed = 06:40  loss = 6.1082e+00  error = 3.5404e-01  l1 = 0.980690  l2 = 0.005376\n",
            "nt_epoch =    620  elapsed = 06:41  loss = 5.9474e+00  error = 3.4840e-01  l1 = 0.978458  l2 = 0.005333\n",
            "nt_epoch =    630  elapsed = 06:43  loss = 5.7540e+00  error = 3.4090e-01  l1 = 0.979258  l2 = 0.005287\n",
            "nt_epoch =    640  elapsed = 06:44  loss = 5.6235e+00  error = 3.3603e-01  l1 = 0.979988  l2 = 0.005259\n",
            "nt_epoch =    650  elapsed = 06:46  loss = 5.5462e+00  error = 3.3523e-01  l1 = 0.979532  l2 = 0.005252\n",
            "nt_epoch =    660  elapsed = 06:47  loss = 5.4459e+00  error = 3.3518e-01  l1 = 0.980777  l2 = 0.005256\n",
            "nt_epoch =    670  elapsed = 06:49  loss = 5.3063e+00  error = 3.3070e-01  l1 = 0.980070  l2 = 0.005225\n",
            "nt_epoch =    680  elapsed = 06:50  loss = 5.1977e+00  error = 3.2806e-01  l1 = 0.981803  l2 = 0.005214\n",
            "nt_epoch =    690  elapsed = 06:52  loss = 5.0885e+00  error = 3.2392e-01  l1 = 0.981171  l2 = 0.005185\n",
            "nt_epoch =    700  elapsed = 06:53  loss = 5.0219e+00  error = 3.2296e-01  l1 = 0.982298  l2 = 0.005183\n",
            "nt_epoch =    710  elapsed = 06:54  loss = 5.1541e+00  error = 3.1653e-01  l1 = 0.983644  l2 = 0.005146\n",
            "nt_epoch =    720  elapsed = 06:56  loss = 4.9093e+00  error = 3.1746e-01  l1 = 0.981600  l2 = 0.005146\n",
            "nt_epoch =    730  elapsed = 06:57  loss = 4.8520e+00  error = 3.1326e-01  l1 = 0.982050  l2 = 0.005120\n",
            "nt_epoch =    740  elapsed = 06:59  loss = 4.7853e+00  error = 3.1327e-01  l1 = 0.981378  l2 = 0.005118\n",
            "nt_epoch =    750  elapsed = 07:00  loss = 4.7370e+00  error = 3.1234e-01  l1 = 0.981622  l2 = 0.005113\n",
            "nt_epoch =    760  elapsed = 07:02  loss = 4.6917e+00  error = 3.1142e-01  l1 = 0.981291  l2 = 0.005106\n",
            "nt_epoch =    770  elapsed = 07:03  loss = 4.6324e+00  error = 3.1037e-01  l1 = 0.981113  l2 = 0.005099\n",
            "nt_epoch =    780  elapsed = 07:04  loss = 4.5676e+00  error = 3.0531e-01  l1 = 0.980430  l2 = 0.005064\n",
            "nt_epoch =    790  elapsed = 07:06  loss = 4.4945e+00  error = 3.0279e-01  l1 = 0.980853  l2 = 0.005050\n",
            "nt_epoch =    800  elapsed = 07:07  loss = 4.4528e+00  error = 3.0302e-01  l1 = 0.980040  l2 = 0.005049\n",
            "nt_epoch =    810  elapsed = 07:09  loss = 4.4211e+00  error = 3.0240e-01  l1 = 0.979953  l2 = 0.005044\n",
            "nt_epoch =    820  elapsed = 07:10  loss = 4.3699e+00  error = 3.0187e-01  l1 = 0.980295  l2 = 0.005042\n",
            "nt_epoch =    830  elapsed = 07:11  loss = 4.3108e+00  error = 3.0105e-01  l1 = 0.980259  l2 = 0.005037\n",
            "nt_epoch =    840  elapsed = 07:13  loss = 4.2555e+00  error = 3.0158e-01  l1 = 0.980581  l2 = 0.005041\n",
            "nt_epoch =    850  elapsed = 07:14  loss = 4.1913e+00  error = 3.0120e-01  l1 = 0.981394  l2 = 0.005041\n",
            "nt_epoch =    860  elapsed = 07:16  loss = 4.1669e+00  error = 3.0101e-01  l1 = 0.980046  l2 = 0.005036\n",
            "nt_epoch =    870  elapsed = 07:17  loss = 4.1284e+00  error = 3.0207e-01  l1 = 0.981388  l2 = 0.005047\n",
            "nt_epoch =    880  elapsed = 07:19  loss = 4.0950e+00  error = 3.0359e-01  l1 = 0.981228  l2 = 0.005056\n",
            "nt_epoch =    890  elapsed = 07:20  loss = 4.0321e+00  error = 3.0450e-01  l1 = 0.981725  l2 = 0.005063\n",
            "nt_epoch =    900  elapsed = 07:21  loss = 3.9775e+00  error = 3.0578e-01  l1 = 0.981942  l2 = 0.005072\n",
            "nt_epoch =    910  elapsed = 07:23  loss = 3.9354e+00  error = 3.0666e-01  l1 = 0.981654  l2 = 0.005077\n",
            "nt_epoch =    920  elapsed = 07:24  loss = 3.9053e+00  error = 3.0712e-01  l1 = 0.982297  l2 = 0.005082\n",
            "nt_epoch =    930  elapsed = 07:26  loss = 3.8652e+00  error = 3.0846e-01  l1 = 0.982106  l2 = 0.005090\n",
            "nt_epoch =    940  elapsed = 07:27  loss = 3.8258e+00  error = 3.0775e-01  l1 = 0.982220  l2 = 0.005086\n",
            "nt_epoch =    950  elapsed = 07:28  loss = 3.7878e+00  error = 3.0964e-01  l1 = 0.982974  l2 = 0.005100\n",
            "nt_epoch =    960  elapsed = 07:30  loss = 3.7540e+00  error = 3.0991e-01  l1 = 0.983233  l2 = 0.005103\n",
            "nt_epoch =    970  elapsed = 07:31  loss = 3.7208e+00  error = 3.1044e-01  l1 = 0.983012  l2 = 0.005105\n",
            "nt_epoch =    980  elapsed = 07:33  loss = 3.6858e+00  error = 3.1032e-01  l1 = 0.983339  l2 = 0.005106\n",
            "nt_epoch =    990  elapsed = 07:34  loss = 3.6396e+00  error = 3.0908e-01  l1 = 0.983093  l2 = 0.005097\n",
            "nt_epoch =   1000  elapsed = 07:36  loss = 3.5983e+00  error = 3.0679e-01  l1 = 0.983825  l2 = 0.005085\n",
            "nt_epoch =   1010  elapsed = 07:37  loss = 3.5567e+00  error = 3.0539e-01  l1 = 0.982995  l2 = 0.005073\n",
            "nt_epoch =   1020  elapsed = 07:39  loss = 3.5134e+00  error = 3.0393e-01  l1 = 0.984499  l2 = 0.005069\n",
            "nt_epoch =   1030  elapsed = 07:40  loss = 3.4804e+00  error = 3.0274e-01  l1 = 0.983934  l2 = 0.005059\n",
            "nt_epoch =   1040  elapsed = 07:42  loss = 3.4311e+00  error = 3.0087e-01  l1 = 0.984429  l2 = 0.005049\n",
            "nt_epoch =   1050  elapsed = 07:43  loss = 3.3842e+00  error = 2.9886e-01  l1 = 0.984258  l2 = 0.005036\n",
            "nt_epoch =   1060  elapsed = 07:45  loss = 3.3490e+00  error = 2.9669e-01  l1 = 0.984159  l2 = 0.005021\n",
            "nt_epoch =   1070  elapsed = 07:46  loss = 3.3187e+00  error = 2.9460e-01  l1 = 0.983721  l2 = 0.005007\n",
            "nt_epoch =   1080  elapsed = 07:48  loss = 3.2984e+00  error = 2.9352e-01  l1 = 0.984596  l2 = 0.005003\n",
            "nt_epoch =   1090  elapsed = 07:49  loss = 3.2645e+00  error = 2.9244e-01  l1 = 0.985104  l2 = 0.004997\n",
            "nt_epoch =   1100  elapsed = 07:50  loss = 3.2238e+00  error = 2.9000e-01  l1 = 0.985555  l2 = 0.004983\n",
            "nt_epoch =   1110  elapsed = 07:52  loss = 3.1845e+00  error = 2.8617e-01  l1 = 0.985177  l2 = 0.004958\n",
            "nt_epoch =   1120  elapsed = 07:53  loss = 3.1477e+00  error = 2.8225e-01  l1 = 0.985516  l2 = 0.004934\n",
            "nt_epoch =   1130  elapsed = 07:55  loss = 3.1052e+00  error = 2.7892e-01  l1 = 0.985183  l2 = 0.004912\n",
            "nt_epoch =   1140  elapsed = 07:56  loss = 3.0698e+00  error = 2.7439e-01  l1 = 0.985692  l2 = 0.004884\n",
            "nt_epoch =   1150  elapsed = 07:58  loss = 3.0312e+00  error = 2.7153e-01  l1 = 0.985969  l2 = 0.004867\n",
            "nt_epoch =   1160  elapsed = 07:59  loss = 2.9925e+00  error = 2.6837e-01  l1 = 0.986978  l2 = 0.004850\n",
            "nt_epoch =   1170  elapsed = 08:01  loss = 2.9669e+00  error = 2.6512e-01  l1 = 0.987138  l2 = 0.004830\n",
            "nt_epoch =   1180  elapsed = 08:02  loss = 2.9399e+00  error = 2.6112e-01  l1 = 0.987097  l2 = 0.004804\n",
            "nt_epoch =   1190  elapsed = 08:04  loss = 2.9175e+00  error = 2.5999e-01  l1 = 0.987256  l2 = 0.004798\n",
            "nt_epoch =   1200  elapsed = 08:05  loss = 2.8918e+00  error = 2.5619e-01  l1 = 0.987404  l2 = 0.004774\n",
            "nt_epoch =   1210  elapsed = 08:07  loss = 2.8567e+00  error = 2.5239e-01  l1 = 0.987667  l2 = 0.004751\n",
            "nt_epoch =   1220  elapsed = 08:08  loss = 2.8233e+00  error = 2.4851e-01  l1 = 0.987508  l2 = 0.004725\n",
            "nt_epoch =   1230  elapsed = 08:09  loss = 2.7941e+00  error = 2.4652e-01  l1 = 0.988415  l2 = 0.004716\n",
            "nt_epoch =   1240  elapsed = 08:11  loss = 2.7676e+00  error = 2.4282e-01  l1 = 0.988324  l2 = 0.004692\n",
            "nt_epoch =   1250  elapsed = 08:12  loss = 2.7405e+00  error = 2.4146e-01  l1 = 0.988619  l2 = 0.004684\n",
            "nt_epoch =   1260  elapsed = 08:14  loss = 2.6985e+00  error = 2.3458e-01  l1 = 0.989254  l2 = 0.004642\n",
            "nt_epoch =   1270  elapsed = 08:15  loss = 2.6628e+00  error = 2.3238e-01  l1 = 0.989305  l2 = 0.004628\n",
            "nt_epoch =   1280  elapsed = 08:16  loss = 2.6309e+00  error = 2.2727e-01  l1 = 0.990036  l2 = 0.004598\n",
            "nt_epoch =   1290  elapsed = 08:18  loss = 2.6084e+00  error = 2.2564e-01  l1 = 0.990203  l2 = 0.004588\n",
            "nt_epoch =   1300  elapsed = 08:19  loss = 2.5877e+00  error = 2.2253e-01  l1 = 0.991037  l2 = 0.004571\n",
            "nt_epoch =   1310  elapsed = 08:21  loss = 2.5621e+00  error = 2.2012e-01  l1 = 0.990372  l2 = 0.004554\n",
            "nt_epoch =   1320  elapsed = 08:22  loss = 2.5368e+00  error = 2.1952e-01  l1 = 0.990722  l2 = 0.004551\n",
            "nt_epoch =   1330  elapsed = 08:24  loss = 2.5145e+00  error = 2.1682e-01  l1 = 0.990935  l2 = 0.004535\n",
            "nt_epoch =   1340  elapsed = 08:25  loss = 2.4932e+00  error = 2.1396e-01  l1 = 0.991563  l2 = 0.004518\n",
            "nt_epoch =   1350  elapsed = 08:26  loss = 2.4748e+00  error = 2.1251e-01  l1 = 0.991260  l2 = 0.004508\n",
            "nt_epoch =   1360  elapsed = 08:28  loss = 2.4566e+00  error = 2.0950e-01  l1 = 0.991549  l2 = 0.004490\n",
            "nt_epoch =   1370  elapsed = 08:29  loss = 2.4323e+00  error = 2.0732e-01  l1 = 0.991901  l2 = 0.004477\n",
            "nt_epoch =   1380  elapsed = 08:31  loss = 2.4108e+00  error = 2.0467e-01  l1 = 0.991860  l2 = 0.004460\n",
            "nt_epoch =   1390  elapsed = 08:32  loss = 2.3928e+00  error = 2.0193e-01  l1 = 0.991796  l2 = 0.004443\n",
            "nt_epoch =   1400  elapsed = 08:34  loss = 2.3751e+00  error = 2.0112e-01  l1 = 0.992655  l2 = 0.004440\n",
            "nt_epoch =   1410  elapsed = 08:35  loss = 2.3559e+00  error = 1.9686e-01  l1 = 0.992391  l2 = 0.004412\n",
            "nt_epoch =   1420  elapsed = 08:37  loss = 2.3330e+00  error = 1.9580e-01  l1 = 0.992834  l2 = 0.004407\n",
            "nt_epoch =   1430  elapsed = 08:38  loss = 2.3137e+00  error = 1.9404e-01  l1 = 0.993263  l2 = 0.004397\n",
            "nt_epoch =   1440  elapsed = 08:40  loss = 2.2924e+00  error = 1.9178e-01  l1 = 0.993088  l2 = 0.004382\n",
            "nt_epoch =   1450  elapsed = 08:41  loss = 2.2761e+00  error = 1.9036e-01  l1 = 0.993048  l2 = 0.004373\n",
            "nt_epoch =   1460  elapsed = 08:42  loss = 2.2629e+00  error = 1.8886e-01  l1 = 0.993165  l2 = 0.004364\n",
            "nt_epoch =   1470  elapsed = 08:44  loss = 2.2511e+00  error = 1.8761e-01  l1 = 0.993044  l2 = 0.004355\n",
            "nt_epoch =   1480  elapsed = 08:45  loss = 2.2342e+00  error = 1.8662e-01  l1 = 0.993102  l2 = 0.004349\n",
            "nt_epoch =   1490  elapsed = 08:47  loss = 2.2220e+00  error = 1.8525e-01  l1 = 0.993015  l2 = 0.004340\n",
            "nt_epoch =   1500  elapsed = 08:48  loss = 2.2076e+00  error = 1.8442e-01  l1 = 0.992909  l2 = 0.004335\n",
            "nt_epoch =   1510  elapsed = 08:49  loss = 2.1969e+00  error = 1.8235e-01  l1 = 0.993218  l2 = 0.004322\n",
            "nt_epoch =   1520  elapsed = 08:51  loss = 2.1837e+00  error = 1.8071e-01  l1 = 0.992823  l2 = 0.004311\n",
            "nt_epoch =   1530  elapsed = 08:52  loss = 2.1745e+00  error = 1.8012e-01  l1 = 0.993067  l2 = 0.004308\n",
            "nt_epoch =   1540  elapsed = 08:54  loss = 2.1615e+00  error = 1.7761e-01  l1 = 0.993016  l2 = 0.004292\n",
            "nt_epoch =   1550  elapsed = 08:55  loss = 2.1461e+00  error = 1.7549e-01  l1 = 0.992815  l2 = 0.004277\n",
            "nt_epoch =   1560  elapsed = 08:57  loss = 2.1371e+00  error = 1.7373e-01  l1 = 0.992785  l2 = 0.004266\n",
            "nt_epoch =   1570  elapsed = 08:58  loss = 2.1281e+00  error = 1.7320e-01  l1 = 0.992673  l2 = 0.004262\n",
            "nt_epoch =   1580  elapsed = 09:00  loss = 2.1210e+00  error = 1.7349e-01  l1 = 0.992537  l2 = 0.004264\n",
            "nt_epoch =   1590  elapsed = 09:01  loss = 2.1123e+00  error = 1.7114e-01  l1 = 0.992445  l2 = 0.004249\n",
            "nt_epoch =   1600  elapsed = 09:02  loss = 2.1025e+00  error = 1.7024e-01  l1 = 0.992769  l2 = 0.004244\n",
            "nt_epoch =   1610  elapsed = 09:04  loss = 2.0949e+00  error = 1.6911e-01  l1 = 0.992187  l2 = 0.004235\n",
            "nt_epoch =   1620  elapsed = 09:05  loss = 2.0876e+00  error = 1.6762e-01  l1 = 0.992220  l2 = 0.004225\n",
            "nt_epoch =   1630  elapsed = 09:07  loss = 2.0804e+00  error = 1.6801e-01  l1 = 0.992214  l2 = 0.004228\n",
            "nt_epoch =   1640  elapsed = 09:08  loss = 2.0698e+00  error = 1.6570e-01  l1 = 0.992167  l2 = 0.004213\n",
            "nt_epoch =   1650  elapsed = 09:10  loss = 2.0605e+00  error = 1.6640e-01  l1 = 0.991871  l2 = 0.004217\n",
            "nt_epoch =   1660  elapsed = 09:11  loss = 2.0525e+00  error = 1.6550e-01  l1 = 0.991667  l2 = 0.004210\n",
            "nt_epoch =   1670  elapsed = 09:13  loss = 2.0465e+00  error = 1.6491e-01  l1 = 0.991651  l2 = 0.004206\n",
            "nt_epoch =   1680  elapsed = 09:14  loss = 2.0392e+00  error = 1.6470e-01  l1 = 0.991573  l2 = 0.004205\n",
            "nt_epoch =   1690  elapsed = 09:16  loss = 2.0306e+00  error = 1.6343e-01  l1 = 0.991528  l2 = 0.004197\n",
            "nt_epoch =   1700  elapsed = 09:17  loss = 2.0232e+00  error = 1.6237e-01  l1 = 0.991151  l2 = 0.004189\n",
            "nt_epoch =   1710  elapsed = 09:18  loss = 2.0170e+00  error = 1.6169e-01  l1 = 0.991036  l2 = 0.004184\n",
            "nt_epoch =   1720  elapsed = 09:20  loss = 2.0093e+00  error = 1.6166e-01  l1 = 0.990939  l2 = 0.004183\n",
            "nt_epoch =   1730  elapsed = 09:21  loss = 2.0034e+00  error = 1.6045e-01  l1 = 0.991117  l2 = 0.004176\n",
            "nt_epoch =   1740  elapsed = 09:23  loss = 1.9950e+00  error = 1.5927e-01  l1 = 0.991044  l2 = 0.004169\n",
            "nt_epoch =   1750  elapsed = 09:24  loss = 1.9812e+00  error = 1.5751e-01  l1 = 0.990634  l2 = 0.004156\n",
            "nt_epoch =   1760  elapsed = 09:26  loss = 1.9726e+00  error = 1.5534e-01  l1 = 0.990861  l2 = 0.004143\n",
            "nt_epoch =   1770  elapsed = 09:27  loss = 1.9661e+00  error = 1.5503e-01  l1 = 0.990743  l2 = 0.004141\n",
            "nt_epoch =   1780  elapsed = 09:29  loss = 1.9582e+00  error = 1.5453e-01  l1 = 0.990585  l2 = 0.004137\n",
            "nt_epoch =   1790  elapsed = 09:30  loss = 1.9495e+00  error = 1.5312e-01  l1 = 0.990760  l2 = 0.004128\n",
            "nt_epoch =   1800  elapsed = 09:32  loss = 1.9384e+00  error = 1.5239e-01  l1 = 0.990694  l2 = 0.004124\n",
            "nt_epoch =   1810  elapsed = 09:33  loss = 1.9293e+00  error = 1.5184e-01  l1 = 0.990796  l2 = 0.004120\n",
            "nt_epoch =   1820  elapsed = 09:35  loss = 1.9217e+00  error = 1.5087e-01  l1 = 0.991175  l2 = 0.004115\n",
            "nt_epoch =   1830  elapsed = 09:36  loss = 1.9175e+00  error = 1.5078e-01  l1 = 0.991187  l2 = 0.004115\n",
            "nt_epoch =   1840  elapsed = 09:37  loss = 1.9087e+00  error = 1.5010e-01  l1 = 0.990871  l2 = 0.004110\n",
            "nt_epoch =   1850  elapsed = 09:39  loss = 1.8968e+00  error = 1.4819e-01  l1 = 0.990836  l2 = 0.004097\n",
            "nt_epoch =   1860  elapsed = 09:40  loss = 1.8844e+00  error = 1.4699e-01  l1 = 0.991189  l2 = 0.004091\n",
            "nt_epoch =   1870  elapsed = 09:42  loss = 1.8785e+00  error = 1.4630e-01  l1 = 0.991202  l2 = 0.004086\n",
            "nt_epoch =   1880  elapsed = 09:43  loss = 1.8669e+00  error = 1.4382e-01  l1 = 0.991336  l2 = 0.004071\n",
            "nt_epoch =   1890  elapsed = 09:45  loss = 1.8571e+00  error = 1.4202e-01  l1 = 0.991680  l2 = 0.004061\n",
            "nt_epoch =   1900  elapsed = 09:46  loss = 1.8466e+00  error = 1.4043e-01  l1 = 0.991458  l2 = 0.004050\n",
            "nt_epoch =   1910  elapsed = 09:47  loss = 1.8367e+00  error = 1.3875e-01  l1 = 0.991723  l2 = 0.004040\n",
            "nt_epoch =   1920  elapsed = 09:49  loss = 1.8283e+00  error = 1.3826e-01  l1 = 0.992082  l2 = 0.004038\n",
            "nt_epoch =   1930  elapsed = 09:50  loss = 1.8287e+00  error = 1.3700e-01  l1 = 0.992307  l2 = 0.004031\n",
            "nt_epoch =   1940  elapsed = 09:52  loss = 1.8149e+00  error = 1.3597e-01  l1 = 0.992430  l2 = 0.004025\n",
            "nt_epoch =   1950  elapsed = 09:53  loss = 1.8078e+00  error = 1.3427e-01  l1 = 0.992655  l2 = 0.004014\n",
            "nt_epoch =   1960  elapsed = 09:55  loss = 1.7971e+00  error = 1.3348e-01  l1 = 0.993082  l2 = 0.004011\n",
            "nt_epoch =   1970  elapsed = 09:56  loss = 1.7881e+00  error = 1.3207e-01  l1 = 0.992915  l2 = 0.004001\n",
            "nt_epoch =   1980  elapsed = 09:57  loss = 1.7834e+00  error = 1.3103e-01  l1 = 0.993210  l2 = 0.003996\n",
            "nt_epoch =   1990  elapsed = 09:59  loss = 1.7775e+00  error = 1.2994e-01  l1 = 0.992971  l2 = 0.003988\n",
            "==================\n",
            "Training finished (epoch 100): duration = 10:00  error = 1.2810e-01  l1 = 0.993191  l2 = 0.003977\n",
            "l1:  0.9921382\n",
            "l2:  0.00411678\n",
            "noisy l1:  0.99319065\n",
            "noisy l2:  0.0039769323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kUtmuGlpw1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "3c202adb-832f-4f71-fd18-33656f04143c"
      },
      "source": [
        "plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
        "  ub, lb, U_1_pred, Exact_u, lambda_1_pred, lambda_1_pred_noisy, lambda_2_pred, lambda_2_pred_noisy, x_star, t_star)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAF+CAYAAAAhsMkvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsTc95zsmQQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 388.543x360.199 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}